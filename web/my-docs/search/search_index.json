{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Home for all my documentation on several different things. Honestly just a info dump in a private repository.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/","title":"Azure 204 - Module 01 Revision","text":"<p>Issue</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#overview","title":"Overview","text":"<p>Azure App Service is an HTTP-based service for hosting web applications, REST APIs, and mobile back ends. You can develop in your favorite programming language or framework. Applications run and scale with ease on both Windows and Linux-based environments.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#benefits","title":"Benefits","text":"<ul> <li>Built-in auto scale support</li> <li>Continuous integration/deployment support</li> <li>Deployment slots</li> <li>Does support Linux</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#limitations","title":"Limitations","text":"<ul> <li>App Service on Linux isn't supported on Shared pricing tier.</li> <li>The Azure portal shows only features that currently work for Linux apps. As features are enabled, they're activated on the portal.</li> <li>When deployed to built-in images, your code and content are allocated a storage volume for web content, backed by Azure Storage. The disk latency of this volume is higher and more variable than the latency of the container filesystem. Apps that require heavy read-only access to content files may benefit from the custom container option, which places files in the container filesystem instead of on the content volume.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#app-service-plans","title":"App Service plans","text":"<p>In App Service, an app always runs in an App Service plan. An App Service plan defines a set of compute resources for a web app to run. One or more apps can be configured to run on the same computing resources (or in the same App Service plan).</p> <p>Each App Service plan defines:</p> <ul> <li>Operating System (Windows, Linux)</li> <li>Region (West US, East US, etc.)</li> <li>Number of VM instances</li> <li>Size of VM instances (Small, Medium, Large)</li> <li>Pricing tier (Free, Shared, Basic, Standard, Premium, PremiumV2, PremiumV3, Isolated, IsolatedV2)</li> </ul> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#pricing-tiers","title":"Pricing tiers","text":"<p>The pricing tier of an App Service plan determines what App Service features you get and how much you pay for the plan. There are a few categories of pricing tiers:</p> <ul> <li>Shared compute: Free and Shared, the two base tiers, runs an app on the same Azure VM as other App Service apps, including apps of other customers. These tiers allocate CPU quotas to each app that runs on the shared resources, and the resources can't scale out.</li> <li>Dedicated compute: The Basic, Standard, Premium, PremiumV2, and PremiumV3 tiers run apps on dedicated Azure VMs. Only apps in the same App Service plan share the same compute resources. The higher the tier, the more VM instances are available to you for scale-out.</li> <li>Isolated: The Isolated and IsolatedV2 tiers run dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#scaling","title":"Scaling","text":"<p>In the Free and Shared tiers, an app receives CPU minutes on a shared VM instance and can't scale out. In other tiers, an app runs and scales as follows:</p> <ul> <li>An app runs on all the VM instances configured in the App Service plan.</li> <li>If multiple apps are in the same App Service plan, they all share the same VM instances.</li> <li>If you have multiple deployment slots for an app, all deployment slots also run on the same VM instances.</li> <li>If you enable diagnostic logs, perform backups, or run WebJobs, they also use CPU cycles and memory on these VM instances.</li> </ul> <p></p> <p></p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#when-to-isolate-your-app-service-plans","title":"When to isolate your app service plans","text":"<p>Isolate your app into a new App Service plan when:</p> <ul> <li>The app is resource-intensive.</li> <li>You want to scale the app independently from the other apps in the existing plan.</li> <li>The app needs resource in a different geographical region.</li> </ul> <p>This way you can allocate a new set of resources for your app and gain greater control of your apps.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#deployment","title":"Deployment","text":"<p>App Service supports both automated and manual deployment.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#automated-deployment","title":"Automated deployment","text":"<p>Azure supports automated deployment directly from several sources. The following options are available:</p> <ul> <li>Azure DevOps Services: You can push your code to Azure DevOps Services, build your code in the cloud, run the tests, generate a release from the code, and finally, push your code to an Azure Web App.</li> <li>GitHub: Azure supports automated deployment directly from GitHub. When you connect your GitHub repository to Azure for automated deployment, any changes you push to your production branch on GitHub are automatically deployed for you.</li> <li>Bitbucket: With its similarities to GitHub, you can configure an automated deployment with Bitbucket.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#manual-deployment","title":"Manual deployment","text":"<p>There are a few options that you can use to manually push your code to Azure:</p> <ul> <li>Git: App Service web apps feature a Git URL that you can add as a remote repository. Pushing to the remote repository deploys your app.</li> <li>CLI: <code>webapp up</code> is a feature of the <code>az</code> command-line interface that packages your app and deploys it. Unlike other deployment methods, <code>az webapp up</code> can create a new App Service web app for you if you haven't already created one.</li> <li>Zip deploy: Use <code>curl</code> or a similar HTTP utility to send a ZIP of your application files to App Service.</li> <li>FTP/S: FTP or FTPS is a traditional way of pushing your code to many hosting environments, including App Service.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#deployment-slots","title":"Deployment slots","text":"<p>Whenever possible, use deployment slots when deploying a new production build. When using a Standard App Service Plan tier or better, you can deploy your app to a staging environment and then swap your staging and production slots. The swap operation warms up the necessary worker instances to match your production scale, thus eliminating downtime.</p> <p></p> <p></p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#authentication-and-authorization-in-app-service","title":"Authentication and authorization in App Service","text":"<p>Azure App Service provides built-in authentication and authorization support, so you can sign in users and access data by writing minimal, or no code in your web app, RESTful API, mobile back end, and Azure Functions.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#identity-providers","title":"Identity providers","text":"Provider Sign-in endpoint How-To guidance Microsoft identity platform <code>/.auth/login/aad</code> App Service Microsoft identity platform login Facebook <code>/.auth/login/facebook</code> App Service Facebook login Google <code>/.auth/login/google</code> App Service Google login Twitter <code>/.auth/login/twitter</code> App Service Twitter login Any OpenID Connect provider <code>/.auth/login/&lt;providerName&gt;</code> App Service OpenID Connect login GitHub <code>/.auth/login/github</code> App Service GitHub login ### How it works <p>The authentication and authorization module runs in the same sandbox as your application code. When it's enabled, every incoming HTTP request passes through it before being handled by your application code. This module handles several things for your app:</p> <ul> <li>Authenticates users and clients with the specified identity provider(s)</li> <li>Validates, stores, and refreshes OAuth tokens issued by the configured identity provider(s)</li> <li>Manages the authenticated session</li> <li>Injects identity information into HTTP request headers</li> </ul> <pre><code>In Linux and containers the authentication and authorization module runs in a separate container, isolated from your application code. Because it does not run in-process, no direct integration with specific language frameworks is possible.\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#authentication-flow","title":"Authentication flow","text":"<p>The authentication flow is the same for all providers, but differs depending on whether you want to sign in with the provider's SDK.</p> <ul> <li>Without provider SDK: The application delegates federated sign-in to App Service. This is typically the case with browser apps, which can present the provider's login page to the user. The server code manages the sign-in process, so it's also called server-directed flow or server flow.</li> <li>With provider SDK: The application signs users in to the provider manually and then submits the authentication token to App Service for validation. This is typically the case with browser-less apps, which can't present the provider's sign-in page to the user. The application code manages the sign-in process, so it's also called client-directed flow or client flow. This applies to REST APIs, Azure Functions, JavaScript browser clients, and native mobile apps that sign users in using the provider's SDK.</li> </ul> <p>The following table shows the steps of the authentication flow.</p> Step Without provider SDK With provider SDK Sign user in Redirects client to <code>/.auth/login/&lt;provider&gt;</code>. Client code signs user in directly with provider's SDK and receives an authentication token. For information, see the provider's documentation. Post-authentication Provider redirects client to <code>/.auth/login/&lt;provider&gt;/callback</code>. Client code posts token from provider to <code>/.auth/login/&lt;provider&gt;</code> for validation. Establish authenticated session App Service adds authenticated cookie to response. App Service returns its own authentication token to client code. Serve authenticated content Client includes authentication cookie in subsequent requests (automatically handled by browser). Client code presents authentication token in <code>X-ZUMO-AUTH</code> header (automatically handled by Mobile Apps client SDKs). <p>For client browsers, App Service can automatically direct all unauthenticated users to <code>/.auth/login/&lt;provider&gt;</code>. You can also present users with one or more <code>/.auth/login/&lt;provider&gt;</code> links to sign in to your app using their provider of choice.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#authorization-behavior","title":"Authorization behavior","text":"<p>In the Azure portal, you can configure App Service with many behaviors when an incoming request isn't authenticated.</p> <ul> <li>Allow unauthenticated requests: This option defers authorization of unauthenticated traffic to your application code. For authenticated requests, App Service also passes along authentication information in the HTTP headers. This option provides more flexibility in handling anonymous requests. It lets you present multiple sign-in providers to your users.</li> <li>Require authentication: This option rejects any unauthenticated traffic to your application. This rejection can be a redirect action to one of the configured identity providers. In these cases, a browser client is redirected to <code>/.auth/login/&lt;provider&gt;</code> for the provider you choose. If the anonymous request comes from a native mobile app, the returned response is an <code>HTTP 401 Unauthorized</code>. You can also configure the rejection to be an <code>HTTP 401 Unauthorized</code> or <code>HTTP 403 Forbidden</code> for all requests.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#token-store","title":"Token store","text":"<p>App Service provides a built-in token store, which is a repository of tokens that are associated with the users of your web apps, APIs, or native mobile apps. When you enable authentication with any provider, this token store is immediately available to your app.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2001%20Revision/#logging-and-tracing","title":"Logging and tracing","text":"<p>If you enable application logging, authentication and authorization traces are collected directly in your log files. If you see an authentication error that you didn't expect, you can conveniently find all the details by looking in your existing application logs.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/","title":"Overview","text":"<p>Azure Functions lets you develop serverless applications on Microsoft Azure. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.</p> <p>After completing this module, should be able to:</p> <ul> <li>Explain functional differences between Azure Functions, Azure Logic Apps, and WebJobs</li> <li>Describe Azure Functions hosting plan options</li> <li>Describe how Azure Functions scale to meet business needs</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#comparisons","title":"Comparisons","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#compare-azure-functions-and-azure-logic-apps","title":"Compare Azure Functions and Azure Logic Apps","text":"<p>Both Functions and Logic Apps are Azure Services that enable serverless workloads. Azure Functions is a serverless compute service, whereas Azure Logic Apps is a serverless workflow integration platform. Both can create complex orchestrations. An orchestration is a collection of functions or steps, called actions in Logic Apps, that are executed to accomplish a complex task.</p> <p>For Azure Functions, you develop orchestrations by writing code and using the Durable Functions extension. For Logic Apps, you create orchestrations by using a GUI or editing configuration files.</p> <p>The following table lists some of the key differences between Functions and Logic Apps:</p> Topic Azure Functions Logic Apps Development Code-first (imperative) Designer-first (declarative) Connectivity About a dozen built-in binding types, write code for custom bindings Large collection of connectors, Enterprise Integration Pack for B2B scenarios, build custom connectors Actions Each activity is an Azure function; write code for activity functions Large collection of ready-made actions Monitoring Azure Application Insights Azure portal, Azure Monitor logs Management REST API, Visual Studio Azure portal, REST API, PowerShell, Visual Studio Execution context Runs in Azure, or locally Runs in Azure, locally, or on premises"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#compare-functions-and-webjobs","title":"Compare Functions and WebJobs","text":"<p>Like Azure Functions, Azure App Service WebJobs with the WebJobs SDK is a code-first integration service that is designed for developers. Both are built on Azure App Service and support features such as source control integration, authentication, and monitoring with Application Insights integration.</p> <p>Azure Functions is built on the WebJobs SDK, so it shares many of the same event triggers and connections to other Azure services. Here are some factors to consider when you're choosing between Azure Functions and WebJobs with the WebJobs SDK:</p> Factor Functions WebJobs with WebJobs SDK Serverless app model with automatic scaling Yes No Develop and test in browser Yes No Pay-per-use pricing Yes No Integration with Logic Apps Yes No Trigger events Timer  Azure Storage queues and blobs  Azure Service Bus queues and topics  Azure Cosmos DB  Azure Event Hubs  HTTP/WebHook (GitHub  Slack)  Azure Event Grid Timer  Azure Storage queues and blobs  Azure Service Bus queues and topics  Azure Cosmos DB  Azure Event Hubs  File system Azure Functions offers more developer productivity than Azure App Service WebJobs does. It also offers more options for programming languages, development environments, Azure service integration, and pricing. For most scenarios, it's the best choice."},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#hosting-plans","title":"Hosting Plans","text":"Hosting option Service Availability Container support Consumption plan Azure Functions Generally available (GA) None Flex Consumption plan Azure Functions Preview None Premium plan Azure Functions GA Linux Dedicated plan Azure Functions GA Linux Container Apps Azure Container Apps GA Linux Azure App Service infrastructure facilitates Azure Functions hosting on both Linux and Windows virtual machines. The hosting option you choose dictates the following behaviors: <ul> <li>How your function app is scaled.</li> <li>The resources available to each function app instance.</li> <li>Support for advanced functionality, such as Azure Virtual Network connectivity.</li> <li>Support for Linux containers.</li> </ul> <p>The plan you choose also impacts the costs for running your function code.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#consumption-plan","title":"Consumption plan","text":"<p>The Consumption plan is the default hosting plan. Pay for compute resources only when your functions are running (pay-as-you-go) with automatic scale. On the Consumption plan, instances of the Functions host are dynamically added and removed based on the number of incoming events.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#flex-consumption-plan","title":"Flex Consumption plan","text":"<p>Get high scalability with compute choices, virtual networking, and pay-as-you-go billing. On the Flex Consumption plan, instances of the Functions host are dynamically added and removed based on the configured per instance concurrency and the number of incoming events.</p> <p>You can reduce cold starts by specifying the number of pre-provisioned (always ready) instances. Scales automatically based on demand.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#premium-plan","title":"Premium plan","text":"<p>Automatically scales based on demand using prewarmed workers, which run applications with no delay after being idle, runs on more powerful instances, and connects to virtual networks.</p> <p>Consider the Azure Functions Premium plan in the following situations:</p> <ul> <li>Your function apps run continuously, or nearly continuously.</li> <li>You want more control of your instances and want to deploy multiple function apps on the same plan with event-driven scaling.</li> <li>You have a high number of small executions and a high execution bill, but low GB seconds in the Consumption plan.</li> <li>You need more CPU or memory options than are provided by consumption plans.</li> <li>Your code needs to run longer than the maximum execution time allowed on the Consumption plan.</li> <li>You require virtual network connectivity.</li> <li>You want to provide a custom Linux image in which to run your functions.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#dedicated-plan","title":"Dedicated plan","text":"<p>Run your functions within an App Service plan at regular App Service plan rates. Best for long-running scenarios where Durable Functions can't be used.</p> <p>Consider an App Service plan in the following situations:</p> <ul> <li>You must have fully predictable billing, or you need to manually scale instances.</li> <li>You want to run multiple web apps and function apps on the same plan</li> <li>You need access to larger compute size choices.</li> <li>Full compute isolation and secure network access provided by an App Service Environment (ASE).</li> <li>High memory usage and high scale (ASE).</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#container-apps","title":"Container Apps","text":"<p>Create and deploy containerized function apps in a fully managed environment hosted by Azure Container Apps.</p> <p>Use the Azure Functions programming model to build event-driven, serverless, cloud native function apps. Run your functions alongside other microservices, APIs, websites, and workflows as container-hosted programs.</p> <p>Consider hosting your functions on Container Apps in the following situations:</p> <ul> <li>You want to package custom libraries with your function code to support line-of-business apps.</li> <li>You need to migration code execution from on-premises or legacy apps to cloud native microservices running in containers.</li> <li>You want to avoid the overhead and complexity of managing Kubernetes clusters and dedicated compute.</li> <li>You need the high-end processing power provided by dedicated CPU compute resources for your functions.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#function-app-timeouts","title":"Function App Timeouts","text":"<p>The <code>functionTimeout</code> property in the host.json project file specifies the timeout duration for functions in a function app. This property applies specifically to function executions. After the trigger starts function execution, the function needs to return/respond within the timeout duration.</p> <p>The following table shows the default and maximum values (in minutes) for specific plans:</p> Plan Default Maximum1 Consumption plan 5 10 Flex Consumption plan 30 Unlimited - 3 Premium plan 30 - 2 Unlimited - 3 Dedicated plan 30 - 2 Unlimited - 3 Container Apps 30 - 5 Unlimited - 3 <ol> <li>Regardless of the function app timeout setting, 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request.</li> <li>The default timeout for version 1.x of the Functions runtime is unlimited.</li> <li>Guaranteed for up to 60 minutes. OS and runtime patching, vulnerability patching, and scale in behaviours can still cancel function executions.</li> <li>In a Flex Consumption plan, the host doesn't enforce an execution time limit. However, there are currently no guarantees because the platform might need to terminate your instances during scale-in, deployments, or to apply updates.</li> <li>When the minimum number of replicas is set to zero, the default timeout depends on the specific triggers used in the app.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#scaling-azure-functions","title":"Scaling Azure Functions","text":"Plan Scale out Max # instances Consumption plan Event driven. Scales out automatically, even during periods of high load. Functions infrastructure scales CPU and memory resources by adding more instances based on the number of incoming trigger events. Windows: 200  Linux: 100 Flex Consumption plan Per-function scaling. Event-driven scaling decisions are calculated on a per-function basis, which provides a more deterministic way of scaling the functions in your app. Limited only by total memory usage of all instances across a given region. Premium plan Event driven. Scale out automatically based on the number of events that its functions are triggered on. Windows: 100  Linux: 20-100 Dedicated plan Manual/autoscale 10-30  100 (ASE) Container Apps Event driven. Scale out automatically by adding more instances of the Functions host, based on the number of events that its functions are triggered on. 10-300 <ol> <li>During scale-out, there's currently a limit of 500 instances per subscription per hour for Linux 1. apps on a Consumption plan.</li> <li>In some regions, Linux apps on a Premium plan can scale to 100 instances.</li> <li>For specific limits for the various App Service plan options, see the App Service plan limits.</li> <li>On Container Apps, you can set the maximum number of replicas, which is honoured as long as there's enough cores quota available</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#developing-azure-function-apps","title":"Developing Azure Function Apps","text":"<p>Functions share a few core technical concepts and components, regardless of the language or binding you use.</p> <p>After completing this module, you'll be able to:</p> <ul> <li>Explain the key components of a function</li> <li>Create triggers and bindings to control when a function runs and where the output is directed</li> <li>Connect a function to services in Azure</li> <li>Create a function by using Visual Studio Code and the Azure Functions Core Tools</li> </ul> <p>NOTE: In Functions 2.x all functions in a function app must be authored in the same language. In previous versions of the Azure Functions runtime, this wasn't required.</p> <p>NOTE: Because of limitations on editing function code in the Azure portal, you should develop your functions locally and publish your code project to a function app in Azure. For more information, see Development limitations in the Azure portal</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#local-project-files","title":"Local project files","text":"<p>A Functions project directory contains the following files in the project root folder, regardless of language:</p> <ul> <li><code>host.json</code></li> <li><code>local.settings.json</code></li> <li>Other files in the project depend on your language and specific functions.\\</li> </ul> <p>The <code>host.json</code> metadata file contains configuration options that affect all functions in a function app instance. Other function app configuration options are managed depending on where the function app runs:</p> <ul> <li>Deployed to Azure: in your application settings</li> <li>On your local computer: in the local.settings.json file.</li> </ul> <p>Configurations in <code>host.json</code> related to bindings are applied equally to each function in the function app. You can also override or apply settings per environment using application settings. To learn more, see the host.json reference.</p> <p>The <code>local.settings.json</code> file stores app settings, and settings used by local development tools. Settings in the <code>local.settings.json</code> file are used only when you're running your project locally. When you publish your project to Azure, be sure to also add any required settings to the app settings for the function app.</p> <p>IMPORTANT: Because the <code>local.settings.json</code> may contain secrets, such as connection strings, you should never store it in a remote repository.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#trigger-and-binding-definitions","title":"Trigger and binding definitions","text":"Language Configure triggers and bindings by... C# class library decorating methods and parameters with C# attributes Java decorating methods and parameters with Java annotations JavaScript/PowerShell/Python/TypeScript updating function.json schema"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#binding-direction","title":"Binding direction","text":"<p>All triggers and bindings have a direction property in the function.json file:</p> <ul> <li>For triggers, the direction is always <code>in</code></li> <li>Input and output bindings use <code>in</code> and <code>out</code></li> <li>Some bindings support a special direction <code>inout</code>. If you use <code>inout</code>, only the Advanced editor is available via the Integrate tab in the portal.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#azure-functions-trigger-and-binding-example","title":"Azure Functions trigger and binding example","text":"<p>Suppose you want to write a new row to Azure Table storage whenever a new message appears in Azure Queue storage. This scenario can be implemented using an Azure Queue storage trigger and an Azure Table storage output binding.</p> <p>Here's a function.json file for this scenario.</p> <pre><code>{\n  \"disabled\": false,\n    \"bindings\": [\n        {\n            \"type\": \"queueTrigger\",\n            \"direction\": \"in\",\n            \"name\": \"myQueueItem\",\n            \"queueName\": \"myqueue-items\",\n            \"connection\":\"MyStorageConnectionAppSetting\"\n        },\n        {\n          \"tableName\": \"Person\",\n          \"connection\": \"MyStorageConnectionAppSetting\",\n          \"name\": \"tableBinding\",\n          \"type\": \"table\",\n          \"direction\": \"out\"\n        }\n  ]\n}\n</code></pre> <p>The first element in the <code>bindings</code> array is the Queue storage trigger. The <code>type</code> and <code>direction</code> properties identify the trigger. The <code>name</code> property identifies the function parameter that receives the queue message content. The name of the queue to monitor is in <code>queueName</code>, and the connection string is in the app setting identified by <code>connection</code>.</p> <p>The second element in the <code>bindings</code> array is the Azure Table Storage output binding. The <code>type</code> and <code>direction</code> properties identify the binding. The <code>name</code> property specifies how the function provides the new table row, in this case by using the function return value. The name of the table is in <code>tableName</code>, and the connection string is in the app setting identified by <code>connection</code>.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2002%20Revision/#c-function-example","title":"C# function example","text":"<p>Following is the same example represented in a C# function. The same trigger and binding information, queue and table names, storage accounts, and function parameters for input and output are provided by attributes instead of a function.json file.</p> <pre><code>public static class QueueTriggerTableOutput\n{\n    [FunctionName(\"QueueTriggerTableOutput\")]\n    [return: Table(\"outTable\", Connection = \"MY_TABLE_STORAGE_ACCT_APP_SETTING\")]\n    public static Person Run(\n        [QueueTrigger(\"myqueue-items\", Connection = \"MY_STORAGE_ACCT_APP_SETTING\")]JObject order,\n        ILogger log)\n    {\n        return new Person() {\n                PartitionKey = \"Orders\",\n                RowKey = Guid.NewGuid().ToString(),\n                Name = order[\"Name\"].ToString(),\n                MobileNumber = order[\"MobileNumber\"].ToString() };\n    }\n}\n\npublic class Person\n{\n    public string PartitionKey { get; set; }\n    public string RowKey { get; set; }\n    public string Name { get; set; }\n    public string MobileNumber { get; set; }\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/","title":"Azure 204 - Module 03 Revision","text":"<p>The topics covered in this module include:</p> <ul> <li>Understanding Azure Blob Storage: Features, Types of Storage Accounts, and Access Tiers</li> <li>Understanding Azure Blob Storage: Storage Accounts, Containers, and Blobs</li> <li>Understanding Azure Storage Security and Encryption Features</li> <li>Describe how each of the access tiers is optimized.</li> <li>Create and implement a lifecycle policy.</li> <li>Rehydrate blob data stored in an archive tier.</li> <li>Create an application to create and manipulate data by using the Azure Storage client library for Blob storage.</li> <li>Manage container properties and metadata by using .NET and REST.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#31-explore-azure-blob-storage","title":"3.1 - Explore Azure Blob Storage","text":"<p>Blob storage is designed for:</p> <ul> <li>Serving images or documents directly to a browser.</li> <li>Storing files for distributed access.</li> <li>Streaming video and audio.</li> <li>Writing to log files.</li> <li>Storing data for backup and restore, disaster recovery, and archiving.</li> <li>Storing data for analysis by an on-premises or Azure-hosted service.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#types-of-storage-accounts","title":"Types of storage accounts","text":"<p>Azure Storage offers two performance levels of storage accounts, standard and premium. Each performance level supports different features and has its own pricing model.</p> <ul> <li>Standard: This is the standard general-purpose v2 account and is recommended for most scenarios using Azure Storage.</li> <li>Premium: Premium accounts offer higher performance by using solid-state drives. If you create a premium account you can choose between three account types, block blobs, page blobs, or file shares.</li> </ul> <p>The following table describes the types of storage accounts recommended by Microsoft for most scenarios using Blob storage.</p> Type of storage account Supported storage services Redundancy options Usage Standard general-purpose v2 Blob Storage (including Data Lake Storage), Queue Storage, Table Storage, and Azure Files Locally redundant storage (LRS) / geo-redundant storage (GRS) / read-access geo-redundant storage (RA-GRS)   Zone-redundant storage (ZRS) / geo-zone-redundant storage (GZRS) / read-access geo-zone-redundant storage (RA-GZRS) Standard storage account type for blobs, file shares, queues, and tables. Recommended for most scenarios using Azure Storage. If you want support for network file system (NFS) in Azure Files, use the premium file shares account type. Premium block blobs Blob Storage (including Data Lake Storage) LRS and ZRS Premium storage account type for block blobs and append blobs. Recommended for scenarios with high transaction rates or that use smaller objects or require consistently low storage latency. Premium file shares Azure Files LRS and ZRS Premium storage account type for file shares only. Recommended for enterprise or high-performance scale applications. Premium page blobs Page blobs only LRS and ZRS Premium storage account type for page blobs only. ## Access tiers for block blob data <p>The available access tiers are:</p> <ul> <li>The Hot access tier, which is optimized for frequent access of objects in the storage account. The Hot tier has the highest storage costs, but the lowest access costs. New storage accounts are created in the hot tier by default.</li> <li>The Cool access tier, which is optimized for storing large amounts of data that is infrequently accessed and stored for a minimum of 30 days. The Cool tier has lower storage costs and higher access costs compared to the Hot tier.</li> <li>The Cold access tier, which is optimized for storing data that is infrequently accessed and stored for a minimum of 90 days. The cold tier has lower storage costs and higher access costs compared to the cool tier.</li> <li>The Archive tier, which is available only for individual block blobs. The archive tier is optimized for data that can tolerate several hours of retrieval latency and remains in the Archive tier for a minimum 180 days. The archive tier is the most cost-effective option for storing data, but accessing that data is more expensive than accessing data in the hot or cool tiers.</li> </ul> <p>If there's a change in the usage pattern of your data, you can switch between these access tiers at any time.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#azure-blob-storage-resource-types","title":"Azure Blob storage resource types","text":"<p>Blob storage offers three types of resources:</p> <ul> <li>The storage account.</li> <li>A container in the storage account</li> <li>A blob in a container</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#storage-accounts","title":"Storage accounts","text":"<p>A storage account provides a unique namespace in Azure for your data. Every object that you store in Azure Storage has an address that includes your unique account name. The combination of the account name and the Azure Storage blob endpoint forms the base address for the objects in your storage account.</p> <p>For example, if your storage account is named mystorageaccount, then the default endpoint for Blob storage is:</p> <pre><code>http://mystorageaccount.blob.core.windows.net\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#containers","title":"Containers","text":"<p>A container organizes a set of blobs, similar to a directory in a file system. A storage account can include an unlimited number of containers, and a container can store an unlimited number of blobs.</p> <p>A container name must be a valid DNS name, as it forms part of the unique URI (Uniform resource identifier) used to address the container or its blobs. Follow these rules when naming a container:</p> <ul> <li>Container names can be between 3 and 63 characters long.</li> <li>Container names must start with a letter or number, and can contain only lowercase letters, numbers, and the dash (-) character.</li> <li>Two or more consecutive dash characters aren't permitted in container names.</li> </ul> <p>The URI for a container is similar to:</p> <p>Bash</p> <pre><code>https://myaccount.blob.core.windows.net/mycontainer\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#blobs","title":"Blobs","text":"<p>Azure Storage supports three types of blobs:</p> <ul> <li>Block blobs store text and binary data. Block blobs are made up of blocks of data that can be managed individually. Block blobs can store up to about 190.7 TiB.</li> <li>Append blobs are made up of blocks like block blobs, but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines.</li> <li>Page blobs store random access files up to 8 TB in size. Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.</li> </ul> <p>The URI for a blob is similar to:</p> <p>Bash</p> <pre><code>https://myaccount.blob.core.windows.net/mycontainer/myblob\n</code></pre> <p>or</p> <p>Bash</p> <pre><code>https://myaccount.blob.core.windows.net/mycontainer/myvirtualdirectory/myblob\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#azure-storage-encryption-for-data-at-rest","title":"Azure Storage encryption for data at rest","text":"<p>Azure Storage automatically encrypts your data when persisting it to the cloud. Encryption protects your data and helps you meet your organizational security and compliance commitments. Data in Azure Storage is encrypted and decrypted transparently using 256-bit Advanced Encryption Standard (AES) encryption, one of the strongest block ciphers available, and is Federal Information Processing Standards (FIPS) 140-2 compliant. Azure Storage encryption is similar to BitLocker encryption on Windows.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#encryption-key-management","title":"Encryption key management","text":"<p>If you choose to manage encryption with your own keys, you have two options. You can use either type of key management, or both:</p> <ul> <li>You can specify a customer-managed key to use for encrypting and decrypting data in Blob Storage and in Azure Files.Customer-managed keys must be stored in Azure Key Vault or Azure Key Vault Managed Hardware Security Model (HSM).</li> <li>You can specify a customer-provided key on Blob Storage operations. A client can include an encryption key on a read/write request for granular control over how blob data is encrypted and decrypted.</li> </ul> <p>The following table compares key management options for Azure Storage encryption.</p> Key management parameter Microsoft-managed keys Customer-managed keys Customer-provided keys Encryption/decryption operations Azure Azure Azure Azure Storage services supported All Blob Storage, Azure Files Blob Storage Key storage Microsoft key store Azure Key Vault or Key Vault HSM Customer's own key store Key rotation responsibility Microsoft Customer Customer Key control Microsoft Customer Customer Key scope Account (default), container, or blob Account (default), container, or blob N/A ## Client-side encryption <p>The Azure Blob Storage client libraries for .NET, Java, and Python support encrypting data within client applications before uploading to Azure Storage, and decrypting data while downloading to the client. The Queue Storage client libraries for .NET and Python also support client-side encryption.</p> <p>The Blob Storage and Queue Storage client libraries uses AES in order to encrypt user data. There are two versions of client-side encryption available in the client libraries:</p> <ul> <li>Version 2 uses Galois/Counter Mode (GCM) mode with AES. The Blob Storage and Queue Storage SDKs support client-side encryption with v2.</li> <li>Version 1 uses Cipher Block Chaining (CBC) mode with AES. The Blob Storage, Queue Storage, and Table Storage SDKs support client-side encryption with v1.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#32-manage-the-azure-blob-storage-lifecycle","title":"3.2 - Manage the Azure Blob storage lifecycle","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#access-tiers","title":"Access tiers","text":"<p>Azure storage offers different access tiers, allowing you to store blob object data in the most cost-effective manner. Available access tiers include:</p> <ul> <li>Hot - An online tier optimized for storing data that is accessed frequently.</li> <li>Cool - An online tier optimized for storing data that is infrequently accessed and stored for a minimum of 30 days.</li> <li>Cold tier - An online tier optimized for storing data that is infrequently accessed and stored for a minimum of 90 days. The cold tier has lower storage costs and higher access costs compared to the cool tier.</li> <li>Archive - An offline tier optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.</li> </ul> <p>Data storage limits are set at the account level and not per access tier. You can choose to use all of your limit in one tier or across all three tiers.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#manage-the-data-lifecycle","title":"Manage the data lifecycle","text":"<p>With the lifecycle management policy, you can:</p> <ul> <li>Transition blobs from cool to hot immediately when accessed, to optimize for performance.</li> <li>Transition current versions of a blob, previous versions of a blob, or blob snapshots to a cooler storage tier if these objects aren't accessed or modified for a period of time, to optimize for cost.</li> <li>Delete current versions of a blob, previous versions of a blob, or blob snapshots at the end of their lifecycles.</li> <li>Apply rules to an entire storage account, to select containers, or to a subset of blobs using name prefixes or blob index tags as filters.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#lifecycle-policies-and-management","title":"Lifecycle policies and management","text":"<p>A lifecycle management policy is a collection of rules in a JSON document. Each rule definition within a policy includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names. The action set applies the tier or delete actions to the filtered set of objects:</p> <p>JSON</p> <pre><code>{\n  \"rules\": [\n    {\n      \"name\": \"rule1\",\n      \"enabled\": true,\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    },\n    {\n      \"name\": \"rule2\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    }\n  ]\n}\n</code></pre> <p>A policy is a collection of rules:</p> Parameter name Parameter type Notes <code>rules</code> An array of rule objects At least one rule is required in a policy. You can define up to 100 rules in a policy. <p>Each rule within the policy has several parameters:</p> Parameter name Parameter type Notes Required <code>name</code> String A rule name can include up to 256 alphanumeric characters. Rule name is case-sensitive. It must be unique within a policy. True <code>enabled</code> Boolean An optional boolean to allow a rule to be temporarily disabled. Default value is true. False <code>type</code> An enum value The current valid type is Lifecycle. True <code>definition</code> An object that defines the lifecycle rule Each definition is made up of a filter set and an action set. True ### Rules <p>Each rule definition includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names. The action set applies the tier or delete actions to the filtered set of objects.</p> <p>The following sample rule filters the account to run the actions on objects that exist inside <code>sample-container</code> and start with <code>blob1</code>.</p> <ul> <li>Tier blob to cool tier 30 days after last modification</li> <li>Tier blob to archive tier 90 days after last modification</li> <li>Delete blob 2,555 days (seven years) after last modification</li> <li>Delete blob snapshots 90 days after snapshot creation</li> </ul> <p>JSON</p> <pre><code>{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"sample-rule\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"version\": {\n            \"delete\": {\n              \"daysAfterCreationGreaterThan\": 90\n            }\n          },\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysAfterModificationGreaterThan\": 30\n            },\n            \"tierToArchive\": {\n              \"daysAfterModificationGreaterThan\": 90,\n              \"daysAfterLastTierChangeGreaterThan\": 7\n            },\n            \"delete\": {\n              \"daysAfterModificationGreaterThan\": 2555\n            }\n          }\n        },\n        \"filters\": {\n          \"blobTypes\": [\n            \"blockBlob\"\n          ],\n          \"prefixMatch\": [\n            \"sample-container/blob1\"\n          ]\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#rule-filters","title":"Rule Filters","text":"<p>Filters limit rule actions to a subset of blobs within the storage account. If more than one filter is defined, a logical AND runs on all filters. Filters include:</p> Filter name Type Is Required blobTypes An array of predefined enum values. Yes prefixMatch An array of strings for prefixes to be match. Each rule can define up to 10 prefixes. A prefix string must start with a container name. No blobIndexMatch An array of dictionary values consisting of blob index tag key and value conditions to be matched. Each rule can define up to 10 blob index tag condition. No ### Rule Actions <p>Actions are applied to the filtered blobs when the run condition is met.</p> <p>Lifecycle management supports tiering and deletion of blobs and deletion of blob snapshots. Define at least one action for each rule on blobs or blob snapshots.</p> Action Current Version Snapshot Previous Versions tierToCool Supported for <code>blockBlob</code> Supported Supported tierToCold Supported for <code>blockBlob</code> Supported Supported enableAutoTierToHotFromCool Supported for <code>blockBlob</code> Not supported Not supported tierToArchive Supported for <code>blockBlob</code> Supported Supported delete Supported for <code>blockBlob</code> and <code>appendBlob</code> Supported Supported <p>The run conditions are based on age. Base blobs use the last modified time to track age, and blob snapshots use the snapshot creation time to track age.</p> Action run condition Condition value Description daysAfterModificationGreaterThan Integer value indicating the age in days The condition for base blob actions daysAfterCreationGreaterThan Integer value indicating the age in days The condition for blob snapshot actions daysAfterLastAccessTimeGreaterThan Integer value indicating the age in days The condition for a current version of a blob when access tracking is enabled daysAfterLastTierChangeGreaterThan Integer value indicating the age in days after last blob tier change time The minimum duration in days that a rehydrated blob is kept in hot, cool, or cold tiers before being returned to the archive tier. This condition applies only to <code>tierToArchive</code> actions. ### Azure CLI <p>To add a lifecycle management policy with Azure CLI, write the policy to a JSON file, then call the <code>az storage account management-policy create</code> command to create the policy.</p> <p>Azure CLI</p> <pre><code>az storage account management-policy create \\\n    --account-name &lt;storage-account&gt; \\\n    --policy @policy.json \\\n    --resource-group &lt;resource-group&gt;\n</code></pre> <p>A lifecycle management policy must be read or written in full. Partial updates aren't supported.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#rehydrate-blob-data-from-the-archive-tier","title":"Rehydrate blob data from the archive tier","text":"<p>While a blob is in the archive access tier, it's considered to be offline and can't be read or modified. In order to read or modify data in an archived blob, you must first rehydrate the blob to an online tier, either the hot or cool tier. There are two options for rehydrating a blob that is stored in the archive tier:</p> <ul> <li>Copy an archived blob to an online tier: You can rehydrate an archived blob by copying it to a new blob in the hot or cool tier with the Copy Blob or Copy Blob from URL operation. Microsoft recommends this option for most scenarios.</li> <li>Change a blob's access tier to an online tier: You can rehydrate an archived blob to hot or cool by changing its tier using the Set Blob Tier operation.</li> </ul> <p>Rehydrating a blob from the archive tier can take several hours to complete. Microsoft recommends rehydrating larger blobs for optimal performance. Rehydrating several small blobs concurrently might require extra time.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#rehydration-priority","title":"Rehydration priority","text":"<p>When you rehydrate a blob, you can set the priority for the rehydration operation via the optional <code>x-ms-rehydrate-priority</code> header on a Set Blob Tier or Copy Blob/Copy Blob From URL operation. Rehydration priority options include:</p> <ul> <li>Standard priority: The rehydration request is processed in the order it was received and might take up to 15 hours.</li> <li>High priority: The rehydration request is prioritized over standard priority requests and might complete in under one hour for objects under 10 GB in size.</li> </ul> <p>To check the rehydration priority while the rehydration operation is underway, call Get Blob Properties to return the value of the <code>x-ms-rehydrate-priority</code> header. The rehydration priority property returns either Standard or High.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#copy-an-archived-blob-to-an-online-tier","title":"Copy an archived blob to an online tier","text":"<p>The first option for moving a blob from the archive tier to an online tier is to copy the archived blob to a new destination blob that is in either the hot or cool tier. You can use the Copy Blob operation to copy the blob. When you copy an archived blob to a new blob in an online tier, the source blob remains unmodified in the archive tier. You must copy the archived blob to a new blob with a different name or to a different container. You can't overwrite the source blob by copying to the same blob.</p> <p>Rehydrating an archived blob by copying it to an online destination tier is supported within the same storage account only for service versions earlier than 2021-02-12. Beginning with service version 2021-02-12, you can rehydrate an archived blob by copying it to a different storage account, as long as the destination account is in the same region as the source account.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#change-a-blobs-access-tier-to-an-online-tier","title":"Change a blob's access tier to an online tier","text":"<p>The second option for rehydrating a blob from the archive tier to an online tier is to change the blob's tier by calling Set Blob Tier. With this operation, you can change the tier of the archived blob to either hot or cool.</p> <p>Once a Set Blob Tier request is initiated, it can't be canceled. During the rehydration operation, the blob's access tier setting continues to show as archived until the rehydration process is complete.</p> <p>To learn how to rehydrate a blob by changing its tier to an online tier, see Rehydrate a blob by changing its tier.</p> <pre><code>Caution\n\nChanging a blob's tier doesn't affect its last modified time. If there is a lifecycle management policy in effect for the storage account, then rehydrating a blob with **Set Blob Tier** can result in a scenario where the lifecycle policy moves the blob back to the archive tier after rehydration because the last modified time is beyond the threshold set for the policy.\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#33-work-with-azure-blob-storage","title":"3.3  - Work with azure blob storage","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#explore-azure-blob-storage-client-library","title":"Explore Azure Blob storage client library","text":"<p>The Azure Storage client libraries for .NET offer a convenient interface for making calls to Azure Storage. The latest version of the Azure Storage client library is version 12.x. Microsoft recommends using version 12.x for new applications.</p> <p>The following table lists the basic classes, along with a brief description:</p> Class Description <code>BlobClient</code> The <code>BlobClient</code> allows you to manipulate Azure Storage blobs. <code>BlobClientOptions</code> Provides the client configuration options for connecting to Azure Blob Storage. <code>BlobContainerClient</code> The BlobContainerClient allows you to manipulate Azure Storage containers and their blobs. <code>BlobServiceClient</code> The BlobServiceClient allows you to manipulate Azure Storage service resources and blob containers. The storage account provides the top-level namespace for the Blob service. <code>BlobUriBuilder</code> The BlobUriBuilder class provides a convenient way to modify the contents of a Uri instance to point to different Azure Storage resources like an account, container, or blob. <p>The following packages contain the classes used to work with Blob Storage data resources:</p> <ul> <li>Azure.Storage.Blobs: Contains the primary classes (client objects) that you can use to operate on the service, containers, and blobs.</li> <li>Azure.Storage.Blobs.Specialized: Contains classes that you can use to perform operations specific to a blob type, such as block blobs.</li> <li>Azure.Storage.Blobs.Models: All other utility classes, structures, and enumeration types.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#create-a-blobserviceclient-object","title":"Create a BlobServiceClient object","text":"<p>An authorized <code>BlobServiceClient</code> object allows your app to interact with resources at the storage account level. <code>BlobServiceClient</code> provides methods to retrieve and configure account properties, as well as list, create, and delete containers within the storage account. This client object is the starting point for interacting with resources in the storage account.</p> <p>The following example shows how to create a <code>BlobServiceClient</code> object:</p> <pre><code>using Azure.Identity;\nusing Azure.Storage.Blobs;\n\npublic BlobServiceClient GetBlobServiceClient(string accountName)\n{\n    BlobServiceClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net\"),\n        new DefaultAzureCredential());\n\n    return client;\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#create-a-blobcontainerclient-object","title":"Create a BlobContainerClient object","text":"<p>You can use a <code>BlobServiceClient</code> object to create a new <code>BlobContainerClient</code> object. A <code>BlobContainerClient</code> object allows you to interact with a specific container resource. <code>BlobContainerClient</code> provides methods to create, delete, or configure a container, and includes methods to list, upload, and delete the blobs within it.</p> <p>The following example shows how to create a container client from a <code>BlobServiceClient</code> object to interact with a specific container resource:</p> <pre><code>public BlobContainerClient GetBlobContainerClient(\n    BlobServiceClient blobServiceClient,\n    string containerName)\n{\n    // Create the container client using the service client object\n    BlobContainerClient client = blobServiceClient.GetBlobContainerClient(containerName);\n    return client;\n}\n</code></pre> <p>If your work is narrowly scoped to a single container, you might choose to create a <code>BlobContainerClient</code> object directly without using <code>BlobServiceClient</code>.</p> <pre><code>public BlobContainerClient GetBlobContainerClient(\n    string accountName,\n    string containerName,\n    BlobClientOptions clientOptions)\n{\n    // Append the container name to the end of the URI\n    BlobContainerClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net/{containerName}\"),\n        new DefaultAzureCredential(),\n        clientOptions);\n\n    return client;\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#create-a-blobclient-object","title":"Create a BlobClient object","text":"<p>To interact with a specific blob resource, create a <code>BlobClient</code> object from a service client or container client. A <code>BlobClient</code> object allows you to interact with a specific blob resource.</p> <p>The following example shows how to create a blob client to interact with a specific blob resource:</p> <pre><code>public BlobClient GetBlobClient(\n    BlobServiceClient blobServiceClient,\n    string containerName,\n    string blobName)\n{\n    BlobClient client =\n        blobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(blobName);\n    return client;\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#az-cli-commands-to-create-these-resources","title":"AZ CLI Commands to create these resources","text":"<pre><code>az login\n</code></pre> <pre><code>az group create --location &lt;myLocation&gt; --name az204-blob-rg\n</code></pre> <pre><code>az storage account create --resource-group az204-blob-rg --name &lt;myStorageAcct&gt; --location &lt;myLocation&gt; --sku Standard_LRS\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#set-and-retrieve-properties-and-metadata-for-blob-resources-by-using-rest","title":"Set and retrieve properties and metadata for blob resources by using REST","text":"<p>Containers and blobs support custom metadata, represented as HTTP headers. Metadata headers can be set on a request that creates a new container or blob resource, or on a request that explicitly creates a property on an existing resource.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#metadata-header-format","title":"Metadata header format","text":"<p>Metadata headers are name/value pairs. The format for the header is:</p> <pre><code>x-ms-meta-name:string-value  \n</code></pre> <p>Beginning with version 2009-09-19, metadata names must adhere to the naming rules for C# identifiers.</p> <p>Names are case-insensitive. Metadata names preserve the case with which they were created, but are case-insensitive when set or read. If two or more metadata headers with the same name are submitted for a resource, the Blob service returns status code <code>400 (Bad Request)</code>.</p> <p>The metadata consists of name/value pairs. The total size of all metadata pairs can be up to 8 KB in size.</p> <p>Metadata name/value pairs are valid HTTP headers, and so they adhere to all restrictions governing HTTP headers.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#operations-on-metadata","title":"Operations on metadata","text":"<p>Metadata on a blob or container resource can be retrieved or set directly, without returning or altering the content of the resource.</p> <p>Metadata values can only be read or written in full; partial updates aren't supported. Setting metadata on a resource overwrites any existing metadata values for that resource.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#retrieving-properties-and-metadata","title":"Retrieving properties and metadata","text":"<p>The GET and HEAD operations both retrieve metadata headers for the specified container or blob. These operations return headers only; they don't return a response body. The URI syntax for retrieving metadata headers on a container is as follows:</p> <pre><code>GET/HEAD https://myaccount.blob.core.windows.net/mycontainer?restype=container  \n</code></pre> <p>The URI syntax for retrieving metadata headers on a blob is as follows:</p> <pre><code>GET/HEAD https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#setting-metadata-headers","title":"Setting Metadata Headers","text":"<p>The PUT operation sets metadata headers on the specified container or blob, overwriting any existing metadata on the resource. Calling PUT without any headers on Sthe request clears all existing metadata on the resource.</p> <p>The URI syntax for setting metadata headers on a container is as follows:</p> <pre><code>PUT https://myaccount.blob.core.windows.net/mycontainer?comp=metadata&amp;restype=container\n</code></pre> <p>The URI syntax for setting metadata headers on a blob is as follows:</p> <pre><code>PUT https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2003%20Revision/#standard-http-properties-for-containers-and-blobs","title":"Standard HTTP properties for containers and blobs","text":"<p>Containers and blobs also support certain standard HTTP properties. Properties and metadata are both represented as standard HTTP headers; the difference between them is in the naming of the headers. Metadata headers are named with the header prefix <code>x-ms-meta-</code> and a custom name. Property headers use standard HTTP header names, as specified in the Header Field Definitions section 14 of the HTTP/1.1 protocol specification.</p> <p>The standard HTTP headers supported on containers include:</p> <ul> <li><code>ETag</code></li> <li><code>Last-Modified</code></li> </ul> <p>The standard HTTP headers supported on blobs include:</p> <ul> <li><code>ETag</code></li> <li><code>Last-Modified</code></li> <li><code>Content-Length</code></li> <li><code>Content-Type</code></li> <li><code>Content-MD5</code></li> <li><code>Content-Encoding</code></li> <li><code>Content-Language</code></li> <li><code>Cache-Control</code></li> <li><code>Origin</code></li> <li><code>Range</code></li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/","title":"Azure 204 - Module 04 Revision","text":"<p>Azure Cosmos DB is a globally distributed database system that allows you to read and write data from the local replicas of your database and it transparently replicates the data to all the regions associated with your Cosmos account.</p> <p>After completing this module, you'll be able to:</p> <ul> <li>Identify the key benefits provided by Azure Cosmos DB</li> <li>Describe the elements in an Azure Cosmos DB account and how they're organized</li> <li>Explain the different consistency levels and choose the correct one for your project</li> <li>Explore the APIs supported in Azure Cosmos DB and choose the appropriate API for your solution</li> <li>Describe how request units impact costs</li> <li>Create Azure Cosmos DB resources by using the Azure portal.</li> <li>Identify classes and methods used to create resources</li> <li>Create resources by using the Azure Cosmos DB .NET v3 SDK</li> <li>Write stored procedures, triggers, and user-defined functions by using JavaScript</li> <li>Implement change feed notifications</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#41-explore-azure-cosmos-db","title":"4.1 - Explore Azure Cosmos DB","text":"<p>Azure Cosmos DB is a fully managed NoSQL database designed to provide low latency, elastic scalability of throughput, well-defined semantics for data consistency, and high availability.</p> <p>You can configure your databases to be globally distributed and available in any of the Azure regions. To lower the latency, place the data close to where your users are. Choosing the required regions depends on the global reach of your application and where your users are located.</p> <p>With Azure Cosmos DB, you can add or remove the regions associated with your account at any time. Your application doesn't need to be paused or redeployed to add or remove a region.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#key-benefits-of-global-distribution","title":"Key benefits of global distribution","text":"<p>With its novel multi-master replication protocol, every region supports both writes and reads. The multi-master capability also enables:</p> <ul> <li>Unlimited elastic write and read scalability.</li> <li>99.999% read and write availability all around the world.</li> <li>Guaranteed reads and writes served in less than 10 milliseconds at the 99th percentile.</li> </ul> <p>Your application can perform near real-time reads and writes against all the regions you chose for your database. Azure Cosmos DB internally handles the data replication between regions with consistency level guarantees of the level you selected.</p> <p>Running a database in multiple regions worldwide increases the availability of a database. If one region is unavailable, other regions automatically handle application requests. Azure Cosmos DB offers 99.999% read and write availability for multi-region databases.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#elements-in-an-azure-cosmos-db-account","title":"Elements in an Azure Cosmos DB account","text":"<p>Currently, you can create a maximum of 50 Azure Cosmos DB accounts under an Azure subscription (can be increased via support request). After you create an account under your Azure subscription, you can manage the data in your account by creating databases, containers, and items.</p> <p>The following image shows the hierarchy of different entities in an Azure Cosmos DB account:</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#azure-cosmos-db-databases","title":"Azure Cosmos DB databases","text":"<p>You can create one or multiple Azure Cosmos DB databases under your account. A database is analogous to a namespace. A database is the unit of management for a set of Azure Cosmos DB containers.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#azure-cosmos-db-containers","title":"Azure Cosmos DB containers","text":"<p>An Azure Cosmos DB container is where data is stored. Unlike most relational databases, which scale up with larger sizes of virtual machines, Azure Cosmos DB scales out.</p> <p>Data is stored on one or more servers called partitions. To increase partitions, you increase throughput, or they grow automatically as storage increases. This relationship provides a virtually unlimited amount of throughput and storage for a container.</p> <p>When you create a container, you need to supply a partition key. The partition key is a property that you select from your items to help Azure Cosmos DB distribute the data efficiently across partitions. Azure Cosmos DB uses the value of this property to route data to the appropriate partition to be written, updated, or deleted. You can also use the partition key in the <code>WHERE</code> clause in queries for efficient data retrieval.</p> <p>The underlying storage mechanism for data in Azure Cosmos DB is called a physical partition. Physical partitions can have a throughput amount up to 10,000 Request Units per second, and they can store up to 50 GB of data. Azure Cosmos DB abstracts this partitioning concept with a logical partition, which can store up to 20 GB of data.</p> <p>When you create a container, you configure throughput in one of the following modes:</p> <ul> <li>Dedicated throughput: The throughput on a container is exclusively reserved for that container. There are two types of dedicated throughput: standard and autoscale.</li> <li>Shared throughput: Throughput is specified at the database level and then shared with up to 25 containers within the database. Sharing of throughput excludes containers that are configured with their own dedicated throughput.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#azure-cosmos-db-items","title":"Azure Cosmos DB items","text":"<p>Depending on which API you use, individual data entities can be represented in various ways:</p> Azure Cosmos DB entity API for NoSQL API for Cassandra API for MongoDB API for Gremlin API for Table Azure Cosmos DB item Item Row Document Node or edge Item"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#consistency-levels","title":"Consistency Levels","text":"<p>Azure Cosmos DB approaches data consistency as a spectrum of choices instead of two extremes. Strong consistency and eventual consistency are at the ends of the spectrum, but there are many consistency choices along the spectrum. Developers can use these options to make precise choices and granular tradeoffs with respect to high availability and performance.</p> <p>Azure Cosmos DB offers five well-defined levels. From strongest to weakest, the levels are:</p> <ul> <li>Strong</li> <li>Bounded staleness</li> <li>Session</li> <li>Consistent prefix</li> <li>Eventual</li> </ul> <p>Each level provides availability and performance tradeoffs. The following image shows the different consistency levels as a spectrum.</p> <p></p> <p>The consistency levels are region-agnostic and are guaranteed for all operations, regardless of:</p> <ul> <li>The region where the reads and writes are served</li> <li>The number of regions associated with your Azure Cosmos DB account</li> <li>Whether your account is configured with a single or multiple write regions.</li> </ul> <p>You can configure the default consistency level on your Azure Cosmos DB account at any time. The default consistency level configured on your account applies to all Azure Cosmos DB databases and containers under that account. All reads and queries issued against a container or a database use the specified consistency level by default.</p> <p>Read consistency applies to a single read operation scoped within a logical partition. The read operation can be issued by a remote client or a stored procedure.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#strong-consistency","title":"Strong consistency","text":"<p>Strong consistency offers a linearizability guarantee. Linearizability refers to serving requests concurrently. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#bounded-staleness-consistency","title":"Bounded staleness consistency","text":"<p>In bounded staleness consistency, the lag of data between any two regions is always less than a specified amount. The amount can be \"K\" versions (that is, \"updates\") of an item or by \"T\" time intervals, whichever is reached first. In other words, when you choose bounded staleness, the maximum \"staleness\" of the data in any region can be configured in two ways:</p> <ul> <li>The number of versions (K) of the item</li> <li>The time interval (T) reads might lag behind the writes</li> </ul> <p>Bounded Staleness is beneficial primarily to single-region write accounts with two or more regions. If the data lag in a region (determined per physical partition) exceeds the configured staleness value, writes for that partition are throttled until staleness is back within the configured upper bound.</p> <p>For a single-region account, Bounded Staleness provides the same write consistency guarantees as Session and Eventual Consistency. With Bounded Staleness, data is replicated to a local majority (three replicas in a four replica set) in the single region.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#session-consistency","title":"Session consistency","text":"<p>In session consistency, within a single client session, reads are guaranteed to honor the read-your-writes, and write-follows-reads guarantees. This guarantee assumes a single \u201cwriter\u201d session or sharing the session token for multiple writers.</p> <p>Like all consistency levels weaker than Strong, writes are replicated to a minimum of three replicas (in a four replica set) in the local region, with asynchronous replication to all other regions.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#consistent-prefix-consistency","title":"Consistent prefix consistency","text":"<p>In consistent prefix, updates made as single document writes see eventual consistency. Updates made as a batch within a transaction, are returned consistent to the transaction in which they were committed. Write operations within a transaction of multiple documents are always visible together.</p> <p>Assume two write operations are performed on documents Doc 1 and Doc 2, within transactions T1 and T2. When client does a read in any replica, the user sees either \u201cDoc 1 v1 and Doc 2 v1\u201d or \u201cDoc 1 v2 and Doc 2 v2\u201d, but never \u201cDoc 1 v1 and Doc 2 v2\u201d or \u201cDoc 1 v2 and Doc 2 v1\u201d for the same read or query operation.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#eventual-consistency","title":"Eventual consistency","text":"<p>In eventual consistency, there's no ordering guarantee for reads. In the absence of any further writes, the replicas eventually converge.</p> <p>Eventual consistency is the weakest form of consistency because a client might read the values that are older than the ones it read before. Eventual consistency is ideal where the application doesn't require any ordering guarantees. Examples include count of Retweets, Likes, or nonthreaded comments.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#reference-screenshot","title":"Reference Screenshot","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#cosmos-apis","title":"Cosmos APIs","text":"<p>Azure Cosmos DB offers multiple database APIs, which include NoSQL, MongoDB, PostgreSQL, Cassandra, Gremlin, and Table. By using these APIs, you can model real world data using documents, key-value, graph, and column-family data models. These APIs allow your applications to treat Azure Cosmos DB as if it were various other databases technologies, without the overhead of management, and scaling approaches. Azure Cosmos DB helps you to use the ecosystems, tools, and skills you already have for data modeling and querying with its various APIs.</p> <p>### Considerations when choosing an API\\</p> <p>API for NoSQL is native to Azure Cosmos DB.</p> <p>API for MongoDB, PostgreSQL, Cassandra, Gremlin, and Table implement the wire protocol of open-source database engines. These APIs are best suited if the following conditions are true:</p> <ul> <li>If you have existing MongoDB, PostgreSQL Cassandra, or Gremlin applications</li> <li>If you don't want to rewrite your entire data access layer</li> <li>If you want to use the open-source developer ecosystem, client-drivers, expertise, and resources for your database</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#api-for-nosql","title":"API for NoSQL","text":"<p>The Azure Cosmos DB API for NoSQL stores data in document format. It offers the best end-to-end experience as we have full control over the interface, service, and the SDK client libraries. Any new feature that is rolled out to Azure Cosmos DB is first available on API for NoSQL accounts. NoSQL accounts provide support for querying items using the Structured Query Language (SQL) syntax.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#api-for-mongodb","title":"API for MongoDB","text":"<p>The Azure Cosmos DB API for MongoDB stores data in a document structure, via BSON format. It's compatible with MongoDB wire protocol; however, it doesn't use any native MongoDB related code. The API for MongoDB is a great choice if you want to use the broader MongoDB ecosystem and skills, without compromising on using Azure Cosmos DB features.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#api-for-postgresql","title":"API for PostgreSQL","text":"<p>Azure Cosmos DB for PostgreSQL is a managed service for running PostgreSQL at any scale, with the Citus open source superpower of distributed tables. It stores data either on a single node, or distributed in a multi-node configuration.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#api-for-apache-cassandra","title":"API for Apache Cassandra","text":"<p>The Azure Cosmos DB API for Cassandra stores data in column-oriented schema. Apache Cassandra offers a highly distributed, horizontally scaling approach to storing large volumes of data while offering a flexible approach to a column-oriented schema. API for Cassandra in Azure Cosmos DB aligns with this philosophy to approaching distributed NoSQL databases. This API for Cassandra is wire protocol compatible with native Apache Cassandra.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#api-for-apache-gremlin","title":"API for Apache Gremlin","text":"<p>The Azure Cosmos DB API for Gremlin allows users to make graph queries and stores data as edges and vertices.</p> <p>Use the API for Gremlin for scenarios:</p> <ul> <li>Involving dynamic data</li> <li>Involving data with complex relations</li> <li>Involving data that is too complex to be modeled with relational databases</li> <li>If you want to use the existing Gremlin ecosystem and skills</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#api-for-table","title":"API for Table","text":"<p>The Azure Cosmos DB API for Table stores data in key/value format. If you're currently using Azure Table storage, you might see some limitations in latency, scaling, throughput, global distribution, index management, low query performance. API for Table overcomes these limitations and the recommendation is to migrate your app if you want to use the benefits of Azure Cosmos DB. API for Table only supports OLTP scenarios.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#request-units","title":"Request units","text":"<p>With Azure Cosmos DB, you pay for the throughput you provision and the storage you consume on an hourly basis. Throughput must be provisioned to ensure that sufficient system resources are available for your Azure Cosmos database always.</p> <p>The cost of all database operations is normalized in Azure Cosmos DB and expressed by request units (or RUs, for short). A request unit represents the system resources such as CPU, IOPS, and memory that are required to perform the database operations supported by Azure Cosmos DB.</p> <p>The cost to do a point read, which is fetching a single item by its ID and partition key value, for a 1-KB item is 1RU. All other database operations are similarly assigned a cost using RUs. No matter which API you use to interact with your Azure Cosmos container, costs are measured by RUs. Whether the database operation is a write, point read, or query, costs are measured in RUs.</p> <p>The following image shows the high-level idea of RUs:</p> <p></p> <p>The type of Azure Cosmos DB account you're using determines the way consumed RUs get charged. There are three modes in which you can create an account:</p> <ul> <li>Provisioned throughput mode: In this mode, you provision the number of RUs for your application on a per-second basis in increments of 100 RUs per second. To scale the provisioned throughput for your application, you can increase or decrease the number of RUs at any time in increments or decrements of 100 RUs. You can make your changes either programmatically or by using the Azure portal. You can provision throughput at container and database granularity level.</li> <li>Serverless mode: In this mode, you don't have to provision any throughput when creating resources in your Azure Cosmos DB account. At the end of your billing period, you get billed for the number of request units that have been consumed by your database operations.</li> <li>Autoscale mode: In this mode, you can automatically and instantly scale the throughput (RU/s) of your database or container based on its usage. This scaling operation doesn't affect the availability, latency, throughput, or performance of the workload. This mode is well suited for mission-critical workloads that have variable or unpredictable traffic patterns, and require SLAs on high performance and scale.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#42-work-with-cosmosdb","title":"4.2 - Work with CosmosDB","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#cosmosclient","title":"CosmosClient","text":"<p>Creates a new <code>CosmosClient</code> with a connection string. <code>CosmosClient</code> is thread-safe. The recommendation is to maintain a single instance of <code>CosmosClient</code> per lifetime of the application that enables efficient connection management and performance.</p> <pre><code>CosmosClient client = new CosmosClient(endpoint, key);\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#database-examples","title":"Database examples","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#create-a-database","title":"Create a database","text":"<p>The <code>CosmosClient.CreateDatabaseAsync</code> method throws an exception if a database with the same name already exists.</p> <pre><code>// New instance of Database class referencing the server-side database\nDatabase database1 = await client.CreateDatabaseAsync(\n    id: \"adventureworks-1\"\n);\n</code></pre> <p>The <code>CosmosClient.CreateDatabaseIfNotExistsAsync</code> checks if a database exists, and if it doesn't, creates it. Only the database <code>id</code> is used to verify if there's an existing database.</p> <pre><code>// New instance of Database class referencing the server-side database\nDatabase database2 = await client.CreateDatabaseIfNotExistsAsync(\n    id: \"adventureworks-2\"\n);\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#read-a-database-by-id","title":"Read a database by ID","text":"<p>Reads a database from the Azure Cosmos DB service as an asynchronous operation.</p> <pre><code>DatabaseResponse readResponse = await database.ReadAsync();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#delete-a-database","title":"Delete a database","text":"<p>Delete a Database as an asynchronous operation.</p> <pre><code>await database.DeleteAsync();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#container-examples","title":"Container examples","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#create-a-container","title":"Create a container","text":"<p>The <code>Database.CreateContainerIfNotExistsAsync</code> method checks if a container exists, and if it doesn't, it creates it. Only the container <code>id</code> is used to verify if there's an existing container.</p> <pre><code>// Set throughput to the minimum value of 400 RU/s\nContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(\n    id: containerId,\n    partitionKeyPath: partitionKey,\n    throughput: 400);\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#get-a-container-by-id","title":"Get a container by ID","text":"<pre><code>Container container = database.GetContainer(containerId);\nContainerProperties containerProperties = await container.ReadContainerAsync();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#delete-a-container","title":"Delete a container","text":"<p>Delete a Container as an asynchronous operation.</p> <pre><code>await database.GetContainer(containerId).DeleteContainerAsync();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#item-examples","title":"Item examples","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#create-an-item","title":"Create an item","text":"<p>Use the <code>Container.CreateItemAsync</code> method to create an item. The method requires a JSON serializable object that must contain an <code>id</code> property, and a <code>partitionKey</code>.</p> <pre><code>ItemResponse&lt;SalesOrder&gt; response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#read-an-item","title":"Read an item","text":"<p>Use the <code>Container.ReadItemAsync</code> method to read an item. The method requires type to serialize the item to along with an <code>id</code> property, and a <code>partitionKey</code>.</p> <pre><code>string id = \"[id]\";\nstring accountNumber = \"[partition-key]\";\nItemResponse&lt;SalesOrder&gt; response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#query-an-item","title":"Query an item","text":"<p>The <code>Container.GetItemQueryIterator</code> method creates a query for items under a container in an Azure Cosmos database using a SQL statement with parameterized values. It returns a <code>FeedIterator</code>.</p> <pre><code>QueryDefinition query = new QueryDefinition(\n    \"select * from sales s where s.AccountNumber = @AccountInput \")\n    .WithParameter(\"@AccountInput\", \"Account1\");\n\nFeedIterator&lt;SalesOrder&gt; resultSet = container.GetItemQueryIterator&lt;SalesOrder&gt;(\n    query,\n    requestOptions: new QueryRequestOptions()\n    {\n        PartitionKey = new PartitionKey(\"Account1\"),\n        MaxItemCount = 1\n    });\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#az-commands-for-cosmosdb","title":"AZ Commands for CosmosDB","text":"<pre><code>az group create --location &lt;myLocation&gt; --name az204-cosmos-rg\n</code></pre> <pre><code>az cosmosdb create --name &lt;myCosmosDBacct&gt; --resource-group az204-cosmos-rg\n</code></pre> <pre><code># Retrieve the primary key\naz cosmosdb keys list --name &lt;myCosmosDBacct&gt; --resource-group az204-cosmos-rg\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#stored-procedures","title":"Stored procedures","text":"<p>Azure Cosmos DB provides language-integrated, transactional execution of JavaScript that lets you write stored procedures, triggers, and user-defined functions (UDFs). To call a stored procedure, trigger, or user-defined function, you need to register it. For more information, see How to work with stored procedures, triggers, user-defined functions in Azure Cosmos DB.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#writing-stored-procedures","title":"Writing stored procedures","text":"<p>Stored procedures can create, update, read, query, and delete items inside an Azure Cosmos container. Stored procedures are registered per collection, and can operate on any document or an attachment present in that collection.</p> <p>Here's a simple stored procedure that returns a \"Hello World\" response.</p> <pre><code>var helloWorldStoredProc = {\n    id: \"helloWorld\",\n    serverScript: function () {\n        var context = getContext();\n        var response = context.getResponse();\n\n        response.setBody(\"Hello, World\");\n    }\n}\n</code></pre> <p>The context object provides access to all operations that can be performed in Azure Cosmos DB, and access to the request and response objects. In this case, you use the response object to set the body of the response to be sent back to the client.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#create-an-item-using-stored-procedure","title":"Create an item using stored procedure","text":"<p>When you create an item by using a stored procedure, the item is inserted into the Azure Cosmos DB container and an ID for the newly created item is returned. Creating an item is an asynchronous operation and depends on the JavaScript callback functions. The callback function has two parameters: one for the error object in case the operation fails, and another for a return value, in this case, the created object. Inside the callback, you can either handle the exception or throw an error. If a callback isn't provided and there's an error, the Azure Cosmos DB runtime throws an error.</p> <p>The stored procedure also includes a parameter to set the description as a boolean value. When the parameter is set to true and the description is missing, the stored procedure throws an exception. Otherwise, the rest of the stored procedure continues to run.</p> <p>This stored procedure takes as input <code>documentToCreate</code>, the body of a document to be created in the current collection. All such operations are asynchronous and depend on JavaScript function callbacks.</p> <pre><code>var createDocumentStoredProc = {\n    id: \"createMyDocument\",\n    body: function createMyDocument(documentToCreate) {\n        var context = getContext();\n        var collection = context.getCollection();\n        var accepted = collection.createDocument(collection.getSelfLink(),\n              documentToCreate,\n              function (err, documentCreated) {\n                  if (err) throw new Error('Error' + err.message);\n                  context.getResponse().setBody(documentCreated.id)\n              });\n        if (!accepted) return;\n    }\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#arrays-as-input-parameters-for-stored-procedures","title":"Arrays as input parameters for stored procedures","text":"<p>When defining a stored procedure in the Azure portal, input parameters are always sent as a string to the stored procedure. Even if you pass an array of strings as an input, the array is converted to string and sent to the stored procedure. To work around this, you can define a function within your stored procedure to parse the string as an array. The following code shows how to parse a string input parameter as an array:</p> <pre><code>function sample(arr) {\n    if (typeof arr === \"string\") arr = JSON.parse(arr);\n\n    arr.forEach(function(a) {\n        // do something here\n        console.log(a);\n    });\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#bounded-execution","title":"Bounded execution","text":"<p>All Azure Cosmos DB operations must complete within a limited amount of time. Stored procedures have a limited amount of time to run on the server. All collection functions return a Boolean value that represents whether that operation completes or not.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#transactions-within-stored-procedures","title":"Transactions within stored procedures","text":"<p>You can implement transactions on items within a container by using a stored procedure. JavaScript functions can implement a continuation-based model to batch or resume execution. The continuation value can be any value of your choice and your applications can then use this value to resume a transaction from a new starting point. The following diagram depicts how the transaction continuation model can be used to repeat a server-side function until the function finishes its entire processing workload.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#create-triggers-and-user-defined-functions","title":"Create triggers and user-defined functions","text":"<p>Azure Cosmos DB supports pretriggers and post-triggers. Pretriggers are executed before modifying a database item and post-triggers are executed after modifying a database item. Triggers aren't automatically executed. They must be specified for each database operation where you want them to execute. After you define a trigger, you should register it by using the Azure Cosmos DB SDKs.</p> <p>For examples of how to register and call a trigger, see pretriggers and post-triggers.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#pretriggers","title":"Pretriggers","text":"<p>The following example shows how a pretrigger is used to validate the properties of an Azure Cosmos item that is being created. It adds a timestamp property to a newly added item if it doesn't contain one.</p> <pre><code>function validateToDoItemTimestamp() {\n    var context = getContext();\n    var request = context.getRequest();\n\n    // item to be created in the current operation\n    var itemToCreate = request.getBody();\n\n    // validate properties\n    if (!(\"timestamp\" in itemToCreate)) {\n        var ts = new Date();\n        itemToCreate[\"timestamp\"] = ts.getTime();\n    }\n\n    // update the item that will be created\n    request.setBody(itemToCreate);\n}\n</code></pre> <p>Pretriggers can't have any input parameters. The request object in the trigger is used to manipulate the request message associated with the operation. In the previous example, the pretrigger is run when creating an Azure Cosmos item and the request message body contains the item to be created in JSON format.</p> <p>When triggers are registered, you can specify the operations that it can run with. This trigger should be created with a <code>TriggerOperation</code> value of <code>TriggerOperation.Create</code>, using the trigger in a replace operation isn't permitted.</p> <p>For examples of how to register and call a pretrigger, visit the pretriggers article.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#post-triggers","title":"Post-triggers","text":"<p>The following example shows a post-trigger. This trigger queries for the metadata item and updates it with details about the newly created item.</p> <pre><code>function updateMetadata() {\nvar context = getContext();\nvar container = context.getCollection();\nvar response = context.getResponse();\n\n// item that was created\nvar createdItem = response.getBody();\n\n// query for metadata document\nvar filterQuery = 'SELECT * FROM root r WHERE r.id = \"_metadata\"';\nvar accept = container.queryDocuments(container.getSelfLink(), filterQuery,\n    updateMetadataCallback);\nif(!accept) throw \"Unable to update metadata, abort\";\n\nfunction updateMetadataCallback(err, items, responseOptions) {\n    if(err) throw new Error(\"Error\" + err.message);\n        if(items.length != 1) throw 'Unable to find metadata document';\n\n        var metadataItem = items[0];\n\n        // update metadata\n        metadataItem.createdItems += 1;\n        metadataItem.createdNames += \" \" + createdItem.id;\n        var accept = container.replaceDocument(metadataItem._self,\n            metadataItem, function(err, itemReplaced) {\n                    if(err) throw \"Unable to update metadata, abort\";\n            });\n        if(!accept) throw \"Unable to update metadata, abort\";\n        return;\n    }\n}\n</code></pre> <p>One thing that is important to note is the transactional execution of triggers in Azure Cosmos DB. The post-trigger runs as part of the same transaction for the underlying item itself. An exception during the post-trigger execution fails the whole transaction. Anything committed is rolled back and an exception returned.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#user-defined-functions","title":"User-defined functions","text":"<p>The following sample creates a UDF to calculate income tax for various income brackets. This user-defined function would then be used inside a query. For the purposes of this example assume there's a container called \"Incomes\" with properties as follows:</p> <pre><code>{\n   \"name\": \"User One\",\n   \"country\": \"USA\",\n   \"income\": 70000\n}\n</code></pre> <p>The following code sample is a function definition to calculate income tax for various income brackets:</p> <pre><code>function tax(income) {\n\n        if(income == undefined)\n            throw 'no input';\n\n        if (income &lt; 1000)\n            return income * 0.1;\n        else if (income &lt; 10000)\n            return income * 0.2;\n        else\n            return income * 0.4;\n    }\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#explore-change-feed-in-azure-cosmos-db","title":"Explore change feed in Azure Cosmos DB","text":"<p>Change feed in Azure Cosmos DB is a persistent record of changes to a container in the order they occur. Change feed support in Azure Cosmos DB works by listening to an Azure Cosmos DB container for any changes. It then outputs the sorted list of documents that were changed in the order in which they were modified. The persisted changes can be processed asynchronously and incrementally, and the output can be distributed across one or more consumers for parallel processing.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#change-feed-and-different-operations","title":"Change feed and different operations","text":"<p>Today, you see all inserts and updates in the change feed. You can't filter the change feed for a specific type of operation. Currently change feed doesn't log delete operations. As a workaround, you can add a soft marker on the items that are being deleted. For example, you can add an attribute in the item called \"deleted,\" set its value to \"true,\" and then set a time-to-live (TTL) value on the item. Setting the TTL ensures that the item is automatically deleted.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#reading-azure-cosmos-db-change-feed","title":"Reading Azure Cosmos DB change feed","text":"<p>You can work with the Azure Cosmos DB change feed using either a push model or a pull model. With a push model, the change feed processor pushes work to a client that has business logic for processing this work. However, the complexity in checking for work and storing state for the last processed work is handled within the change feed processor.</p> <p>With a pull model, the client has to pull the work from the server. In this case, the client has business logic for processing work and also stores state for the last processed work. The client handles load balancing across multiple clients processing work in parallel, and handling errors.</p> <p>Note</p> <p>It is recommended to use the push model because you won't need to worry about polling the change feed for future changes, storing state for the last processed change, and other benefits.</p> <p>Most scenarios that use the Azure Cosmos DB change feed use one of the push model options. However, there are some scenarios where you might want the extra low level control of the pull model. The extra low-level control includes:</p> <ul> <li>Reading changes from a particular partition key</li> <li>Controlling the pace at which your client receives changes for processing</li> <li>Doing a one-time read of the existing data in the change feed (for example, to do a data migration)</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#reading-change-feed-with-a-push-model","title":"Reading change feed with a push model","text":"<p>There are two ways you can read from the change feed with a push model: Azure Functions Azure Cosmos DB triggers, and the change feed processor library. Azure Functions uses the change feed processor behind the scenes, so these are both similar ways to read the change feed. Think of Azure Functions as simply a hosting platform for the change feed processor, not an entirely different way of reading the change feed. Azure Functions uses the change feed processor behind the scenes. It automatically parallelizes change processing across your container's partitions.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#azure-functions","title":"Azure Functions","text":"<p>You can create small reactive Azure Functions that are automatically triggered on each new event in your Azure Cosmos DB container's change feed. With the Azure Functions trigger for Azure Cosmos DB, you can use the Change Feed Processor's scaling and reliable event detection functionality without the need to maintain any worker infrastructure.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2004%20Revision/#change-feed-processor","title":"Change feed processor","text":"<p>The change feed processor is part of the Azure Cosmos DB .NET V3 and Java V4 SDKs. It simplifies the process of reading the change feed and distributes the event processing across multiple consumers effectively.</p> <p>There are four main components of implementing the change feed processor:</p> <ol> <li>The monitored container: The monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.</li> <li>The lease container: The lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.</li> <li>The compute instance: A compute instance hosts the change feed processor to listen for changes. Depending on the platform, it might represented by a VM, a kubernetes pod, an Azure App Service instance, an actual physical machine. It has a unique identifier referenced as the instance name throughout this article.</li> <li>The delegate: The delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.</li> </ol> <p>When implementing the change feed processor the point of entry is always the monitored container, from a <code>Container</code> instance you call <code>GetChangeFeedProcessorBuilder</code>:</p> <pre><code>/// &lt;summary&gt;\n/// Start the Change Feed Processor to listen for changes and process them with the HandleChangesAsync implementation.\n/// &lt;/summary&gt;\nprivate static async Task&lt;ChangeFeedProcessor&gt; StartChangeFeedProcessorAsync(\n    CosmosClient cosmosClient,\n    IConfiguration configuration)\n{\n    string databaseName = configuration[\"SourceDatabaseName\"];\n    string sourceContainerName = configuration[\"SourceContainerName\"];\n    string leaseContainerName = configuration[\"LeasesContainerName\"];\n\n    Container leaseContainer = cosmosClient.GetContainer(databaseName, leaseContainerName);\n    ChangeFeedProcessor changeFeedProcessor = cosmosClient.GetContainer(databaseName, sourceContainerName)\n        .GetChangeFeedProcessorBuilder&lt;ToDoItem&gt;(processorName: \"changeFeedSample\", onChangesDelegate: HandleChangesAsync)\n            .WithInstanceName(\"consoleHost\")\n            .WithLeaseContainer(leaseContainer)\n            .Build();\n\n    Console.WriteLine(\"Starting Change Feed Processor...\");\n    await changeFeedProcessor.StartAsync();\n    Console.WriteLine(\"Change Feed Processor started.\");\n    return changeFeedProcessor;\n}\n</code></pre> <p>Where the first parameter is a distinct name that describes the goal of this processor and the second name is the delegate implementation that handles changes. Following is an example of a delegate:</p> <pre><code>/// &lt;summary&gt;\n/// The delegate receives batches of changes as they are generated in the change feed and can process them.\n/// &lt;/summary&gt;\nstatic async Task HandleChangesAsync(\n    ChangeFeedProcessorContext context,\n    IReadOnlyCollection&lt;ToDoItem&gt; changes,\n    CancellationToken cancellationToken)\n{\n    Console.WriteLine($\"Started handling changes for lease {context.LeaseToken}...\");\n    Console.WriteLine($\"Change Feed request consumed {context.Headers.RequestCharge} RU.\");\n    // SessionToken if needed to enforce Session consistency on another client instance\n    Console.WriteLine($\"SessionToken ${context.Headers.Session}\");\n\n    // We may want to track any operation's Diagnostics that took longer than some threshold\n    if (context.Diagnostics.GetClientElapsedTime() &gt; TimeSpan.FromSeconds(1))\n    {\n        Console.WriteLine($\"Change Feed request took longer than expected. Diagnostics:\" + context.Diagnostics.ToString());\n    }\n\n    foreach (ToDoItem item in changes)\n    {\n        Console.WriteLine($\"Detected operation for item with id {item.id}, created at {item.creationTime}.\");\n        // Simulate some asynchronous operation\n        await Task.Delay(10);\n    }\n\n    Console.WriteLine(\"Finished handling changes.\");\n}\n</code></pre> <p>Afterwards, you define the compute instance name or unique identifier with <code>WithInstanceName</code>, this should be unique and different in each compute instance you're deploying, and finally, which is the container to maintain the lease state with <code>WithLeaseContainer</code>.</p> <p>Calling <code>Build</code> gives you the processor instance that you can start by calling <code>StartAsync</code>.</p> <p>The normal life cycle of a host instance is:</p> <ol> <li>Read the change feed.</li> <li>If there are no changes, sleep for a predefined amount of time (customizable with <code>WithPollInterval</code> in the <code>Builder</code>) and go to #1.</li> <li>If there are changes, send them to the delegate.</li> <li>When the delegate finishes processing the changes successfully, update the lease store with the latest processed point in time and go to #1.</li> </ol> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/","title":"Implement containerized solutions","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Explain the features and benefits Azure Container Registry offers</li> <li>Describe how to use ACR Tasks to automate builds and deployments</li> <li>Explain the elements in a Dockerfile</li> <li>Build and run an image in the ACR by using Azure CLI</li> <li>Describe the benefits of Azure Container Instances and how resources are grouped</li> <li>Deploy a container instance in Azure by using the Azure CLI</li> <li>Start and stop containers using policies</li> <li>Set environment variables in your container instances</li> <li>Mount file shares in your container instances</li> <li>Describe the features benefits of Azure Container Apps</li> <li>Deploy container app in Azure by using the Azure CLI</li> <li>Utilize Azure Container Apps built-in authentication and authorization</li> <li>Create revisions and implement app secrets</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#azure-container-registry","title":"Azure Container Registry","text":"<p>Azure Container Registry (ACR) is a managed, private Docker registry service based on the open-source Docker Registry 2.0. Create and maintain Azure container registries to store and manage your private Docker container images.</p> <p>Azure Container Registry (ACR) is a managed registry service based on the open-source Docker Registry 2.0. Create and maintain Azure container registries to store and manage your container images and related artifacts.</p> <p>Use the ACR service with your existing container development and deployment pipelines, or use Azure Container Registry Tasks to build container images in Azure. Build on demand, or fully automate builds with triggers such as source code commits and base image updates.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#use-cases","title":"Use cases","text":"<p>Pull images from an Azure container registry to various deployment targets:</p> <ul> <li>Scalable orchestration systems that manage containerized applications across clusters of hosts, including Kubernetes, DC/OS, and Docker Swarm.</li> <li>Azure services that support building and running applications at scale, including Azure Kubernetes Service (AKS), App Service, Batch, and Service Fabric.</li> </ul> <p>Developers can also push to a container registry as part of a container development workflow. For example, target a container registry from a continuous integration and delivery tool such as Azure Pipelines or Jenkins.</p> <p>Configure ACR Tasks to automatically rebuild application images when their base images are updated, or automate image builds when your team commits code to a Git repository. Create multi-step tasks to automate building, testing, and patching multiple container images in parallel in the cloud.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#azure-container-registry-service-tiers","title":"Azure Container Registry service tiers","text":"<p>Azure Container Registry is available in multiple service tiers. These tiers provide predictable pricing and several options for aligning to the capacity and usage patterns of your private Docker registry in Azure.</p> Tier Description Basic A cost-optimized entry point for developers learning about Azure Container Registry. Basic registries have the same programmatic capabilities as Standard and Premium (such as Microsoft Entra authentication integration, image deletion, and webhooks). However, the included storage and image throughput are most appropriate for lower usage scenarios. Standard Standard registries offer the same capabilities as Basic, with increased included storage and image throughput. Standard registries should satisfy the needs of most production scenarios. Premium Premium registries provide the highest amount of included storage and concurrent operations, enabling high-volume scenarios. In addition to higher image throughput, Premium adds features such as: geo-replication for managing a single registry across multiple regions, content trust for image tag signing, and private link with private endpoints to restrict access to the registry. ### Explore storage capabilities <p>All Azure Container Registry tiers benefit from advanced Azure storage features like encryption-at-rest for image data security and geo-redundancy for image data protection.</p> <ul> <li>Encryption-at-rest: All container images and other artifacts in your registry are encrypted at rest. Azure automatically encrypts an image before storing it, and decrypts it on-the-fly when you or your applications and services pull the image. Optionally apply an extra encryption layer with a customer-managed key.</li> <li>Regional storage: Azure Container Registry stores data in the region where the registry is created, to help customers meet data residency and compliance requirements. In all regions except Brazil South and Southeast Asia, Azure might also store registry data in a paired region in the same geography. In the Brazil South and Southeast Asia regions, registry data is always confined to the region, to accommodate data residency requirements for those regions. If a regional outage occurs, the registry data might become unavailable and isn't automatically recovered. Customers who wish to have their registry data stored in multiple regions for better performance across different geographies, or who wish to have resiliency in a regional outage event, should enable geo-replication.</li> <li>Geo-replication: For scenarios requiring high-availability assurance, consider using the geo-replication feature of Premium registries. Geo-replication helps guard against losing access to your registry in a regional failure event. Geo-replication provides other benefits, too, like network-close image storage for faster pushes and pulls in distributed development or deployment scenarios.</li> <li>Zone redundancy: A feature of the Premium service tier, zone redundancy uses Azure availability zones to replicate your registry to a minimum of three separate zones in each enabled region.</li> <li>Scalable storage: Azure Container Registry allows you to create as many repositories, images, layers, or tags as you need, up to the registry storage limit. High numbers of repositories and tags can impact the performance of your registry. Periodically delete unused repositories, tags, and images as part of your registry maintenance routine. Deleted registry resources like repositories, images, and tags can't be recovered after deletion.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#task-scenarios","title":"Task scenarios","text":"<p>ACR Tasks supports several scenarios to build and maintain container images and other artifacts.</p> <ul> <li>Quick task - Build and push a single container image to a container registry on-demand, in Azure, without needing a local Docker Engine installation. Think <code>docker build</code>, <code>docker push</code> in the cloud.</li> <li>Automatically triggered tasks - Enable one or more triggers to build an image:<ul> <li>Trigger on source code update</li> <li>Trigger on base image update</li> <li>Trigger on a schedule</li> </ul> </li> <li>Multi-step task - Extend the single image build-and-push capability of ACR Tasks with multi-step, multi-container-based workflows.</li> </ul> <p>Each ACR Task has an associated source code context - the location of a set of source files used to build a container image or other artifact. Example contexts include a Git repository or a local filesystem.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#quick-task","title":"Quick Task","text":"<p>Using the familiar <code>docker build</code> format, the az acr build command in the Azure CLI takes a context (the set of files to build), sends it to ACR Tasks and, by default, pushes the built image to its registry upon completion.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#trigger-task-on-source-code-update","title":"Trigger task on source code update","text":"<p>Trigger a container image build or multi-step task when code is committed, or a pull request is made or updated, to a Git repository in GitHub or Azure DevOps Services. For example, configure a build task with the Azure CLI command <code>az acr task create</code> by specifying a Git repository and optionally a branch and Dockerfile. When your team updates code in the repository, an ACR Tasks-created webhook triggers a build of the container image defined in the repo.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#trigger-on-base-image-update","title":"Trigger on base image update","text":"<p>You can set up an ACR task to track a dependency on a base image when it builds an application image. When the updated base image is pushed to your registry, or a base image is updated in a public repo such as in Docker Hub, ACR Tasks can automatically build any application images based on it.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#schedule-a-task","title":"Schedule a task","text":"<p>Optionally schedule a task by setting up one or more timer triggers when you create or update the task. Scheduling a task is useful for running container workloads on a defined schedule, or running maintenance operations or tests on images pushed regularly to your registry.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#multi-step-tasks","title":"Multi-step tasks","text":"<p>Multi-step tasks, defined in a YAML file specify individual build and push operations for container images or other artifacts. They can also define the execution of one or more containers, with each step using the container as its execution environment. For example, you can create a multi-step task that automates the following:</p> <ol> <li>Build a web application image</li> <li>Run the web application container</li> <li>Build a web application test image</li> <li>Run the web application test container, which performs tests against the running application container</li> <li>If the tests pass, build a Helm chart archive package</li> <li>Perform a <code>helm upgrade</code> using the new Helm chart archive package</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#image-platforms","title":"Image platforms","text":"<p>By default, ACR Tasks builds images for the Linux OS and the amd64 architecture. Specify the <code>--platform</code> tag to build Windows images or Linux images for other architectures. Specify the OS and optionally a supported architecture in OS/architecture format (for example, <code>--platform Linux/arm</code>). For ARM architectures, optionally specify a variant in OS/architecture/variant format (for example, <code>--platform Linux/arm64/v8</code>):</p> OS Architecture Linux AMD64  Arm  Arm64  386 Windows AMD64"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#azure-container-instances","title":"Azure Container Instances","text":"<p>Azure Container Instances (ACI) offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service.</p> <p>Azure Container Instances (ACI) is a great solution for any scenario that can operate in isolated containers, including simple applications, task automation, and build jobs. Here are some of the benefits:</p> <ul> <li>Fast startup: ACI can start containers in Azure in seconds, without the need to provision and manage a virtual machine (VM)</li> <li>Container access: ACI enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN)</li> <li>Hypervisor-level security: Isolate your application as completely as it would be in a VM</li> <li>Customer data: The ACI service stores the minimum customer data required to ensure your container groups are running as expected</li> <li>Custom sizes: ACI provides optimum utilization by allowing exact specifications of CPU cores and memory</li> <li>Persistent storage: Mount Azure Files shares directly to a container to retrieve and persist state</li> <li>Linux and Windows: Schedule both Windows and Linux containers using the same API.</li> </ul> <p>For scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, we recommend Azure Kubernetes Service (AKS).</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#container-groups","title":"Container groups","text":"<p>The top-level resource in Azure Container Instances is the container group. A container group is a collection of containers that get scheduled on the same host machine. The containers in a container group share a lifecycle, resources, local network, and storage volumes. It's similar in concept to a pod in Kubernetes.</p> <p>The following diagram shows an example of a container group that includes multiple containers:</p> <p></p> <p>This example container group:</p> <ul> <li>Is scheduled on a single host machine.</li> <li>Is assigned a DNS name label.</li> <li>Exposes a single public IP address, with one exposed port.</li> <li>Consists of two containers. One container listens on port 80, while the other listens on port 5000.</li> <li>Includes two Azure file shares as volume mounts, and each container mounts one of the shares locally.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#deployment","title":"Deployment","text":"<p>There are two common ways to deploy a multi-container group: use a Resource Manager template or a YAML file. A Resource Manager template is recommended when you need to deploy more Azure service resources (for example, an Azure Files share) when you deploy the container instances. Due to the YAML format's more concise nature, a YAML file is recommended when your deployment includes only container instances.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#resource-allocation","title":"Resource allocation","text":"<p>Azure Container Instances allocates resources such as CPUs, memory, and optionally GPUs (preview) to a container group by adding the resource requests of the instances in the group. Using CPU resources as an example, if you create a container group with two instances, each requesting one CPU, then the container group is allocated two CPUs.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#networking","title":"Networking","text":"<p>Container groups share an IP address and a port namespace on that IP address. To enable external clients to reach a container within the group, you must expose the port on the IP address and from the container. Because containers within the group share a port namespace, port mapping isn't supported. Containers within a group can reach each other via localhost on the ports that they exposed, even if those ports aren't exposed externally on the group's IP address.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#storage","title":"Storage","text":"<p>You can specify external volumes to mount within a container group. You can map those volumes into specific paths within the individual containers in a group. Supported volumes include:</p> <ul> <li>Azure file share</li> <li>Secret</li> <li>Empty directory</li> <li>Cloned git repo</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#common-scenarios","title":"Common scenarios","text":"<p>Multi-container groups are useful in cases where you want to divide a single functional task into a few container images. These images might be delivered by different teams and have separate resource requirements.</p> <p>Example usage could include:</p> <ul> <li>A container serving a web application and a container pulling the latest content from source control.</li> <li>An application container and a logging container. The logging container collects the logs and metrics output by the main application and writes them to long-term storage.</li> <li>An application container and a monitoring container. The monitoring container periodically makes a request to the application to ensure that it's running and responding correctly, and raises an alert if it's not.</li> <li>A front-end container and a back-end container. The front end might serve a web application, with the back end running a service to retrieve data.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#container-restart-policy","title":"Container restart policy","text":"<p>When you create a container group in Azure Container Instances, you can specify one of three restart policy settings.</p> Restart policy Description <code>Always</code> Containers in the container group are always restarted. This is the default setting applied when no restart policy is specified at container creation. <code>Never</code> Containers in the container group are never restarted. The containers run at most once. <code>OnFailure</code> Containers in the container group are restarted only when the process executed in the container fails (when it terminates with a nonzero exit code). The containers are run at least once."},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#specify-a-restart-policy","title":"Specify a restart policy","text":"<p>Specify the <code>--restart-policy</code> parameter when you call <code>az container create</code>.</p> <pre><code>az container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer \\\n    --image mycontainerimage \\\n    --restart-policy OnFailure\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#run-to-completion","title":"Run to completion","text":"<p>Azure Container Instances starts the container, and then stops it when its application, or script, exits. When Azure Container Instances stops a container whose restart policy is <code>Never</code> or <code>OnFailure</code>, the container's status is set to Terminated.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#env-values","title":"Env Values","text":"<pre><code>az container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer2 \\\n    --image mcr.microsoft.com/azuredocs/aci-wordcount:latest \n    --restart-policy OnFailure \\\n    --environment-variables 'NumWords'='5' 'MinLength'='8'\\\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#secure-values","title":"Secure values","text":"<p>Objects with secure values are intended to hold sensitive information like passwords or keys for your application. Using secure values for environment variables is both safer and more flexible than including it in your container's image.</p> <p>Environment variables with secure values aren't visible in your container's properties. Their values can be accessed only from within the container. For example, container properties viewed in the Azure portal or Azure CLI display only a secure variable's name, not its value.</p> <p>Set a secure environment variable by specifying the <code>secureValue</code> property instead of the regular <code>value</code> for the variable's type. The two variables defined in the following YAML demonstrate the two variable types.</p> <pre><code>apiVersion: 2018-10-01\nlocation: eastus\nname: securetest\nproperties:\n  containers:\n  - name: mycontainer\n    properties:\n      environmentVariables:\n        - name: 'NOTSECRET'\n          value: 'my-exposed-value'\n        - name: 'SECRET'\n          secureValue: 'my-secret-value'\n      image: nginx\n      ports: []\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n  osType: Linux\n  restartPolicy: Always\ntags: null\ntype: Microsoft.ContainerInstance/containerGroups\n</code></pre> <p>You would run the following command to deploy the container group with YAML:</p> <pre><code>az container create --resource-group myResourceGroup \\\n    --file secure-env.yaml \\\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#mounting-file-share","title":"Mounting file share","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#limitations","title":"Limitations","text":"<ul> <li>You can only mount Azure Files shares to Linux containers.</li> <li>Azure file share volume mount requires the Linux container run as root.</li> <li>Azure File share volume mounts are limited to CIFS support.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#deploy-container-and-mount-volume","title":"Deploy container and mount volume","text":"<p>To mount an Azure file share as a volume in a container by using the Azure CLI, specify the share and volume mount point when you create the container with <code>az container create</code>. Following is an example of the command:</p> <pre><code>az container create \\\n    --resource-group $ACI_PERS_RESOURCE_GROUP \\\n    --name hellofiles \\\n    --image mcr.microsoft.com/azuredocs/aci-hellofiles \\\n    --dns-name-label aci-demo \\\n    --ports 80 \\\n    --azure-file-volume-account-name $ACI_PERS_STORAGE_ACCOUNT_NAME \\\n    --azure-file-volume-account-key $STORAGE_KEY \\\n    --azure-file-volume-share-name $ACI_PERS_SHARE_NAME \\\n    --azure-file-volume-mount-path /aci/logs/\n</code></pre> <p>The <code>--dns-name-label</code> value must be unique within the Azure region where you create the container instance. Update the value in the preceding command if you receive a DNS name label error message when you execute the command.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#deploy-container-and-mount-volume-yaml","title":"Deploy container and mount volume - YAML","text":"<p>You can also deploy a container group and mount a volume in a container with the Azure CLI and a YAML template. Deploying by YAML template is the preferred method when deploying container groups consisting of multiple containers.</p> <p>The following YAML template defines a container group with one container created with the <code>aci-hellofiles</code> image. The container mounts the Azure file share acishare created previously as a volume. Following is an example YAML file.</p> <pre><code>apiVersion: '2019-12-01'\nlocation: eastus\nname: file-share-demo\nproperties:\n  containers:\n  - name: hellofiles\n    properties:\n      environmentVariables: []\n      image: mcr.microsoft.com/azuredocs/aci-hellofiles\n      ports:\n      - port: 80\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n      volumeMounts:\n      - mountPath: /aci/logs/\n        name: filesharevolume\n  osType: Linux\n  restartPolicy: Always\n  ipAddress:\n    type: Public\n    ports:\n      - port: 80\n    dnsNameLabel: aci-demo\n  volumes:\n  - name: filesharevolume\n    azureFile:\n      sharename: acishare\n      storageAccountName: &lt;Storage account name&gt;\n      storageAccountKey: &lt;Storage account key&gt;\ntags: {}\ntype: Microsoft.ContainerInstance/containerGroups\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#mount-multiple-volumes","title":"Mount multiple volumes","text":"<p>To mount multiple volumes in a container instance, you must deploy using an Azure Resource Manager template or a YAML file. To use a template or YAML file, provide the share details and define the volumes by populating the <code>volumes</code> array in the <code>properties</code> section of the template.</p> <p>For example, if you created two Azure Files shares named share1 and share2 in storage account myStorageAccount, the <code>volumes</code> array in a Resource Manager template would appear similar to the following:</p> <pre><code>\"volumes\": [{\n  \"name\": \"myvolume1\",\n  \"azureFile\": {\n    \"shareName\": \"share1\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"&lt;storage-account-key&gt;\"\n  }\n},\n{\n  \"name\": \"myvolume2\",\n  \"azureFile\": {\n    \"shareName\": \"share2\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"&lt;storage-account-key&gt;\"\n  }\n}]\n</code></pre> <p>Next, for each container in the container group in which you'd like to mount the volumes, populate the <code>volumeMounts</code> array in the <code>properties</code> section of the container definition. For example, this mounts the two volumes, myvolume1 and myvolume2, previously defined:</p> <pre><code>\"volumeMounts\": [{\n  \"name\": \"myvolume1\",\n  \"mountPath\": \"/mnt/share1/\"\n},\n{\n  \"name\": \"myvolume2\",\n  \"mountPath\": \"/mnt/share2/\"\n}]\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#azure-container-apps","title":"Azure Container Apps","text":"<p>Azure Container Apps enables you to run microservices and containerized applications on a serverless platform that runs on top of Azure Kubernetes Service. Common uses of Azure Container Apps include:</p> <ul> <li>Deploying API endpoints</li> <li>Hosting background processing applications</li> <li>Handling event-driven processing</li> <li>Running microservices</li> </ul> <p>Applications built on Azure Container Apps can dynamically scale based on: HTTP traffic, event-driven processing, CPU or memory load, and any KEDA-supported scaler.</p> <p>With Azure Container Apps, you can:</p> <ul> <li>Run multiple container revisions and manage the container app's application lifecycle.</li> <li>Autoscale your apps based on any KEDA-supported scale trigger. Most applications can scale to zero. (Applications that scale on CPU or memory load can't scale to zero.)</li> <li>Enable HTTPS ingress without having to manage other Azure infrastructure.</li> <li>Split traffic across multiple versions of an application for Blue/Green deployments and A/B testing scenarios.</li> <li>Use internal ingress and service discovery for secure internal-only endpoints with built-in DNS-based service discovery.</li> <li>Build microservices with Dapr and access its rich set of APIs.</li> <li>Run containers from any registry, public or private, including Docker Hub and Azure Container Registry (ACR).</li> <li>Use the Azure CLI extension, Azure portal or ARM templates to manage your applications.</li> <li>Provide an existing virtual network when creating an environment for your container apps.</li> <li>Securely manage secrets directly in your application.</li> <li>Monitor logs using Azure Log Analytics.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#azure-container-apps-environments","title":"Azure Container Apps environments","text":"<p>Individual container apps are deployed to a single Container Apps environment, which acts as a secure boundary around groups of container apps. Container Apps in the same environment are deployed in the same virtual network and write logs to the same Log Analytics workspace. You might provide an existing virtual network when you create an environment.</p> <p>Reasons to deploy container apps to the same environment include situations when you need to:</p> <ul> <li>Manage related services</li> <li>Deploy different applications to the same virtual network</li> <li>Instrument Dapr applications that communicate via the Dapr service invocation API</li> <li>Have applications to share the same Dapr configuration</li> <li>Have applications share the same log analytics workspace</li> </ul> <p>Reasons to deploy container apps to different environments include situations when you want to ensure:</p> <ul> <li>Two applications never share the same compute resources</li> <li>Two Dapr applications can't communicate via the Dapr service invocation API</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#microservices-with-azure-container-apps","title":"Microservices with Azure Container Apps","text":"<p>Microservice architectures allow you to independently develop, upgrade, version, and scale core areas of functionality in an overall system. Azure Container Apps provides the foundation for deploying microservices featuring:</p> <ul> <li>Independent scaling, versioning, and upgrades</li> <li>Service discovery</li> <li>Native Dapr integration</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#dapr-integration","title":"Dapr integration","text":"<p>When you implement a system composed of microservices, function calls are spread across the network. To support the distributed nature of microservices, you need to account for failures, retries, and timeouts. While Container Apps features the building blocks for running microservices, use of Dapr provides an even richer microservices programming model. Dapr includes features like observability, pub/sub, and service-to-service invocation with mutual TLS, retries, and more.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#deploy-a-container-app","title":"Deploy a container app","text":"<p>Install the Azure Container Apps extension for the CLI.</p> <pre><code>az extension add --name containerapp --upgrade\n</code></pre> <p>Register the <code>Microsoft.App</code> namespace.</p> <pre><code>az provider register --namespace Microsoft.App\n</code></pre> <p>Register the <code>Microsoft.OperationalInsights</code> provider for the Azure Monitor Log Analytics workspace if you haven't used it before.</p> <pre><code>az provider register --namespace Microsoft.OperationalInsights\n</code></pre> <p>Set env values</p> <pre><code>myRG=az204-appcont-rg\nmyLocation=&lt;location&gt;\nmyAppContEnv=az204-env-$RANDOM\n</code></pre> <p>Create the resource group for your container app.</p> <pre><code>az group create \\\n    --name $myRG \\\n    --location $myLocation\n</code></pre> <p>Create environment</p> <pre><code>az containerapp env create \\\n    --name $myAppContEnv \\\n    --resource-group $myRG \\\n    --location $myLocation\n</code></pre> <p>Deploy a sample app container image by using the <code>containerapp create</code> command.</p> <pre><code>az containerapp create \\\n    --name my-container-app \\\n    --resource-group $myRG \\\n    --environment $myAppContEnv \\\n    --image mcr.microsoft.com/azuredocs/containerapps-helloworld:latest \\\n    --target-port 80 \\\n    --ingress 'external' \\\n    --query properties.configuration.ingress.fqdn\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#explore-containers-in-azure-container-apps","title":"Explore containers in Azure Container Apps","text":"<p>Azure Container Apps manages the details of Kubernetes and container orchestration for you. Containers in Azure Container Apps can use any runtime, programming language, or development stack of your choice.</p> <p></p> <p>Azure Container Apps supports any Linux-based x86-64 (<code>linux/amd64</code>) container image. There's no required base container image, and if a container crashes it automatically restarts.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#configuration","title":"Configuration","text":"<p>The following code is an example of the <code>containers</code> array in the <code>properties.template</code> section of a container app resource template. The excerpt shows some of the available configuration options when setting up a container when using Azure Resource Manager (ARM) templates. Changes to the template ARM configuration section trigger a new container app revision.</p> <pre><code>\"containers\": [\n  {\n       \"name\": \"main\",\n       \"image\": \"[parameters('container_image')]\",\n    \"env\": [\n      {\n        \"name\": \"HTTP_PORT\",\n        \"value\": \"80\"\n      },\n      {\n        \"name\": \"SECRET_VAL\",\n        \"secretRef\": \"mysecret\"\n      }\n    ],\n    \"resources\": {\n      \"cpu\": 0.5,\n      \"memory\": \"1Gi\"\n    },\n    \"volumeMounts\": [\n      {\n        \"mountPath\": \"/myfiles\",\n        \"volumeName\": \"azure-files-volume\"\n      }\n    ]\n    \"probes\":[\n        {\n            \"type\":\"liveness\",\n            \"httpGet\":{\n            \"path\":\"/health\",\n            \"port\":8080,\n            \"httpHeaders\":[\n                {\n                    \"name\":\"Custom-Header\",\n                    \"value\":\"liveness probe\"\n                }]\n            },\n            \"initialDelaySeconds\":7,\n            \"periodSeconds\":3\n// file is truncated for brevity\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#multiple-containers","title":"Multiple containers","text":"<p>You can define multiple containers in a single container app to implement the sidecar pattern. The containers in a container app share hard disk and network resources and experience the same application lifecycle.</p> <p>Examples of sidecar containers include:</p> <ul> <li>An agent that reads logs from the primary app container on a shared volume and forwards them to a logging service.</li> <li>A background process that refreshes a cache used by the primary app container in a shared volume.</li> </ul> <p>Running multiple containers in a single container app is an advanced use case. In most situations where you want to run multiple containers, such as when implementing a microservice architecture, deploy each service as a separate container app.</p> <p>To run multiple containers in a container app, add more than one container in the containers array of the container app template.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#container-registries","title":"Container registries","text":"<p>You can deploy images hosted on private registries by providing credentials in the Container Apps configuration.</p> <p>To use a container registry, you define the required fields in registries array in the properties.configuration section of the container app resource template. The passwordSecretRef field identifies the name of the secret in the secrets array name where you defined the password.</p> <pre><code>{\n  ...\n  \"registries\": [{\n    \"server\": \"docker.io\",\n    \"username\": \"my-registry-user-name\",\n    \"passwordSecretRef\": \"my-password-secret-name\"\n  }]\n}\n</code></pre> <p>With the registry information added, the saved credentials can be used to pull a container image from the private registry when your app is deployed.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#limitations_1","title":"Limitations","text":"<p>Azure Container Apps has the following limitations:</p> <ul> <li>Privileged containers: Azure Container Apps can't run privileged containers. If your program attempts to run a process that requires root access, the application inside the container experiences a runtime error.</li> <li>Operating system: Linux-based (<code>linux/amd64</code>) container images are required.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>Azure Container Apps provides built-in authentication and authorization features to secure your external ingress-enabled container app with minimal or no code. The built-in authentication feature for Container Apps can save you time and effort by providing out-of-the-box authentication with federated identity providers, allowing you to focus on the rest of your application.</p> <ul> <li>Azure Container Apps provides access to various built-in authentication providers.</li> <li>The built-in auth features don\u2019t require any particular language, SDK, security expertise, or even any code that you have to write.</li> </ul> <p>This feature should only be used with HTTPS. Ensure <code>allowInsecure</code> is disabled on your container app's ingress configuration. You can configure your container app for authentication with or without restricting access to your site content and APIs.</p> <ul> <li>To restrict app access only to authenticated users, set its Restrict access setting to Require authentication.</li> <li>To authenticate but not restrict access, set its Restrict access setting to Allow unauthenticated access.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#identity-providers","title":"Identity providers","text":"<p>Container Apps uses federated identity, in which a third-party identity provider manages the user identities and authentication flow for you. The following identity providers are available by default:</p> Provider Sign-in endpoint How-To guidance Microsoft Identity Platform <code>/.auth/login/aad</code> Microsoft Identity Platform Facebook <code>/.auth/login/facebook</code> Facebook GitHub <code>/.auth/login/github</code> GitHub Google <code>/.auth/login/google</code> Google Twitter <code>/.auth/login/twitter</code> Twitter Any OpenID Connect provider <code>/.auth/login/&lt;providerName&gt;</code> OpenID Connect <p>When you use one of these providers, the sign-in endpoint is available for user authentication and authentication token validation from the provider. You can provide your users with any number of these provider options.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#feature-architecture","title":"Feature architecture","text":"<p>The authentication and authorization middleware component is a feature of the platform that runs as a sidecar container on each replica in your application. When enabled, every incoming HTTP request passes through the security layer before being handled by your application.</p> <p></p> <p>The platform middleware handles several things for your app:</p> <ul> <li>Authenticates users and clients with the specified identity providers</li> <li>Manages the authenticated session</li> <li>Injects identity information into HTTP request headers</li> </ul> <p>The authentication and authorization module runs in a separate container, isolated from your application code. As the security container doesn't run in-process, no direct integration with specific language frameworks is possible. However, relevant information your app needs is provided in request headers.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#authentication-flow","title":"Authentication flow","text":"<p>The authentication flow is the same for all providers, but differs depending on whether you want to sign in with the provider's SDK:</p> <ul> <li>Without provider SDK (server-directed flow or server flow): The application delegates federated sign-in to Container Apps. Delegation is typically the case with browser apps, which presents the provider's sign-in page to the user.</li> <li>With provider SDK (client-directed flow or client flow): The application signs users in to the provider manually and then submits the authentication token to Container Apps for validation. This approach is typical for browser-less apps that don't present the provider's sign-in page to the user. An example is a native mobile app that signs users in using the provider's SDK.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#managing-revisions","title":"Managing revisions","text":"<p>By default, Container Apps creates a unique revision name with a suffix consisting of a semi-random string of alphanumeric characters. For example, for a container app named album-api, setting the revision suffix name to 1st-revision would create a revision with the name album-api--1st-revision. You can set the revision suffix in the ARM template, through the Azure CLI <code>az containerapp create</code> and <code>az containerapp update</code> commands, or when creating a revision via the Azure portal.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#updating-your-container-app","title":"Updating your container app","text":"<p>With the <code>az containerapp update</code> command you can modify environment variables, compute resources, scale parameters, and deploy a different image. If your container app update includes revision-scope changes, a new revision is generated.</p> <pre><code>az containerapp update \\\n  --name &lt;APPLICATION_NAME&gt; \\\n  --resource-group &lt;RESOURCE_GROUP_NAME&gt; \\\n  --image &lt;IMAGE_NAME&gt;\n</code></pre> <p>You can list all revisions associated with your container app with the <code>az containerapp revision list</code> command.</p> <pre><code>az containerapp revision list \\\n  --name &lt;APPLICATION_NAME&gt; \\\n  --resource-group &lt;RESOURCE_GROUP_NAME&gt; \\\n  -o table\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#manage-secrets-in-azure-container-apps","title":"Manage secrets in Azure Container Apps","text":"<p>Azure Container Apps allows your application to securely store sensitive configuration values. Once secrets are defined at the application level, secured values are available to container apps. Specifically, you can reference secured values inside scale rules.</p> <ul> <li>Secrets are scoped to an application, outside of any specific revision of an application.</li> <li>Adding, removing, or changing secrets doesn't generate new revisions.</li> <li>Each application revision can reference one or more secrets.</li> <li>Multiple revisions can reference the same secrets.</li> </ul> <p>An updated or deleted secret doesn't automatically affect existing revisions in your app. When a secret is updated or deleted, you can respond to changes in one of two ways:</p> <ol> <li>Deploy a new revision.</li> <li>Restart an existing revision.</li> </ol> <p>Before you delete a secret, deploy a new revision that no longer references the old secret. Then deactivate all revisions that reference the secret.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#defining-secrets","title":"Defining secrets","text":"<p>When you create a container app, secrets are defined using the <code>--secrets</code> parameter.</p> <ul> <li>The parameter accepts a space-delimited set of name/value pairs.</li> <li>Each pair is delimited by an equals sign.</li> </ul> <p>In the example below, a connection string to a queue storage account is declared in the <code>--secrets</code> parameter. The value for queue-connection-string comes from an environment variable named <code>$CONNECTION_STRING</code>.</p> <pre><code>az containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name queuereader \\\n  --environment \"my-environment-name\" \\\n  --image demos/queuereader:v1 \\\n  --secrets \"queue-connection-string=$CONNECTION_STRING\"\n</code></pre> <p>After declaring secrets at the application level, you can reference them in environment variables when you create a new revision in your container app. When an environment variable references a secret, its value is populated with the value defined in the secret. To reference a secret in an environment variable in the Azure CLI, set its value to <code>secretref:</code>, followed by the name of the secret.</p> <p>The following example shows an application that declares a connection string at the application level. This connection is referenced in a container environment variable.</p> <pre><code>az containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name myQueueApp \\\n  --environment \"my-environment-name\" \\\n  --image demos/myQueueApp:v1 \\\n  --secrets \"queue-connection-string=$CONNECTIONSTRING\" \\\n  --env-vars \"QueueName=myqueue\" \"ConnectionString=secretref:queue-connection-string\"\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2005%20Revision/#dapr","title":"DAPR","text":"<p>Please view: https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/7-explore-distributed-application-runtime</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/","title":"Implement user authentication and authorization","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#explore-the-microsoft-identity-platform","title":"Explore the microsoft identity platform","text":"<ul> <li>Identify the components of the Microsoft identity platform</li> <li>Describe the three types of service principals and how they relate to application objects</li> <li>Explain how permissions and user consent operate, and how conditional access impacts your application</li> </ul> <p>The Microsoft identity platform for developers is a set of tools that includes authentication service, open-source libraries, and application management tools.</p> <ul> <li>OAuth 2.0 and OpenID Connect standard-compliant authentication service enabling developers to authenticate several identity types, including:<ul> <li>Work or school accounts, provisioned through Microsoft Entra ID</li> <li>Personal Microsoft account, like Skype, Xbox, and Outlook.com</li> <li>Social or local accounts, by using Azure Active Directory B2C</li> <li>Social or local customer accounts, by using Microsoft Entra External ID</li> </ul> </li> <li>Open-source libraries: Microsoft Authentication Libraries (MSAL) and support for other standards-compliant libraries</li> <li>Microsoft identity platform endpoint: Works with the Microsoft Authentication Libraries (MSAL) or any other standards-compliant library. It implements human readable scopes, in accordance with industry standards.</li> <li>Application management portal: A registration and configuration experience in the Azure portal, along with the other Azure management capabilities.</li> <li>Application configuration API and PowerShell: Programmatic configuration of your applications through the Microsoft Graph API and PowerShell so you can automate your DevOps tasks.</li> </ul> <p>For developers, the Microsoft identity platform offers integration of modern innovations in the identity and security space like passwordless authentication, step-up authentication, and Conditional Access. You don\u2019t need to implement such functionality yourself: applications integrated with the Microsoft identity platform natively take advantage of such innovations.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#explore-service-principals","title":"Explore service principals","text":"<p>To delegate Identity and Access Management functions to Microsoft Entra ID, an application must be registered with a Microsoft Entra tenant. When you register your application with Microsoft Entra ID, you're creating an identity configuration for your application that allows it to integrate with Microsoft Entra ID. When you register an app in the Azure portal, you choose whether it is:</p> <ul> <li>Single tenant: only accessible in your tenant</li> <li>Multi-tenant: accessible in other tenants</li> </ul> <p>If you register an application in the portal, an application object (the globally unique instance of the app) and a service principal object are automatically created in your home tenant. You also have a globally unique ID for your app (the app or client ID). In the portal, you can then add secrets or certificates and scopes to make your app work, customize the branding of your app in the sign-in dialog, and more.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#application-object","title":"Application object","text":"<p>A Microsoft Entra application is scoped to its one and only application object. The application object resides in the Microsoft Entra tenant where the application was registered (known as the application's \"home\" tenant). An application object is used as a template or blueprint to create one or more service principal objects. A service principal is created in every tenant where the application is used. Similar to a class in object-oriented programming, the application object has some static properties that are applied to all the created service principals (or application instances).</p> <p>The application object describes three aspects of an application:</p> <ul> <li>How the service can issue tokens in order to access the application.</li> <li>Resources that the application might need to access.</li> <li>The actions that the application can take.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#service-principal-object","title":"Service principal object","text":"<p>There are three types of service principal:</p> <ul> <li>Application - This type of service principal is the local representation, or application instance, of a global application object in a single tenant or directory. A service principal is created in each tenant where the application is used, and references the globally unique app object. The service principal object defines what the app can actually do in the specific tenant, who can access the app, and what resources the app can access.</li> <li>Managed identity - This type of service principal is used to represent a managed identity. Managed identities provide an identity for applications to use when connecting to resources that support Microsoft Entra authentication. When a managed identity is enabled, a service principal representing that managed identity is created in your tenant. Service principals representing managed identities can be granted access and permissions, but can't be updated or modified directly.</li> <li>Legacy - This type of service principal represents a legacy app, which is an app created before app registrations were introduced or an app created through legacy experiences. A legacy service principal can have:<ul> <li>credentials</li> <li>service principal names</li> <li>reply URLs</li> <li>and other properties that an authorized user can edit, but doesn't have an associated app registration.</li> </ul> </li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#relationship-between-application-objects-and-service-principals","title":"Relationship between application objects and service principals","text":"<p>The application object is the global representation of your application for use across all tenants, and the service principal is the local representation for use in a specific tenant. The application object serves as the template from which common and default properties are derived for use in creating corresponding service principal objects.</p> <p>An application object has:</p> <ul> <li>A one to one relationship with the software application, and</li> <li>A one to many relationships with its corresponding service principal objects.</li> </ul> <p>A service principal must be created in each tenant where the application is used to establish an identity for sign-in and/or access to resources being secured by the tenant. A single-tenant application has only one service principal (in its home tenant), created and consented for use during application registration. A multi-tenant application also has a service principal created in each tenant where a user from that tenant consented to its use.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#discover-permissions-and-consent","title":"Discover permissions and consent","text":"<p>The Microsoft identity platform implements the OAuth 2.0 authorization protocol. OAuth 2.0 is a method through which a third-party app can access web-hosted resources on behalf of a user. Any web-hosted resource that integrates with the Microsoft identity platform has a resource identifier, or application ID URI.</p> <p>Here are some examples of Microsoft web-hosted resources:</p> <ul> <li>Microsoft Graph: <code>https://graph.microsoft.com</code></li> <li>Microsoft 365 Mail API: <code>https://outlook.office.com</code></li> <li>Azure Key Vault: <code>https://vault.azure.net</code></li> </ul> <p>The same is true for any third-party resources that are integrated with the Microsoft identity platform. Any of these resources also can define a set of permissions that can be used to divide the functionality of that resource into smaller chunks. When a resource's functionality is chunked into small permission sets, third-party apps can be built to request only the permissions that they need to perform their function. Users and administrators can know what data the app can access.</p> <p>In OAuth 2.0, these types of permission sets are called scopes. They're also often referred to as permissions. In the Microsoft identity platform, a permission is represented as a string value. An app requests the permissions it needs by specifying the permission in the <code>scope</code> query parameter. Identity platform supports several well-defined OpenID Connect scopes and resource-based permissions (each permission is indicated by appending the permission value to the resource's identifier or application ID URI). For example, the permission string <code>https://graph.microsoft.com/Calendars.Read</code> is used to request permission to read users calendars in Microsoft Graph.</p> <p>An app most commonly requests these permissions by specifying the scopes in requests to the Microsoft identity platform authorize endpoint. However, some high-privilege permissions can be granted only through administrator consent. They can be requested or granted by using the administrator consent endpoint</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#permission-types","title":"Permission types","text":"<p>The Microsoft identity platform supports two types of permissions: delegated access and app-only access.</p> <ul> <li>Delegated access are used by apps that have a signed-in user present. For these apps, either the user or an administrator consents to the permissions that the app requests. The app is delegated with the permission to act as a signed-in user when it makes calls to the target resource.</li> <li>App-only access permissions are used by apps that run without a signed-in user present, for example, apps that run as background services or daemons. Only an administrator can consent to app-only access permissions.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#consent-types","title":"Consent types","text":"<p>Applications in Microsoft identity platform rely on consent in order to gain access to necessary resources or APIs. There are many kinds of consent that your app might need to know about in order to be successful. If you're defining permissions, you'll also need to understand how your users gain access to your app or API.</p> <p>There are three consent types: static user consent, incremental and dynamic user consent, and admin consent.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#static-user-consent","title":"Static user consent","text":"<p>In the static user consent scenario, you must specify all the permissions it needs in the app's configuration in the Azure portal. If the user (or administrator, as appropriate) hasn't granted consent for this app, then Microsoft identity platform prompts the user to provide consent at this time. Static permissions also enable administrators to consent on behalf of all users in the organization.</p> <p>While static permissions of the app defined in the Azure portal keep the code nice and simple, it presents some possible issues for developers:</p> <ul> <li>The app needs to request all the permissions it would ever need upon the user's first sign-in. This can lead to a long list of permissions that discourages end users from approving the app's access on initial sign-in.</li> <li>The app needs to know all of the resources it would ever access ahead of time. It's difficult to create apps that could access an arbitrary number of resources.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#incremental-and-dynamic-user-consent","title":"Incremental and dynamic user consent","text":"<p>With the Microsoft identity platform endpoint, you can ignore the static permissions defined in the app registration information in the Azure portal and request permissions incrementally instead. You can ask for a minimum set of permissions upfront and request more over time as the customer uses more app features.</p> <p>To do so, you can specify the scopes your app needs at any time by including the new scopes in the <code>scope</code> parameter when requesting an access token - without the need to predefine them in the application registration information. If the user hasn't yet consented to new scopes added to the request, they're prompted to consent only to the new permissions. Incremental, or dynamic consent, only applies to delegated permissions and not to app-only access permissions.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#admin-consent","title":"Admin consent","text":"<p>Admin consent is required when your app needs access to certain high-privilege permissions. Admin consent ensures that administrators have some other controls before authorizing apps or users to access highly privileged data from the organization.</p> <p>Admin consent done on behalf of an organization still requires the static permissions registered for the app. Set those permissions for apps in the app registration portal if you need an admin to give consent on behalf of the entire organization. This reduces the cycles required by the organization admin to set up the application.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#requesting-individual-user-consent","title":"Requesting individual user consent","text":"<p>In an OpenID Connect or OAuth 2.0 authorization request, an app can request the permissions it needs by using the scope query parameter. For example, when a user signs in to an app, the app sends a request like the following example. Line breaks are added for legibility.</p> <pre><code>GET https://login.microsoftonline.com/common/oauth2/v2.0/authorize?\nclient_id=6731de76-14a6-49ae-97bc-6eba6914391e\n&amp;response_type=code\n&amp;redirect_uri=http%3A%2F%2Flocalhost%2Fmyapp%2F\n&amp;response_mode=query\n&amp;scope=\nhttps%3A%2F%2Fgraph.microsoft.com%2Fcalendars.read%20\nhttps%3A%2F%2Fgraph.microsoft.com%2Fmail.send\n</code></pre> <p>The <code>scope</code> parameter is a space-separated list of delegated permissions that the app is requesting. Each permission is indicated by appending the permission value to the resource's identifier (the application ID URI). In the request example, the app needs permission to read the user's calendar and send mail as the user.</p> <p>After the user enters their credentials, the Microsoft identity platform checks for a matching record of user consent. If the user hasn't consented to any of the requested permissions in the past, and if the administrator hasn't consented to these permissions on behalf of the entire organization, the Microsoft identity platform asks the user to grant the requested permissions.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#discover-conditional-access","title":"Discover conditional access","text":"<p>The Conditional Access feature in Microsoft Entra ID offers one of several ways that you can use to secure your app and protect a service. Conditional Access enables developers and enterprise customers to protect services in a multitude of ways including:</p> <ul> <li>Multifactor authentication</li> <li>Allowing only Intune enrolled devices to access specific services</li> <li>Restricting user locations and IP ranges</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#how-does-conditional-access-impact-an-app","title":"How does Conditional Access impact an app?","text":"<p>In most common cases, Conditional Access doesn't change an app's behavior or require any changes from the developer. Only in certain cases when an app indirectly or silently requests a token for a service does an app require code changes to handle Conditional Access challenges. It may be as simple as performing an interactive sign-in request.</p> <p>Specifically, the following scenarios require code to handle Conditional Access challenges:</p> <ul> <li>Apps performing the on-behalf-of flow</li> <li>Apps accessing multiple services/resources</li> <li>Single-page apps using MSAL.js</li> <li>Web apps calling a resource</li> </ul> <p>Conditional Access policies can be applied to the app and also a web API your app accesses. Depending on the scenario, an enterprise customer can apply and remove Conditional Access policies at any time. For your app to continue functioning when a new policy is applied, implement challenge handling.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#conditional-access-examples","title":"Conditional Access examples","text":"<p>Some scenarios require code changes to handle Conditional Access whereas others work as is. Here are a few scenarios using Conditional Access to do multi-factor authentication that gives some insight into the difference.</p> <ul> <li>You're building a single-tenant iOS app and apply a Conditional Access policy. The app signs in a user and doesn't request access to an API. When the user signs in, the policy is automatically invoked and the user needs to perform multi-factor authentication.</li> <li>You're building an app that uses a middle tier service to access a downstream API. An enterprise customer at the company using this app applies a policy to the downstream API. When an end user signs in, the app requests access to the middle tier and sends the token. The middle tier performs on-behalf-of flow to request access to the downstream API. At this point, a claims \"challenge\" is presented to the middle tier. The middle tier sends the challenge back to the app, which needs to comply with the Conditional Access policy.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#implement-authentication-by-using-the-microsoft-authentication-library","title":"Implement authentication by using the Microsoft Authentication Library","text":"<ul> <li>Explain the benefits of using MSAL and the application types and scenarios it supports</li> <li>Instantiate both public and confidential client apps from code</li> <li>Register an app with the Microsoft identity platform</li> <li>Create an app that retrieves a token by using the MSAL.NET library</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#explore-the-microsoft-authentication-library","title":"Explore the Microsoft Authentication Library","text":"<p>MSAL gives you many ways to get tokens, with a consistent API for many platforms. Using MSAL provides the following benefits:</p> <ul> <li>No need to directly use the OAuth libraries or code against the protocol in your application.</li> <li>Acquires tokens on behalf of a user or on behalf of an application (when applicable to the platform).</li> <li>Maintains a token cache and refreshes tokens for you when they're close to expire. You don't need to handle token expiration on your own.</li> <li>Helps you specify which audience you want your application to sign in.</li> <li>Helps you set up your application from configuration files.</li> <li>Helps you troubleshoot your app by exposing actionable exceptions, logging, and telemetry.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#application-types-and-scenarios","title":"Application types and scenarios","text":"<p>Within MSAL, a token can be acquired from many application types: web applications, web APIs, single-page apps (JavaScript), mobile and native applications, and daemons and server-side applications. MSAL currently supports the platforms and frameworks listed in the following table.</p> Library Supported platforms and frameworks MSAL for Android Android MSAL Angular Single-page apps with Angular and Angular.js frameworks MSAL for iOS and macOS iOS and macOS MSAL Go (Preview) Windows, macOS, Linux MSAL Java Windows, macOS, Linux MSAL.js JavaScript/TypeScript frameworks such as Vue.js, Ember.js, or Durandal.js MSAL.NET .NET Framework, .NET, .NET MAUI, WINUI, Xamarin Android, Xamarin iOS, Universal Windows Platform MSAL Node Web apps with Express, desktop apps with Electron, Cross-platform console apps MSAL Python Windows, macOS, Linux MSAL React Single-page apps with React and React-based libraries (Next.js, Gatsby.js) ### Authentication flows <p>The following table shows some of the different authentication flows provided by Microsoft Authentication Library (MSAL). These flows can be used in various application scenarios.</p> Authentication flow Supported application types Authorization code User sign-in and access to web APIs on behalf of the user. Desktop, Mobile, Single-page app (SPA) (requires PKCE), Web Client credentials Access to web APIs by using the identity of the application itself. Typically used for server-to-server communication and automated scripts requiring no user interaction. Daemon Device code User sign-in and access to web APIs on behalf of the user on input-constrained devices like smart TVs and IoT devices. Also used by command line interface (CLI) applications. Desktop, Mobile Implicit grant User sign-in and access to web APIs on behalf of the user. The implicit grant flow is no longer recommended - use authorization code with PKCE instead. Single-page app (SPA), Web On-behalf-of (OBO) Access from an \"upstream\" web API to a \"downstream\" web API on behalf of the user. The user's identity and delegated permissions are passed through to the downstream API from the upstream API. Web API Username/password (ROPC) Allows an application to sign in the user by directly handling their password. The ROPC flow is NOT recommended. Desktop, Mobile Integrated Windows authentication (IWA) Allows applications on domain or Microsoft Entra joined computers to acquire a token silently (without any UI interaction from the user). Desktop, Mobile ### Public client and confidential client applications <p>When examining the public or confidential nature of a given client, we're evaluating the ability of that client to prove its identity to the authorization server. This is important because the authorization server must be able to trust the identity of the client in order to issue access tokens.</p> <ul> <li>Public client applications run on devices, such as desktop, browserless APIs, mobile or client-side browser apps. They can't be trusted to safely keep application secrets, so they can only access web APIs on behalf of the user. Anytime the source, or compiled bytecode of a given app, is transmitted anywhere it can be read, disassembled, or otherwise inspected by untrusted parties. As they also only support public client flows and can't hold configuration-time secrets, they can't have client secrets.</li> <li>Confidential client applications run on servers, such as web apps, web API apps, or service/daemon apps. They're considered difficult to access by users or attackers, and therefore can adequately hold configuration-time secrets to assert proof of its identity. The client ID is exposed through the web browser, but the secret is passed only in the back channel and never directly exposed.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#initialize-client-applications","title":"Initialize client applications","text":"<p>With MSAL.NET 3.x, the recommended way to instantiate an application is by using the application builders: <code>PublicClientApplicationBuilder</code> and <code>ConfidentialClientApplicationBuilder</code>. They offer a powerful mechanism to configure the application either from the code, or from a configuration file, or even by mixing both approaches.</p> <p>Before initializing an application, you first need to register it so that your app can be integrated with the Microsoft identity platform. After registration, you may need the following information (which can be found in the Azure portal):</p> <ul> <li>Application (client) ID - This is a string representing a GUID.</li> <li>Directory (tenant) ID - Provides identity and access management (IAM) capabilities to applications and resources used by your organization. It can specify if you're writing a line of business application solely for your organization (also named single-tenant application).</li> <li>The identity provider URL (named the instance) and the sign-in audience for your application. These two parameters are collectively known as the authority.</li> <li>Client credentials - which can take the form of an application secret (client secret string) or certificate (of type <code>X509Certificate2</code>) if it's a confidential client app.</li> <li>For web apps, and sometimes for public client apps (in particular when your app needs to use a broker), you need to set the Redirect URI where the identity provider will contact back your application with the security tokens.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#initializing-public-and-confidential-client-applications-from-code","title":"Initializing public and confidential client applications from code","text":"<p>The following code instantiates a public client application, signing-in users in the Microsoft Azure public cloud, with their work and school accounts, or their personal Microsoft accounts.</p> <pre><code>IPublicClientApplication app = PublicClientApplicationBuilder.Create(clientId).Build();\n</code></pre> <p>In the same way, the following code instantiates a confidential application (a Web app located at <code>https://myapp.azurewebsites.net</code>) handling tokens from users in the Microsoft Azure public cloud, with their work and school accounts, or their personal Microsoft accounts. The application is identified with the identity provider by sharing a client secret:</p> <pre><code>string redirectUri = \"https://myapp.azurewebsites.net\";\nIConfidentialClientApplication app = ConfidentialClientApplicationBuilder.Create(clientId)\n    .WithClientSecret(clientSecret)\n    .WithRedirectUri(redirectUri )\n    .Build();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#builder-modifiers","title":"Builder modifiers","text":"<p>In the code snippets using application builders, <code>.With</code> methods can be applied as modifiers (for example, <code>.WithAuthority</code> and <code>.WithRedirectUri</code>).</p> <ul> <li><code>.WithAuthority</code> modifier: The <code>.WithAuthority</code> modifier sets the application default authority to a Microsoft Entra authority, with the possibility of choosing the Azure Cloud, the audience, the tenant (tenant ID or domain name), or providing directly the authority URI.</li> </ul> <pre><code>IPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(clientId)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n    .Build();\n</code></pre> <ul> <li><code>.WithRedirectUri</code> modifier: The <code>.WithRedirectUri</code> modifier overrides the default redirect URI.</li> </ul> <pre><code>IPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(client_id)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenant_id)\n    .WithRedirectUri(\"http://localhost\")\n    .Build();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#modifiers-common-to-public-and-confidential-client-applications","title":"Modifiers common to public and confidential client applications","text":"<p>The table below lists some of the modifiers you can set on a public, or confidential client.</p> Modifier Description <code>.WithAuthority()</code> Sets the application default authority to a Microsoft Entra authority, with the possibility of choosing the Azure Cloud, the audience, the tenant (tenant ID or domain name), or providing directly the authority URI. <code>.WithTenantId(string tenantId)</code> Overrides the tenant ID, or the tenant description. <code>.WithClientId(string)</code> Overrides the client ID. <code>.WithRedirectUri(string redirectUri)</code> Overrides the default redirect URI. This is useful for scenarios requiring a broker. <code>.WithComponent(string)</code> Sets the name of the library using MSAL.NET (for telemetry reasons). <code>.WithDebugLoggingCallback()</code> If called, the application calls <code>Debug.Write</code> simply enabling debugging traces. <code>.WithLogging()</code> If called, the application calls a callback with debugging traces. <code>.WithTelemetry(TelemetryCallback telemetryCallback)</code> Sets the delegate used to send telemetry. ### Modifiers specific to confidential client applications <p>The modifiers specific to a confidential client application builder can be found in the <code>ConfidentialClientApplicationBuilder</code> class. The different methods can be found in the Azure SDK for .NET documentation.</p> <p>Modifiers such as <code>.WithCertificate(X509Certificate2 certificate)</code> and <code>.WithClientSecret(string clientSecret)</code> are mutually exclusive. If you provide both, MSAL throws a meaningful exception.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#implement-interactive-authentication-by-using-msalnet","title":"Implement interactive authentication by using MSAL.NET","text":"<p>In this exercise you learn how to perform the following actions:</p> <ul> <li>Register an application with the Microsoft identity platform</li> <li>Use the <code>PublicClientApplicationBuilder</code> class in MSAL.NET</li> <li>Acquire a token interactively in a console application</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#register-a-new-application","title":"Register a new application","text":"<ul> <li>Sign in to the portal: https://portal.azure.com</li> <li>Search for and select Microsoft Entra ID.</li> <li>Under Manage, select App registrations &gt; New registration.</li> <li>When the Register an application page appears, enter your application's registration information:</li> </ul> Field Value Name <code>az204appreg</code> Supported account types Select Accounts in this organizational directory only Redirect URI (optional) Select Public client/native (mobile &amp; desktop) and enter <code>http://localhost</code> in the box to the right. - Select Register. <p>Microsoft Entra ID assigns a unique application (client) ID to your app, and you're taken to your application's Overview page.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#set-up-the-console-application","title":"Set up the console application","text":"<pre><code>md az204-auth\ncd az204-auth\n</code></pre> <pre><code>dotnet new console\n</code></pre> <pre><code>code . -r\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#build-the-console-app","title":"Build the console app","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#add-packages-and-using-statements","title":"Add packages and using statements","text":"<ul> <li>Add the <code>Microsoft.Identity.Client</code> package to the project in a terminal in Visual Studio Code.</li> </ul> <pre><code>dotnet add package Microsoft.Identity.Client\n</code></pre> <ul> <li>Open the Program.cs file and add <code>using</code> statements to include <code>Microsoft.Identity.Client</code> and to enable async operations.</li> </ul> <pre><code>using System.Threading.Tasks;\nusing Microsoft.Identity.Client;\n</code></pre> <ul> <li>Change the Main method to enable async.</li> </ul> <pre><code>public static async Task Main(string[] args)\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#add-code-for-the-interactive-authentication","title":"Add code for the interactive authentication","text":"<ul> <li>You need two variables to hold the Application (client) and Directory (tenant) IDs. You can copy those values from the portal. Add the following code and replace the string values with the appropriate values from the portal.</li> </ul> <pre><code>private const string _clientId = \"APPLICATION_CLIENT_ID\";\nprivate const string _tenantId = \"DIRECTORY_TENANT_ID\";\n</code></pre> <ul> <li>Use the <code>PublicClientApplicationBuilder</code> class to build out the authorization context.</li> </ul> <pre><code>var app = PublicClientApplicationBuilder\n    .Create(_clientId)\n    .WithAuthority(AzureCloudInstance.AzurePublic, _tenantId)\n    .WithRedirectUri(\"http://localhost\")\n    .Build();\n</code></pre> Code Description <code>.Create</code> Creates a <code>PublicClientApplicationBuilder</code> from a clientID. <code>.WithAuthority</code> Adds a known Authority corresponding to an ADFS server. In the code we're specifying the Public cloud, and using the tenant for the app we registered. #### Acquire a token <p>When you registered the az204appreg app, it automatically generated an API permission <code>user.read</code> for Microsoft Graph. You use that permission to acquire a token.</p> <ul> <li>Set the permission scope for the token request. Add the following code below the <code>PublicClientApplicationBuilder</code>.</li> </ul> <pre><code>string[] scopes = { \"user.read\" };\n</code></pre> <ul> <li>Add code to request the token and write the result out to the console.</li> </ul> <pre><code>AuthenticationResult result = await app.AcquireTokenInteractive(scopes).ExecuteAsync();\n\nConsole.WriteLine($\"Token:\\t{result.AccessToken}\");\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#review-completed-application","title":"Review completed application","text":"<p>The contents of the Program.cs file should resemble the following example:</p> <pre><code>using System;\nusing System.Threading.Tasks;\nusing Microsoft.Identity.Client;\n\nnamespace az204_auth\n{\n    class Program\n    {\n        private const string _clientId = \"APPLICATION_CLIENT_ID\";\n        private const string _tenantId = \"DIRECTORY_TENANT_ID\";\n\n        public static async Task Main(string[] args)\n        {\n            var app = PublicClientApplicationBuilder\n                .Create(_clientId)\n                .WithAuthority(AzureCloudInstance.AzurePublic, _tenantId)\n                .WithRedirectUri(\"http://localhost\")\n                .Build(); \n            string[] scopes = { \"user.read\" };\n            AuthenticationResult result = await app.AcquireTokenInteractive(scopes).ExecuteAsync();\n\n            Console.WriteLine($\"Token:\\t{result.AccessToken}\");\n        }\n    }\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#implement-shared-access-signatures","title":"Implement shared access signatures","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Identify the three types of shared access signatures</li> <li>Explain when to implement shared access signatures</li> <li>Create a stored access policy</li> </ul> <p>A shared access signature (SAS) is a signed URI that points to one or more storage resources and includes a token that contains a special set of query parameters. The token indicates how the resources might be accessed by the client. One of the query parameters, the signature, is constructed from the SAS parameters and signed with the key that was used to create the SAS. This signature is used by Azure Storage to authorize access to the storage resource.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#types-of-shared-access-signatures","title":"Types of shared access signatures","text":"<p>Azure Storage supports three types of shared access signatures:</p> <ul> <li>User delegation SAS: A user delegation SAS is secured with Microsoft Entra credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only.</li> <li>Service SAS: A service SAS is secured with the storage account key. A service SAS delegates access to a resource in the following Azure Storage services: Blob storage, Queue storage, Table storage, or Azure Files.</li> <li>Account SAS: An account SAS is secured with the storage account key. An account SAS delegates access to resources in one or more of the storage services. All of the operations available via a service or user delegation SAS are also available via an account SAS.</li> </ul> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#how-shared-access-signatures-work","title":"How shared access signatures work","text":"<p>When you use a SAS to access data stored in Azure Storage, you need two components. The first is a URI to the resource you want to access. The second part is a SAS token that you've created to authorize access to that resource.</p> <p>In a single URI, such as <code>https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?sp=r&amp;st=2020-01-20T11:42:32Z&amp;se=2020-01-20T19:42:32Z&amp;spr=https&amp;sv=2019-02-02&amp;sr=b&amp;sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D</code>, you can separate the URI from the SAS token as follows:</p> <ul> <li>URI: <code>https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?</code></li> <li>SAS token: <code>sp=r&amp;st=2020-01-20T11:42:32Z&amp;se=2020-01-20T19:42:32Z&amp;spr=https&amp;sv=2019-02-02&amp;sr=b&amp;sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D</code></li> </ul> <p>The SAS token itself is made up of several components.</p> Component Description <code>sp=r</code> Controls the access rights. The values can be <code>a</code> for add, <code>c</code> for create, <code>d</code> for delete, <code>l</code> for list, <code>r</code> for read, or <code>w</code> for write. This example is read only. The example <code>sp=acdlrw</code> grants all the available rights. <code>st=2020-01-20T11:42:32Z</code> The date and time when access starts. <code>se=2020-01-20T19:42:32Z</code> The date and time when access ends. This example grants eight hours of access. <code>sv=2019-02-02</code> The version of the storage API to use. <code>sr=b</code> The kind of storage being accessed. In this example, b is for blob. <code>sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D</code> The cryptographic signature. ### Best practices <p>To reduce the potential risks of using a SAS, Microsoft provides some guidance:</p> <ul> <li>To securely distribute a SAS and prevent man-in-the-middle attacks, always use HTTPS.</li> <li>The most secure SAS is a user delegation SAS. Use it wherever possible because it removes the need to store your storage account key in code. You must use Microsoft Entra ID to manage credentials. This option might not be possible for your solution.</li> <li>Try to set your expiration time to the smallest useful value. If a SAS key becomes compromised, it can be exploited for only a short time.</li> <li>Apply the rule of minimum-required privileges. Only grant the access that's required. For example, in your app, read-only access is sufficient.</li> <li>There are some situations where a SAS isn't the correct solution. When there's an unacceptable risk of using a SAS, create a middle-tier service to manage users and their access to storage.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#choose-when-to-use-shared-access-signatures","title":"Choose when to use shared access signatures","text":"<p>Use a SAS when you want to provide secure access to resources in your storage account to any client who doesn't otherwise have permissions to those resources.</p> <p>A common scenario where a SAS is useful is a service where users read and write their own data to your storage account. In a scenario where a storage account stores user data, there are two typical design patterns:</p> <ul> <li>Clients upload and download data via a front-end proxy service, which performs authentication. This front-end proxy service has the advantage of allowing validation of business rules, but for large amounts of data or high-volume transactions, creating a service that can scale to match demand may be expensive or difficult.</li> </ul> <p></p> <ul> <li>A lightweight service authenticates the client as needed and then generates a SAS. Once the client application receives the SAS, they can access storage account resources directly with the permissions defined by the SAS and for the interval allowed by the SAS. The SAS mitigates the need for routing all data through the front-end proxy service.</li> </ul> <p> Many real-world services might use a hybrid of these two approaches. For example, some data might be processed and validated via the front-end proxy, while other data is saved and/or read directly using SAS.</p> <p>Additionally, a SAS is required to authorize access to the source object in a copy operation in certain scenarios:</p> <ul> <li>When you copy a blob to another blob that resides in a different storage account, you must use a SAS to authorize access to the source blob. You can optionally use a SAS to authorize access to the destination blob as well.</li> <li>When you copy a file to another file that resides in a different storage account, you must use a SAS to authorize access to the source file. You can optionally use a SAS to authorize access to the destination file as well.</li> <li>When you copy a blob to a file, or a file to a blob, you must use a SAS to authorize access to the source object, even if the source and destination objects reside within the same storage account.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#explore-stored-access-policies","title":"Explore stored access policies","text":"<p>A stored access policy provides an extra level of control over service-level shared access signatures (SAS) on the server side. Establishing a stored access policy groups SAS and provides more restrictions for signatures that bound by the policy. You can use a stored access policy to change the start time, expiry time, or permissions for a signature, or to revoke it after it is issued.</p> <p>The following storage resources support stored access policies:</p> <ul> <li>Blob containers</li> <li>File shares</li> <li>Queues</li> <li>Tables</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#creating-a-stored-access-policy","title":"Creating a stored access policy","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#modifying-or-revoking-a-stored-access-policy","title":"Modifying or revoking a stored access policy","text":"<p>To modify the parameters of the stored access policy you can call the access control list operation for the resource type to replace the existing policy. For example, if your existing policy grants read and write permissions to a resource, you can modify it to grant only read permissions for all future requests.</p> <p>To revoke a stored access policy you can delete it, rename it by changing the signed identifier, or change the expiry time to a value in the past. Changing the signed identifier breaks the associations between any existing signatures and the stored access policy. Changing the expiry time to a value in the past causes any associated signatures to expire. Deleting or modifying the stored access policy immediately affects all of the SAS associated with it.</p> <p>To remove a single access policy, call the resource's <code>Set ACL</code> operation, passing in the set of signed identifiers that you wish to maintain on the container. To remove all access policies from the resource, call the <code>Set ACL</code> operation with an empty request body.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#explore-microsoft-graph","title":"Explore Microsoft Graph","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Explain the benefits of using Microsoft Graph</li> <li>Perform operations on Microsoft Graph by using REST and SDKs</li> <li>Apply best practices to help your applications get the most out of Microsoft Graph</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#discover-microsoft-graph","title":"Discover Microsoft Graph","text":"<p>Microsoft Graph is the gateway to data and intelligence in Microsoft 365. It provides a unified programmability model that you can use to access the tremendous amount of data in Microsoft 365, Windows 10, and Enterprise Mobility + Security.</p> <p></p> <p>In the Microsoft 365 platform, three main components facilitate the access and flow of data:</p> <ul> <li>The Microsoft Graph API offers a single endpoint, <code>https://graph.microsoft.com</code>. You can use REST APIs or SDKs to access the endpoint. Microsoft Graph also includes a powerful set of services that manage user and device identity, access, compliance, security, and help protect organizations from data leakage or loss.</li> <li>Microsoft Graph connectors work in the incoming direction, delivering data external to the Microsoft cloud into Microsoft Graph services and applications, to enhance Microsoft 365 experiences such as Microsoft Search. Connectors exist for many commonly used data sources such as Box, Google Drive, Jira, and Salesforce.</li> <li>Microsoft Graph Data Connect provides a set of tools to streamline secure and scalable delivery of Microsoft Graph data to popular Azure data stores. The cached data serves as data sources for Azure development tools that you can use to build intelligent applications.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#query-microsoft-graph-by-using-rest","title":"Query Microsoft Graph by using REST","text":"<p>Microsoft Graph is a RESTful web API that enables you to access Microsoft Cloud service resources. After you register your app and get authentication tokens for a user or service, you can make requests to the Microsoft Graph API.</p> <p>The Microsoft Graph API defines most of its resources, methods, and enumerations in the OData namespace, <code>microsoft.graph</code>, in the Microsoft Graph metadata. A few API sets are defined in their sub-namespaces, such as the call records API which defines resources like callRecord in <code>microsoft.graph.callRecords</code>.</p> <p>Unless explicitly specified in the corresponding topic, assume types, methods, and enumerations are part of the <code>microsoft.graph</code> namespace.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#call-a-rest-api-method","title":"Call a REST API method","text":"<p>To read from or write to a resource such as a user or an email message, construct a request that looks like the following:</p> <pre><code>{HTTP method} https://graph.microsoft.com/{version}/{resource}?{query-parameters}\n</code></pre> <p>The components of a request include:</p> <ul> <li><code>{HTTP method}</code> - The HTTP method used on the request to Microsoft Graph.</li> <li><code>{version}</code> - The version of the Microsoft Graph API your application is using.</li> <li><code>{resource}</code> - The resource in Microsoft Graph that you're referencing.</li> <li><code>{query-parameters}</code> - Optional OData query options or REST method parameters that customize the response.</li> </ul> <p>After you make a request, a response is returned that includes:</p> <ul> <li>Status code - An HTTP status code that indicates success or failure.</li> <li>Response message - The data that you requested or the result of the operation. The response message can be empty for some operations.</li> <li><code>nextLink</code> - If your request returns numerous data, you need to page through it by using the URL returned in <code>@odata.nextLink</code>.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#version","title":"Version","text":"<p>Microsoft Graph currently supports two versions: <code>v1.0</code> and <code>beta</code>.</p> <ul> <li><code>v1.0</code> includes generally available APIs. Use the v1.0 version for all production apps.</li> <li><code>beta</code> includes APIs that are currently in preview. Because we might introduce breaking changes to our beta APIs, we recommend that you use the beta version only to test apps that are in development; don't use beta APIs in your production apps.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#resource","title":"Resource","text":"<p>A resource can be an entity or complex type, commonly defined with properties. Entities differ from complex types by always including an id property.</p> <p>Your URL includes the resource you're interacting with in the request, such as <code>me</code>, user, group, drive, and site. Often, top-level resources also include relationships, which you can use to access other resources, like <code>me/messages</code> or <code>me/drive</code>. You can also interact with resources using methods; for example, to send an email, use <code>me/sendMail</code>.</p> <p>Each resource might require different permissions to access it. You often need a higher level of permissions to create or update a resource than to read it. For details about required permissions, see the method reference topic.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#query-parameters","title":"Query parameters","text":"<p>Query parameters can be OData system query options, or other strings that a method accepts to customize its response.</p> <p>You can use optional OData system query options to include more or fewer properties than the default response, filter the response for items that match a custom query, or provide another parameters for a method.</p> <p>For example, adding the following <code>filter</code> parameter restricts the messages returned to only those with the <code>emailAddress</code> property of <code>jon@contoso.com</code>.</p> <pre><code>GET https://graph.microsoft.com/v1.0/me/messages?filter=emailAddress eq 'jon@contoso.com'\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#query-microsoft-graph-by-using-sdks","title":"Query Microsoft Graph by using SDKs","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#create-a-microsoft-graph-client","title":"Create a Microsoft Graph client","text":"<pre><code>var scopes = new[] { \"User.Read\" };\n\n// Multi-tenant apps can use \"common\",\n// single-tenant apps must use the tenant ID from the Azure portal\nvar tenantId = \"common\";\n\n// Value from app registration\nvar clientId = \"YOUR_CLIENT_ID\";\n\n// using Azure.Identity;\nvar options = new TokenCredentialOptions\n{\n    AuthorityHost = AzureAuthorityHosts.AzurePublicCloud\n};\n\n// Callback function that receives the user prompt\n// Prompt contains the generated device code that you must\n// enter during the auth process in the browser\nFunc&lt;DeviceCodeInfo, CancellationToken, Task&gt; callback = (code, cancellation) =&gt; {\n    Console.WriteLine(code.Message);\n    return Task.FromResult(0);\n};\n\n// /dotnet/api/azure.identity.devicecodecredential\nvar deviceCodeCredential = new DeviceCodeCredential(\n    callback, tenantId, clientId, options);\n\nvar graphClient = new GraphServiceClient(deviceCodeCredential, scopes);\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#read-information-from-microsoft-graph","title":"Read information from Microsoft Graph","text":"<p>To read information from Microsoft Graph, you first need to create a request object and then run the <code>GET</code> method on the request.</p> <pre><code>// GET https://graph.microsoft.com/v1.0/me\n\nvar user = await graphClient.Me\n    .GetAsync();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#retrieve-a-list-of-entities","title":"Retrieve a list of entities","text":"<p>Retrieving a list of entities is similar to retrieving a single entity except there are other options for configuring the request. The <code>$filter</code> query parameter can be used to reduce the result set to only those rows that match the provided condition. The <code>$orderBy</code> query parameter requests that the server provides the list of entities sorted by the specified properties.</p> <pre><code>// GET https://graph.microsoft.com/v1.0/me/messages?$select=subject,sender&amp;$filter=&lt;some condition&gt;&amp;orderBy=receivedDateTime\n\nvar messages = await graphClient.Me.Messages\n    .Request()\n    .Select(m =&gt; new {\n        m.Subject,\n        m.Sender\n    })\n    .Filter(\"&lt;filter condition&gt;\")\n    .OrderBy(\"receivedDateTime\")\n    .GetAsync();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#delete-an-entity","title":"Delete an entity","text":"<p>Delete requests are constructed in the same way as requests to retrieve an entity, but use a <code>DELETE</code> request instead of a <code>GET</code>.</p> <pre><code>// DELETE https://graph.microsoft.com/v1.0/me/messages/{message-id}\n\nstring messageId = \"AQMkAGUy...\";\nvar message = await graphClient.Me.Messages[messageId]\n    .Request()\n    .DeleteAsync();\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#create-a-new-entity","title":"Create a new entity","text":"<p>For SDKs that support a fluent style, new items can be added to collections with an <code>Add</code> method. For template-based SDKs, the request object exposes a <code>post</code> method.</p> <pre><code>// POST https://graph.microsoft.com/v1.0/me/calendars\n\nvar calendar = new Calendar\n{\n    Name = \"Volunteer\"\n};\n\nvar newCalendar = await graphClient.Me.Calendars\n    .Request()\n    .AddAsync(calendar);\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#apply-best-practices-to-microsoft-graph","title":"Apply best practices to Microsoft Graph","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#authentication","title":"Authentication","text":"<p>To access the data in Microsoft Graph, your application needs to acquire an OAuth 2.0 access token, and presents it to Microsoft Graph in either of the following methods:</p> <ul> <li>The HTTP Authorization request header, as a Bearer token</li> <li>The graph client constructor, when using a Microsoft Graph client library</li> </ul> <p>Use the Microsoft Authentication Library API, MSAL to acquire the access token to Microsoft Graph.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#consent-and-authorization","title":"Consent and authorization","text":"<p>Apply the following best practices for consent and authorization in your app:</p> <ul> <li>Use least privilege. Only request permissions that are necessary, and only when you need them. For the APIs, your application calls check the permissions section in the method topics. For example, see creating a user and choose the least privileged permissions.</li> <li>Use the correct permission type based on scenarios. If you're building an interactive application where a signed in user is present, your application should use delegated permissions. If, however, your application runs without a signed-in user, such as a background service or daemon, your application should use application permissions.</li> <li>Consider the end user and admin experience. Directly affects end user and admin experiences. For example:<ul> <li>Consider who will be consenting to your application, either end users or administrators, and configure your application to request permissions appropriately.</li> <li>Ensure that you understand the difference between static, dynamic and incremental consent.</li> </ul> </li> <li>Consider multi-tenant applications. Expect customers to have various application and consent controls in different states. For example:<ul> <li>Tenant administrators can disable the ability for end users to consent to applications. In this case, an administrator would need to consent on behalf of their users.</li> <li>Tenant administrators can set custom authorization policies such as blocking users from reading other user's profiles, or limiting self-service group creation to a limited set of users. In this case, your application should expect to handle 403 error response when acting on behalf of a user.</li> </ul> </li> </ul> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#handle-responses-effectively","title":"Handle responses effectively","text":"<p>The following are some of the most important practices to follow to ensure that your application behaves reliably and predictably for your end users. For example:</p> <ul> <li>Pagination: When querying resource collections, you should expect that Microsoft Graph returns the result set in multiple pages, due to server-side page size limits. Your application should always handle the possibility that the responses are paged in nature, and use the <code>@odata.nextLink</code> property to obtain the next paged set of results, until all pages of the result set are read. The final page doesn't include an <code>@odata.nextLink</code> property. For more information, visit paging.</li> <li>Evolvable enumerations: Adding members to existing enumerations can break applications already using these enums. Evolvable enums are a mechanism that Microsoft Graph API uses to add new members to existing enumerations without causing a breaking change for applications. By default, a GET operation returns only known members for properties of evolvable enum types and your application needs to handle only the known members. If you design your application to handle unknown members as well, you can opt in to receive those members by using an HTTP <code>Prefer</code> request header.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2006%20Revision/#storing-data-locally","title":"Storing data locally","text":"<p>Your application should ideally make calls to Microsoft Graph to retrieve data in real time as necessary. You should only cache or store data locally necessary for a specific scenario, and if that use case is covered by your terms of use and privacy policy, and doesn't violate the Microsoft APIs Terms of Use. Your application should also implement proper retention and deletion policies.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/","title":"Implement secure Azure solutions","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#implement-azure-key-vault","title":"Implement Azure Key Vault","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Describe the benefits of using Azure Key Vault</li> <li>Explain how to authenticate to Azure Key Vault</li> <li>Set and retrieve a secret from Azure Key Vault by using the Azure CLI</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#explore-azure-key-vault","title":"Explore Azure Key Vault","text":"<p>The Azure Key Vault service supports two types of containers: vaults and managed hardware security module(HSM) pools. Vaults support storing software and HSM-backed keys, secrets, and certificates. Managed HSM pools only support HSM-backed keys.</p> <p>Azure Key Vault helps solve the following problems:</p> <ul> <li>Secrets Management:\u00a0Azure Key Vault can be used to Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets</li> <li>Key Management:\u00a0Azure Key Vault can also be used as a Key Management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data.</li> <li>Certificate Management:\u00a0Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with Azure and your internal connected resources.</li> </ul> <p>Azure Key Vault has two service tiers: Standard, which encrypts with a software key, and a Premium tier, which includes hardware security module(HSM)-protected keys. To see a comparison between the Standard and Premium tiers, see the\u00a0Azure Key Vault pricing page.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#key-benefits-of-using-azure-key-vault","title":"Key benefits of using Azure Key Vault","text":"<ul> <li>Centralized application secrets:\u00a0Centralizing storage of application secrets in Azure Key Vault allows you to control their distribution. For example, instead of storing the connection string in the app's code you can store it securely in Key Vault. Your applications can securely access the information they need by using URIs. These URIs allow the applications to retrieve specific versions of a secret.</li> <li>Securely store secrets and keys:\u00a0Access to a key vault requires proper authentication and authorization before a caller (user or application) can get access. Authentication is done via Microsoft Entra ID. Authorization may be done via Azure role-based access control (Azure RBAC) or Key Vault access policy. Azure RBAC can be used for both management of the vaults and to access data stored in a vault, while key vault access policy can only be used when attempting to access data stored in a vault. Azure Key Vaults may be either software-protected or, with the Azure Key Vault Premium tier, hardware-protected by hardware security modules (HSMs).</li> <li>Monitor access and use:\u00a0You can monitor activity by enabling logging for your vaults. You have control over your logs and you may secure them by restricting access and you may also delete logs that you no longer need. Azure Key Vault can be configured to:<ul> <li>Archive to a storage account.</li> <li>Stream to an event hub.</li> <li>Send the logs to Azure Monitor logs.</li> </ul> </li> <li>Simplified administration of application secrets:\u00a0Security information must be secured, it must follow a life cycle, and it must be highly available. Azure Key Vault simplifies the process of meeting these requirements by:<ul> <li>Removing the need for in-house knowledge of Hardware Security Modules</li> <li>Scaling up on short notice to meet your organization\u2019s usage spikes.</li> <li>Replicating the contents of your Key Vault within a region and to a secondary region. Data replication ensures high availability and takes away the need of any action from the administrator to trigger the failover.</li> <li>Providing standard Azure administration options via the portal, Azure CLI and PowerShell.</li> <li>Automating certain tasks on certificates that you purchase from Public CAs, such as enrolment and renewal.</li> </ul> </li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#discover-azure-key-vault-best-practices","title":"Discover Azure Key Vault best practices","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#authentication","title":"Authentication","text":"<p>To do any operations with Key Vault, you first need to authenticate to it. There are three ways to authenticate to Key Vault:</p> <ul> <li>Managed identities for Azure resources: When you deploy an app on a virtual machine in Azure, you can assign an identity to your virtual machine that has access to Key Vault. You can also assign identities to other Azure resources. The benefit of this approach is that the app or service isn't managing the rotation of the first secret. Azure automatically rotates the service principal client secret associated with the identity. We recommend this approach as a best practice.</li> <li>Service principal and certificate: You can use a service principal and an associated certificate that has access to Key Vault. We don't recommend this approach because the application owner or developer must rotate the certificate.</li> <li>Service principal and secret: Although you can use a service principal and a secret to authenticate to Key Vault, we don't recommend it. It's hard to automatically rotate the bootstrap secret that's used to authenticate to Key Vault.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#encryption-of-data-in-transit","title":"Encryption of data in transit","text":"<p>Azure Key Vault enforces Transport Layer Security (TLS) protocol to protect data when it\u2019s traveling between Azure Key Vault and clients. Clients negotiate a TLS connection with Azure Key Vault. TLS provides strong authentication, message privacy, and integrity (enabling detection of message tampering, interception, and forgery), interoperability, algorithm flexibility, and ease of deployment and use.</p> <p>Perfect Forward Secrecy (PFS) protects connections between customers\u2019 client systems and Microsoft cloud services by unique keys. Connections also use RSA-based 2,048-bit encryption key lengths. This combination makes it difficult for someone to intercept and access data that is in transit.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#azure-key-vault-best-practices","title":"Azure Key Vault best practices","text":"<ul> <li>Use separate key vaults:\u00a0Recommended using a vault per application per environment (Development, Pre-Production and Production). This pattern helps you not share secrets across environments and also reduces the threat if there is a breach.</li> <li>Control access to your vault:\u00a0Key Vault data is sensitive and business critical, you need to secure access to your key vaults by allowing only authorized applications and users.</li> <li>Backup:\u00a0Create regular back ups of your vault on update/delete/create of objects within a Vault.</li> <li>Logging:\u00a0Be sure to turn on logging and alerts.</li> <li>Recovery options:\u00a0Turn on\u00a0soft-delete\u00a0and purge protection if you want to guard against force deletion of the secret.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#authenticate-to-azure-key-vault","title":"Authenticate to Azure Key Vault","text":"<p>Authentication with Key Vault works with Microsoft Entra ID, which is responsible for authenticating the identity of any given security principal.</p> <p>For applications, there are two ways to obtain a service principal: - Enable a system-assigned\u00a0managed identity\u00a0for the application. With managed identity, Azure internally manages the application's service principal and automatically authenticates the application with other Azure services. Managed identity is available for applications deployed to various services. - If you can't use managed identity, you instead register the application with your Microsoft Entra tenant. Registration also creates a second application object that identifies the app across all tenants.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#authentication-to-key-vault-in-application-code","title":"Authentication to Key Vault in application code","text":"<p>The following is information on authenticating to Key Vault without using a managed identity.</p> <p>Key Vault SDK is using Azure Identity client library, which allows seamless authentication to Key Vault across environments with same code. The table below provides information on the Azure Identity client libraries:</p> .NET Python Java JavaScript Azure Identity SDK .NET Azure Identity SDK Python Azure Identity SDK Java Azure Identity SDK JavaScript #### Authentication to Key Vault with REST <p>Access tokens must be sent to the service using the HTTP Authorization header:</p> <pre><code>PUT /keys/MYKEY?api-version=&lt;api_version&gt;  HTTP/1.1  \nAuthorization: Bearer &lt;access_token&gt;\n</code></pre> <p>When an access token isn't supplied, or when a token isn't accepted by the service, an\u00a0<code>HTTP 401</code>\u00a0error is returned to the client and will include the\u00a0<code>WWW-Authenticate</code>\u00a0header, for example:</p> <pre><code>401 Not Authorized  \nWWW-Authenticate: Bearer authorization=\"\u2026\", resource=\"\u2026\"\n</code></pre> <p>The parameters on the\u00a0<code>WWW-Authenticate</code>\u00a0header are:</p> <ul> <li>authorization: The address of the OAuth2 authorization service that may be used to obtain an access token for the request.</li> <li>resource: The name of the resource (<code>https://vault.azure.net</code>) to use in the authorization request.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#other-resources","title":"Other resources","text":"<ul> <li>Azure Key Vault developer's guide</li> <li>Azure Key Vault availability and redundancy</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#set-and-retrieve-a-secret-from-azure-key-vault-by-using-azure-cli","title":"Set and retrieve a secret from Azure Key Vault by using Azure CLI","text":"<ul> <li>Create a Key Vault</li> <li>Add and retrieve a secret</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#create-a-key-vault","title":"Create a Key Vault","text":"<p>Let's set some variables for the CLI commands to use to reduce the amount of retyping. Replace the\u00a0<code>&lt;myLocation&gt;</code>\u00a0variable string with a region that makes sense for you. The Key Vault name needs to be a globally unique name, and the following script generates a random string.</p> <pre><code>myKeyVault=az204vault-$RANDOM \nmyLocation=&lt;myLocation&gt;\n</code></pre> <p>Create a resource group.</p> <pre><code>az group create --name az204-vault-rg --location $myLocation\n</code></pre> <p>Create a Key Vault by using the\u00a0<code>az keyvault create</code>\u00a0command.</p> <pre><code>az keyvault create --name $myKeyVault --resource-group az204-vault-rg --location $myLocation\n</code></pre> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#add-and-retrieve-a-secret","title":"Add and retrieve a secret","text":"<p>Create a secret. Let's add a password that could be used by an app. The password is called\u00a0ExamplePassword\u00a0and will store the value of\u00a0hVFkk965BuUv\u00a0in it.</p> <pre><code>az keyvault secret set --vault-name $myKeyVault --name \"ExamplePassword\" --value \"hVFkk965BuUv\"\n</code></pre> <p>Use the\u00a0<code>az keyvault secret show</code>\u00a0command to retrieve the secret.</p> <pre><code>az keyvault secret show --name \"ExamplePassword\" --vault-name $myKeyVault\n</code></pre> <p>This command returns some JSON. The last line contains the password in plain text.</p> <pre><code>\"value\": \"hVFkk965BuUv\"\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#clean-up-resources","title":"Clean up resources","text":"<pre><code>az group delete --name az204-vault-rg --no-wait\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#implement-managed-identities","title":"Implement managed identities","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Explain the differences between the two types of managed identities</li> <li>Describe the flows for user- and system-assigned managed identities</li> <li>Configure managed identities</li> <li>Acquire access tokens by using REST and code</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#explore-managed-identities","title":"Explore managed identities","text":"<p>A common challenge for developers is the management of secrets, credentials, certificates, and keys used to secure communication between services. Managed identities eliminate the need for developers to manage these credentials.</p> <p>While developers can securely store the secrets in Azure Key Vault, services need a way to access Azure Key Vault. Managed identities provide an automatically managed identity in Microsoft Entra ID for applications to use when connecting to resources that support Microsoft Entra authentication. Applications can use managed identities to obtain Microsoft Entra tokens without having to manage any credentials.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#types-of-managed-identities","title":"Types of managed identities","text":"<p>There are two types of managed identities:</p> <ul> <li>A\u00a0system-assigned managed identity\u00a0is enabled directly on an Azure service instance. When the identity is enabled, Azure creates an identity for the instance in the Microsoft Entra tenant trusted by the subscription of the instance. After the identity is created, the credentials are provisioned onto the instance. The lifecycle of a system-assigned identity is directly tied to the Azure service instance that it's enabled on. If the instance is deleted, Azure automatically cleans up the credentials and the identity in Microsoft Entra ID.</li> <li>A\u00a0user-assigned managed identity\u00a0is created as a standalone Azure resource. Through a create process, Azure creates an identity in the Microsoft Entra tenant that's trusted by the subscription in use. After the identity is created, the identity can be assigned to one or more Azure service instances. The lifecycle of a user-assigned identity is managed separately from the lifecycle of the Azure service instances to which it's assigned.</li> </ul> <p>Internally, managed identities are service principals of a special type, which are locked to only be used with Azure resources. When the managed identity is deleted, the corresponding service principal is automatically removed</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#characteristics-of-managed-identities","title":"Characteristics of managed identities","text":"<p>The following table highlights some of the key differences between the two types of managed identities.</p> Property System-assigned managed identity User-assigned managed identity Creation Created as part of an Azure resource (for example, an Azure virtual machine or Azure App Service) Created as a stand-alone Azure resource Lifecycle Shared lifecycle with the Azure resource that the managed identity is created with. When the parent resource is deleted, the managed identity is deleted as well. Independent life-cycle. Must be explicitly deleted. Sharing across Azure resources Can't be shared, it can only be associated with a single Azure resource. Can be shared. The same user-assigned managed identity can be associated with more than one Azure resource. <p>Following are common use cases for managed identities:</p> <ul> <li>System-assigned managed identity<ul> <li>Workloads contained within a single Azure resource.</li> <li>Workloads needing independent identities.</li> <li>For example, an application that runs on a single virtual machine.</li> </ul> </li> <li>User-assigned managed identity<ul> <li>Workloads that run on multiple resources and can share a single identity.</li> <li>Workloads needing preauthorization to a secure resource, as part of a provisioning flow.</li> <li>Workloads where resources are recycled frequently, but permissions should stay consistent.</li> <li>For example, a workload where multiple virtual machines need to access the same resource.</li> </ul> </li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#what-azure-services-support-managed-identities","title":"What Azure services support managed identities?","text":"<p>Managed identities for Azure resources can be used to authenticate to services that support Microsoft Entra authentication. For a list of Azure services that support the managed identities for Azure resources feature, visit\u00a0Services that support managed identities for Azure resources.</p> <p>The rest of this module uses Azure virtual machines in the examples, but the same concepts and similar actions can be applied to any resource in Azure that supports Microsoft Entra authentication.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#discover-the-managed-identities-authentication-flow","title":"Discover the managed identities authentication flow","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#how-a-system-assigned-managed-identity-works-with-an-azure-virtual-machine","title":"How a system-assigned managed identity works with an Azure virtual machine","text":"<ol> <li>Azure Resource Manager receives a request to enable the system-assigned managed identity on a virtual machine.</li> <li>Azure Resource Manager creates a service principal in Microsoft Entra ID for the identity of the virtual machine. The service principal is created in the Microsoft Entra tenant that's trusted by the subscription.</li> <li>Azure Resource Manager configures the identity on the virtual machine by updating the Azure Instance Metadata Service identity endpoint with the service principal client ID and certificate.</li> <li>After the virtual machine has an identity, use the service principal information to grant the virtual machine access to Azure resources. To call Azure Resource Manager, use role-based access control in Microsoft Entra ID to assign the appropriate role to the virtual machine service principal. To call Key Vault, grant your code access to the specific secret or key in Key Vault.</li> <li>Your code that's running on the virtual machine can request a token from the Azure Instance Metadata service endpoint, accessible only from within the virtual machine:\u00a0<code>http://169.254.169.254/metadata/identity/oauth2/token</code></li> <li>A call is made to Microsoft Entra ID to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Microsoft Entra ID returns a JSON Web Token (JWT) access token.</li> <li>Your code sends the access token on a call to a service that supports Microsoft Entra authentication.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#how-a-user-assigned-managed-identity-works-with-an-azure-virtual-machine","title":"How a user-assigned managed identity works with an Azure virtual machine","text":"<ol> <li>Azure Resource Manager receives a request to create a user-assigned managed identity.</li> <li>Azure Resource Manager creates a service principal in Microsoft Entra ID for the user-assigned managed identity. The service principal is created in the Microsoft Entra tenant that's trusted by the subscription.</li> <li>Azure Resource Manager receives a request to configure the user-assigned managed identity on a virtual machine and updates the Azure Instance Metadata Service identity endpoint with the user-assigned managed identity service principal client ID and certificate.</li> <li>After the user-assigned managed identity is created, use the service principal information to grant the identity access to Azure resources. To call Azure Resource Manager, use role-based access control in Microsoft Entra ID to assign the appropriate role to the service principal of the user-assigned identity. To call Key Vault, grant your code access to the specific secret or key in Key Vault.</li> <li>Your code that's running on the virtual machine can request a token from the Azure Instance Metadata Service identity endpoint, accessible only from within the virtual machine:\u00a0<code>http://169.254.169.254/metadata/identity/oauth2/token</code></li> <li>A call is made to Microsoft Entra ID to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Microsoft Entra ID returns a JSON Web Token (JWT) access token.</li> <li>Your code sends the access token on a call to a service that supports Microsoft Entra authentication.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#configure-managed-identities","title":"Configure managed identities","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#system-assigned-managed-identity","title":"System-assigned managed identity","text":"<p>To create, or enable, an Azure virtual machine with the system-assigned managed identity your account needs the\u00a0Virtual Machine Contributor\u00a0role assignment. No other Microsoft Entra directory role assignments are required.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#enable-system-assigned-managed-identity-during-creation-of-an-azure-virtual-machine","title":"Enable system-assigned managed identity during creation of an Azure virtual machine","text":"<p>The following example creates a virtual machine named\u00a0myVM\u00a0with a system-assigned managed identity, as requested by the\u00a0<code>--assign-identity</code>\u00a0parameter, with the specified\u00a0<code>--role</code>\u00a0and\u00a0<code>--scope</code>. The\u00a0<code>--admin-username</code>\u00a0and\u00a0<code>--admin-password</code>\u00a0parameters specify the administrative user name and password account for virtual machine sign-in. Update these values as appropriate for your environment:</p> <pre><code>az vm create --resource-group myResourceGroup \n    --name myVM \n    --image win2016datacenter \n    --generate-ssh-keys \n    --assign-identity \n    --role contributor \n    --scope mySubscription \n    --admin-username azureuser \n    --admin-password myPassword12\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#enable-system-assigned-managed-identity-on-an-existing-azure-virtual-machine","title":"Enable system-assigned managed identity on an existing Azure virtual machine","text":"<p>Use the\u00a0<code>az vm identity assign</code>\u00a0command to assign the system-assigned identity to an existing virtual machine:</p> <pre><code>az vm identity assign -g myResourceGroup -n myVm\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#user-assigned-managed-identity","title":"User-assigned managed identity","text":"<p>To assign a user-assigned identity to a virtual machine during its creation, your account needs the\u00a0Virtual Machine Contributor\u00a0and\u00a0Managed Identity Operator\u00a0role assignments. No other Microsoft Entra directory role assignments are required.</p> <p>Enabling user-assigned managed identities is a two-step process:</p> <ol> <li>Create the user-assigned identity</li> <li>Assign the identity to a virtual machine</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#create-a-user-assigned-identity","title":"Create a user-assigned identity","text":"<p>Create a user-assigned managed identity using\u00a0<code>az identity create</code>. The\u00a0<code>-g</code>\u00a0parameter specifies the resource group where the user-assigned managed identity is created, and the\u00a0<code>-n</code>\u00a0parameter specifies its name.</p> <pre><code>az identity create -g myResourceGroup -n myUserAssignedIdentity\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#assign-a-user-assigned-managed-identity-during-the-creation-of-an-azure-virtual-machine","title":"Assign a user-assigned managed identity during the creation of an Azure virtual machine","text":"<p>The following example creates a virtual machine associated with the new user-assigned identity, as specified by the\u00a0<code>--assign-identity</code>\u00a0parameter, with the given\u00a0<code>--role</code>\u00a0and\u00a0<code>--scope</code>.</p> <pre><code>az vm create \\\n--resource-group &lt;RESOURCE GROUP&gt; \\\n--name &lt;VM NAME&gt; \\\n--image Ubuntu2204 \\\n--admin-username &lt;USER NAME&gt; \\\n--admin-password &lt;PASSWORD&gt; \\\n--assign-identity &lt;USER ASSIGNED IDENTITY NAME&gt; \\\n--role &lt;ROLE&gt; \\\n--scope &lt;SUBSCRIPTION&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#assign-a-user-assigned-managed-identity-to-an-existing-azure-virtual-machine","title":"Assign a user-assigned managed identity to an existing Azure virtual machine","text":"<p>Assign the user-assigned identity to your virtual machine using\u00a0<code>az vm identity assign</code>.</p> <pre><code>az vm identity assign \\ -g &lt;RESOURCE GROUP&gt; \\ -n &lt;VM NAME&gt; \\ --identities &lt;USER ASSIGNED IDENTITY&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#azure-sdks-with-managed-identities-for-azure-resources-support","title":"Azure SDKs with managed identities for Azure resources support","text":"<p>Azure supports multiple programming platforms through a series of\u00a0Azure SDKs. Several of them have been updated to support managed identities for Azure resources, and provide corresponding samples to demonstrate usage.</p> SDK Sample .NET Manage resource from a virtual machine enabled with managed identities for Azure resources enabled Java Manage storage from a virtual machine enabled with managed identities for Azure resources Node.js Create a virtual machine with system-assigned managed identity enabled Python Create a virtual machine with system-assigned managed identity enabled Ruby Create Azure virtual machine with an system-assigned identity enabled ### Acquire an access token <p>A client application can request managed identities for Azure resources app-only access token for accessing a given resource. The token is based on the managed identities for Azure resources service principal. The recommended method is to use the\u00a0<code>DefaultAzureCredential</code>.</p> <p>The Azure Identity library supports a\u00a0<code>DefaultAzureCredential</code>\u00a0type.\u00a0<code>DefaultAzureCredential</code>\u00a0automatically attempts to authenticate via multiple mechanisms, including environment variables or an interactive sign-in. The credential type can be used in your development environment using your own credentials. It can also be used in your production Azure environment using a managed identity. No code changes are required when you deploy your application.</p> <p></p> <p>The\u00a0<code>DefaultAzureCredential</code>\u00a0attempts to authenticate via the following mechanisms, in this order, stopping when one succeeds:</p> <ol> <li>Environment\u00a0- The\u00a0<code>DefaultAzureCredential</code>\u00a0reads account information specified via environment variables and use it to authenticate.</li> <li>Managed Identity\u00a0- If the application is deployed to an Azure host with Managed Identity enabled, the\u00a0<code>DefaultAzureCredential</code>\u00a0authenticates with that account.</li> <li>Visual Studio\u00a0- If the developer authenticated via Visual Studio, the\u00a0<code>DefaultAzureCredential</code>\u00a0authenticates with that account.</li> <li>Azure CLI\u00a0- If the developer authenticated an account via the Azure CLI\u00a0<code>az login</code>\u00a0command, the\u00a0<code>DefaultAzureCredential</code>\u00a0authenticates with that account. Visual Studio Code users can authenticate their development environment using the Azure CLI.</li> <li>Azure PowerShell\u00a0- If the developer authenticated an account via the Azure PowerShell\u00a0<code>Connect-AzAccount</code>\u00a0command, the\u00a0<code>DefaultAzureCredential</code>\u00a0authenticates with that account.</li> <li>Interactive browser\u00a0- If enabled, the\u00a0<code>DefaultAzureCredential</code>\u00a0interactively authenticates the developer via the current system's default browser. By default, this credential type is disabled.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#examples","title":"Examples","text":"<p>The following examples use the Azure Identity SDK that can be added to a project with this command:</p> <pre><code>dotnet add package Azure.Identity\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#authenticate-with-defaultazurecredential","title":"Authenticate with\u00a0<code>DefaultAzureCredential</code>","text":"<p>This example demonstrates authenticating the\u00a0<code>SecretClient</code>\u00a0from the\u00a0Azure.Security.KeyVault.Secrets\u00a0client library using the\u00a0<code>DefaultAzureCredential</code>.</p> <pre><code>// Create a secret client using the DefaultAzureCredential\nvar client = new SecretClient(new Uri(\"https://myvault.vault.azure.net/\"), new DefaultAzureCredential());\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#specify-a-user-assigned-managed-identity-with-defaultazurecredential","title":"Specify a user-assigned managed identity with\u00a0<code>DefaultAzureCredential</code>","text":"<p>This example demonstrates configuring the\u00a0<code>DefaultAzureCredential</code>\u00a0to authenticate a user-assigned identity when deployed to an Azure host. It then authenticates a\u00a0<code>BlobClient</code>\u00a0from the\u00a0Azure.Storage.Blobs\u00a0client library with credential.</p> <pre><code>// When deployed to an azure host, the default azure credential will authenticate the specified user assigned managed identity. \nstring userAssignedClientId = \"&lt;your managed identity client Id&gt;\"; \nvar credential = new DefaultAzureCredential(new DefaultAzureCredentialOptions { ManagedIdentityClientId = userAssignedClientId });\n\nvar blobClient = new BlobClient(new Uri(\"https://myaccount.blob.core.windows.net/mycontainer/myblob\"), credential);\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#define-a-custom-authentication-flow-with-chainedtokencredential","title":"Define a custom authentication flow with\u00a0<code>ChainedTokenCredential</code>","text":"<p>While the\u00a0<code>DefaultAzureCredential</code>\u00a0is generally the quickest way to get started developing applications for Azure, more advanced users may want to customize the credentials considered when authenticating. The\u00a0<code>ChainedTokenCredential</code>\u00a0enables users to combine multiple credential instances to define a customized chain of credentials. This example demonstrates creating a\u00a0<code>ChainedTokenCredential</code>\u00a0which attempts to authenticate using managed identity, and fall back to authenticating via the Azure CLI if managed identity is unavailable in the current environment. The credential is then used to authenticate an\u00a0<code>EventHubProducerClient</code>\u00a0from the\u00a0Azure.Messaging.EventHubs\u00a0client library.</p> <pre><code>// Authenticate using managed identity if it is available; otherwise use the Azure CLI to authenticate.\n\nvar credential = new ChainedTokenCredential(new ManagedIdentityCredential(), new AzureCliCredential());\n\nvar eventHubProducerClient = new EventHubProducerClient(\"myeventhub.eventhubs.windows.net\", \"myhubpath\", credential);\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#implement-azure-app-configuration","title":"Implement Azure App Configuration","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Explain the benefits of using Azure App Configuration</li> <li>Describe how Azure App Configuration stores information</li> <li>Implement feature management</li> <li>Securely access your app configuration information</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#explore-the-azure-app-configuration-service","title":"Explore the Azure App Configuration service","text":"<p>Azure App Configuration provides a service to centrally manage application settings and feature flags. Modern programs, especially programs running in a cloud, generally have many components that are distributed in nature.</p> <p>App Configuration offers the following benefits:</p> <ul> <li>A fully managed service that can be set up in minutes</li> <li>Flexible key representations and mappings</li> <li>Tagging with labels</li> <li>Point-in-time replay of settings</li> <li>Dedicated UI for feature flag management</li> <li>Comparison of two sets of configurations on custom-defined dimensions</li> <li>Enhanced security through Azure-managed identities</li> <li>Encryption of sensitive information at rest and in transit</li> <li>Native integration with popular frameworks</li> </ul> <p>App Configuration complements Azure Key Vault, which is used to store application secrets. App Configuration makes it easier to implement the following scenarios:</p> <ul> <li>Centralize management and distribution of hierarchical configuration data for different environments and geographies</li> <li>Dynamically change application settings without the need to redeploy or restart an application</li> <li>Control feature availability in real-time</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#use-app-configuration","title":"Use App Configuration","text":"<p>The easiest way to add an App Configuration store to your application is through a client library that Microsoft provides. Based on the programming language and framework, the following best methods are available to you.</p> Programming language and framework How to connect .NET App Configuration\u00a0provider\u00a0for .NET ASP.NET Core App Configuration\u00a0provider\u00a0for .NET .NET Framework and ASP.NET App Configuration\u00a0builder\u00a0for .NET Java Spring App Configuration\u00a0provider\u00a0for Spring Cloud JavaScript/Node.js App Configuration\u00a0provider\u00a0for JavaScript Python App Configuration\u00a0provider\u00a0for Python Others App Configuration\u00a0REST API"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#create-paired-keys-and-values","title":"Create paired keys and values","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#keys","title":"Keys","text":"<p>Keys serve as the name for key-value pairs and are used to store and retrieve corresponding values. It's a common practice to organize keys into a hierarchical namespace by using a character delimiter, such as\u00a0<code>/</code>\u00a0or\u00a0<code>:</code>. Use a convention that's best suited for your application. App Configuration treats keys as a whole. It doesn't parse keys to figure out how their names are structured or enforce any rule on them.</p> <p>Here's an example of key names structured into a hierarchy based on component services:</p> <pre><code>AppName:Service1:ApiEndpoint \nAppName:Service2:ApiEndpoint\n</code></pre> <p>The use of configuration data within application frameworks might dictate specific naming schemes for key-values. For example, Java's Spring Cloud framework defines\u00a0<code>Environment</code>\u00a0resources that supply settings to a Spring application. These resources are parameterized by variables that include\u00a0application name\u00a0and\u00a0profile. Keys for Spring Cloud-related configuration data typically start with these two elements separated by a delimiter.</p> <p>Keys stored in App Configuration are case-sensitive, unicode-based strings. The keys\u00a0app1\u00a0and\u00a0App1\u00a0are distinct in an App Configuration store. Keep this in mind when you use configuration settings within an application because some frameworks handle configuration keys case-insensitively.</p> <p>You can use any unicode character in key names entered into App Configuration except for\u00a0<code>*</code>,\u00a0<code>,</code>, and\u00a0<code>\\</code>. These characters are reserved. If you need to include a reserved character, you must escape it by using\u00a0<code>\\{Reserved Character}</code>. There's a combined size limit of 10,000 characters on a key-value pair. This limit includes all characters in the key, its value, and all associated optional attributes. Within this limit, you can have many hierarchical levels for keys.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#design-key-namespaces","title":"Design key namespaces","text":"<p>There are two general approaches to naming keys used for configuration data: flat or hierarchical. These methods are similar from an application usage standpoint, but hierarchical naming offers many advantages:</p> <ul> <li>Easier to read. Instead of one long sequence of characters, delimiters in a hierarchical key name function as spaces in a sentence.</li> <li>Easier to manage. A key name hierarchy represents logical groups of configuration data.</li> <li>Easier to use. It's simpler to write a query that pattern-matches keys in a hierarchical structure and retrieves only a portion of configuration data.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#label-keys","title":"Label keys","text":"<p>Key-values in App Configuration can optionally have a label attribute. Labels are used to differentiate key-values with the same key. A key\u00a0app1\u00a0with labels\u00a0A\u00a0and\u00a0B\u00a0forms two separate keys in an App Configuration store. By default, a key-value has no label. To explicitly reference a key-value without a label, use\u00a0<code>\\0</code>\u00a0(URL encoded as\u00a0<code>%00</code>).</p> <p>Label provides a convenient way to create variants of a key. A common use of labels is to specify multiple environments for the same key:</p> <pre><code>Key = AppName:DbEndpoint &amp; Label = Test\nKey = AppName:DbEndpoint &amp; Label = Staging\nKey = AppName:DbEndpoint &amp; Label = Production\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#version-key-values","title":"Version key values","text":"<p>App Configuration doesn't version key values automatically as they're modified. Use labels as a way to create multiple versions of a key value. For example, you can input an application version number or a Git commit ID in labels to identify key values associated with a particular software build.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#query-key-values","title":"Query key values","text":"<p>Each key-value is uniquely identified by its key plus a label that can be\u00a0<code>\\0</code>. You query an App Configuration store for key-values by specifying a pattern. The App Configuration store returns all key-values that match the pattern including their corresponding values and attributes.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#values","title":"Values","text":"<p>Values assigned to keys are also unicode strings. You can use all unicode characters for values. There's an optional user-defined content type associated with each value. Use this attribute to store information, for example an encoding scheme, about a value that helps your application to process it properly.</p> <p>Configuration data stored in an App Configuration store, which includes all keys and values, is encrypted at rest and in transit. App Configuration isn't a replacement solution for Azure Key Vault. Don't store application secrets in it.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#manage-application-features","title":"Manage application features","text":"<p>Feature management is a modern software-development practice that decouples feature release from code deployment and enables quick changes to feature availability on demand. It uses a technique called feature flags (also known as feature toggles, feature switches, and so on) to dynamically administer a feature's lifecycle.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#basic-concepts","title":"Basic concepts","text":"<p>Here are several new terms related to feature management:</p> <ul> <li>Feature flag: A feature flag is a variable with a binary state of\u00a0on\u00a0or\u00a0off. The feature flag also has an associated code block. The state of the feature flag triggers whether the code block runs or not.</li> <li>Feature manager: A feature manager is an application package that handles the lifecycle of all the feature flags in an application. The feature manager typically provides extra functionality, such as caching feature flags and updating their states.</li> <li>Filter: A filter is a rule for evaluating the state of a feature flag. A user group, a device or browser type, a geographic location, and a time window are all examples of what a filter can represent.</li> </ul> <p>An effective implementation of feature management consists of at least two components working in concert:</p> <ul> <li>An application that makes use of feature flags.</li> <li>A separate repository that stores the feature flags and their current states.</li> </ul> <p>How these components interact is illustrated in the following examples.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#feature-flag-usage-in-code","title":"Feature flag usage in code","text":"<p>The basic pattern for implementing feature flags in an application is simple. You can think of a feature flag as a Boolean state variable used with an\u00a0<code>if</code>\u00a0conditional statement in your code:</p> <pre><code>if (featureFlag) {\n    // Run the following code\n}\n</code></pre> <p>In this case, if\u00a0<code>featureFlag</code>\u00a0is set to\u00a0<code>True</code>, the enclosed code block is executed; otherwise, it's skipped. You can set the value of\u00a0<code>featureFlag</code>\u00a0statically, as in the following code example:</p> <pre><code>bool featureFlag = true;\n</code></pre> <p>You can also evaluate the flag's state based on certain rules:</p> <pre><code>bool featureFlag = isBetaUser();\n</code></pre> <p>You can extend the conditional to set application behavior for either state:</p> <pre><code>if (featureFlag) {\n    // This following code will run if the featureFlag value is true\n} else {\n    // This following code will run if the featureFlag value is false\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#feature-flag-declaration","title":"Feature flag declaration","text":"<p>Each feature flag has two parts: a name and a list of one or more filters that are used to evaluate if a feature's state is\u00a0on\u00a0(that is, when its value is\u00a0<code>True</code>). A filter defines a use case for when a feature should be turned on.</p> <p>When a feature flag has multiple filters, the filter list is traversed in order until one of the filters determines the feature should be enabled. At that point, the feature flag is\u00a0on, and any remaining filter results are skipped. If no filter indicates the feature should be enabled, the feature flag is\u00a0off.</p> <p>The feature manager supports\u00a0appsettings.json\u00a0as a configuration source for feature flags. The following example shows how to set up feature flags in a JSON file:</p> <pre><code>\"FeatureManagement\": {\n    \"FeatureA\": true, // Feature flag set to on\n    \"FeatureB\": false, // Feature flag set to off\n    \"FeatureC\": {\n        \"EnabledFor\": [\n            {\n                \"Name\": \"Percentage\",\n                \"Parameters\": {\n                    \"Value\": 50\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#feature-flag-repository","title":"Feature flag repository","text":"<p>To use feature flags effectively, you need to externalize all the feature flags used in an application. This approach allows you to change feature flag states without modifying and redeploying the application itself.</p> <p>Azure App Configuration is designed to be a centralized repository for feature flags. You can use it to define different kinds of feature flags and manipulate their states quickly and confidently. You can then use the App Configuration libraries for various programming language frameworks to easily access these feature flags from your application.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#secure-app-configuration-data","title":"Secure app configuration data","text":"<p>In this unit you learn how to secure your apps configuration data by using:</p> <ul> <li>Customer-managed keys</li> <li>Private endpoints</li> <li>Managed identities</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#encrypt-configuration-data-by-using-customer-managed-keys","title":"Encrypt configuration data by using customer-managed keys","text":"<p>Azure App Configuration encrypts sensitive information at rest using a 256-bit AES encryption key provided by Microsoft. Every App Configuration instance has its own encryption key managed by the service and used to encrypt sensitive information. Sensitive information includes the values found in key-value pairs. When customer-managed key capability is enabled, App Configuration uses a managed identity assigned to the App Configuration instance to authenticate with Microsoft Entra ID. The managed identity then calls Azure Key Vault and wraps the App Configuration instance's encryption key. The wrapped encryption key is then stored and the unwrapped encryption key is cached within App Configuration for one hour. App Configuration refreshes the unwrapped version of the App Configuration instance's encryption key hourly. This ensures availability under normal operating conditions.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#enable-customer-managed-key-capability","title":"Enable customer-managed key capability","text":"<p>The following components are required to successfully enable the customer-managed key capability for Azure App Configuration:</p> <ul> <li>Standard tier Azure App Configuration instance</li> <li>Azure Key Vault with soft-delete and purge-protection features enabled</li> <li>An RSA or RSA-HSM key within the Key Vault: The key must not be expired, it must be enabled, and it must have both wrap and unwrap capabilities enabled</li> </ul> <p>Once these resources are configured, two steps remain to allow Azure App Configuration to use the Key Vault key:</p> <ol> <li>Assign a managed identity to the Azure App Configuration instance</li> <li>Grant the identity\u00a0<code>GET</code>,\u00a0<code>WRAP</code>, and\u00a0<code>UNWRAP</code>\u00a0permissions in the target Key Vault's access policy.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#use-private-endpoints-for-azure-app-configuration","title":"Use private endpoints for Azure App Configuration","text":"<p>You can use private endpoints for Azure App Configuration to allow clients on a virtual network to securely access data over a private link. The private endpoint uses an IP address from the virtual network address space for your App Configuration store. Network traffic between the clients on the virtual network and the App Configuration store traverses over the virtual network using a private link on the Microsoft backbone network, eliminating exposure to the public internet.</p> <p>Using private endpoints for your App Configuration store enables you to:</p> <ul> <li>Secure your application configuration details by configuring the firewall to block all connections to App Configuration on the public endpoint.</li> <li>Increase security for the virtual network ensuring data doesn't escape.</li> <li>Securely connect to the App Configuration store from on-premises networks that connect to the virtual network using VPN or ExpressRoutes with private-peering.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#managed-identities","title":"Managed identities","text":"<p>A managed identity from Microsoft Entra ID allows Azure App Configuration to easily access other Microsoft Entra ID-protected resources, such as Azure Key Vault. The identity is managed by the Azure platform. It doesn't require you to provision or rotate any secrets.</p> <p>Your application can be granted two types of identities:</p> <ul> <li>A\u00a0system-assigned identity\u00a0is tied to your configuration store. It's deleted if your configuration store is deleted. A configuration store can only have one system-assigned identity.</li> <li>A\u00a0user-assigned identity\u00a0is a standalone Azure resource that can be assigned to your configuration store. A configuration store can have multiple user-assigned identities.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#add-a-system-assigned-identity","title":"Add a system-assigned identity","text":"<p>To set up a managed identity using the Azure CLI, use the\u00a0<code>az appconfig identity assign</code>\u00a0command against an existing configuration store. The following Azure CLI example creates a system-assigned identity for an Azure App Configuration store named\u00a0<code>myTestAppConfigStore</code>.</p> <pre><code>az appconfig identity assign \\ \n    --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2007%20Revision/#add-a-user-assigned-identity","title":"Add a user-assigned identity","text":"<p>Creating an App Configuration store with a user-assigned identity requires that you create the identity and then assign its resource identifier to your store. The following Azure CLI examples create a user-assigned identity called\u00a0<code>myUserAssignedIdentity</code>\u00a0and assign it to an Azure App Configuration store named\u00a0<code>myTestAppConfigStore</code>.</p> <p>Create an identity using the\u00a0<code>az identity create</code>\u00a0command:</p> <pre><code>az identity create --resource-group myResourceGroup --name myUserAssignedIdentity\n</code></pre> <p>Assign the new user-assigned identity to the\u00a0<code>myTestAppConfigStore</code>\u00a0configuration store:</p> <pre><code>az appconfig identity assign --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup \\ \n    --identities /subscriptions/[subscription id]/resourcegroups/myResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myUserAssignedIdentity\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/","title":"Implement API Management","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#explore-api-management","title":"Explore API Management","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Describe the components, and their function, of the API Management service.</li> <li>Explain how API gateways can help manage calls to your APIs.</li> <li>Secure access to APIs by using subscriptions and certificates.</li> <li>Create a backend API.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#discover-the-api-management-service","title":"Discover the API Management service","text":"<p>API Management provides the core functionality to ensure a successful API program through developer engagement, business insights, analytics, security, and protection. Each API consists of one or more operations, and each API can be added to one or more products. To use an API, developers subscribe to a product that contains that API, and then they can call the API's operation, subject to any usage policies that may be in effect.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#api-management-components","title":"API Management components","text":"<p>Azure API Management is made up of an\u00a0API gateway, a\u00a0management plane, and a\u00a0developer portal. These components are Azure-hosted and fully managed by default. API Management is available in various\u00a0tiers\u00a0differing in capacity and features.</p> <ul> <li>The\u00a0API gateway\u00a0is the endpoint that:<ul> <li>Accepts API calls and routes them to appropriate backends</li> <li>Verifies API keys and other credentials presented with requests</li> <li>Enforces usage quotas and rate limits</li> <li>Transforms requests and responses specified in policy statements</li> <li>Caches responses to improve response latency and minimize the load on backend services</li> <li>Emits logs, metrics, and traces for monitoring, reporting, and troubleshooting</li> </ul> </li> <li>The\u00a0management plane\u00a0is the administrative interface where you set up your API program. Use it to:<ul> <li>Provision and configure API Management service settings</li> <li>Define or import API schema</li> <li>Package APIs into products</li> <li>Set up policies like quotas or transformations on the APIs</li> <li>Get insights from analytics</li> <li>Manage users</li> </ul> </li> <li>The\u00a0Developer portal\u00a0is an automatically generated, fully customizable website with the documentation of your APIs. Using the developer portal, developers can:<ul> <li>Read API documentation</li> <li>Call an API via the interactive console</li> <li>Create an account and subscribe to get API keys</li> <li>Access analytics on their own usage</li> <li>Download API definitions</li> <li>Manage API keys</li> </ul> </li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#products","title":"Products","text":"<p>Products are how APIs are surfaced to developers. Products in API Management have one or more APIs, and are configured with a title, description, and terms of use. Products can be\u00a0Open\u00a0or\u00a0Protected. Protected products must be subscribed to before they can be used, while open products can be used without a subscription. Subscription approval is configured at the product level and can either require administrator approval, or be auto approved.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#groups","title":"Groups","text":"<p>Groups are used to manage the visibility of products to developers. API Management has the following immutable system groups:</p> <ul> <li>Administrators\u00a0- Manage API Management service instances and create the APIs, operations, and products that are used by developers. Azure subscription administrators are members of this group.</li> <li>Developers\u00a0- Authenticated developer portal users that build applications using your APIs. Developers are granted access to the developer portal and build applications that call the operations of an API.</li> <li>Guests\u00a0- Unauthenticated developer portal users. They can be granted certain read-only access, like the ability to view APIs but not call them.</li> </ul> <p>In addition to these system groups, administrators can create custom groups or use external groups in associated Microsoft Entra tenants.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#developers","title":"Developers","text":"<p>Developers represent the user accounts in an API Management service instance. Developers can be created or invited to join by administrators, or they can sign up from the Developer portal. Each developer is a member of one or more groups, and can subscribe to the products that grant visibility to those groups.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#policies","title":"Policies","text":"<p>Policies are a collection of statements that are executed sequentially on the request or response of an API. Popular statements include format conversion from XML to JSON and call rate limiting to restrict the number of incoming calls from a developer, and many other policies are available.</p> <p>Policy expressions can be used as attribute values or text values in any of the API Management policies, unless the policy specifies otherwise. Some policies such as the Control flow and Set variable policies are based on policy expressions.</p> <p>Policies can be applied at different scopes, depending on your needs: global (all APIs), a product, a specific API, or an API operation.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#explore-api-gateways","title":"Explore API gateways","text":"<p>Your solution may contain several front- and back-end services. In this scenario, how does a client know what endpoints to call? What happens when new services are introduced, or existing services are refactored? How do services handle SSL termination, authentication, and other concerns?</p> <p>The API Management gateway (also called data plane or runtime) is the service component that's responsible for proxying API requests, applying policies, and collecting telemetry.</p> <p>An API gateway sits between clients and services. It acts as a reverse proxy, routing requests from clients to services. It may also perform various cross-cutting tasks such as authentication, SSL termination, and rate limiting. If you don't deploy a gateway, clients must send requests directly to back-end services. However, there are some potential problems with exposing services directly to clients:</p> <ul> <li>It can result in complex client code. The client must keep track of multiple endpoints, and handle failures in a resilient way.</li> <li>It creates coupling between the client and the backend. The client needs to know how the individual services are decomposed. That makes it harder to maintain the client and also harder to refactor services.</li> <li>A single operation might require calls to multiple services.</li> <li>Each public-facing service must handle concerns such as authentication, SSL, and client rate limiting.</li> <li>Services must expose a client-friendly protocol such as HTTP or WebSocket. This limits the choice of communication protocols.</li> <li>Services with public endpoints are a potential attack surface, and must be hardened.</li> </ul> <p>A gateway helps to address these issues by decoupling clients from services.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#managed-and-self-hosted","title":"Managed and self-hosted","text":"<p>API Management offers both managed and self-hosted gateways:</p> <ul> <li>Managed\u00a0- The managed gateway is the default gateway component that is deployed in Azure for every API Management instance in every service tier. With the managed gateway, all API traffic flows through Azure regardless of where backends implementing the APIs are hosted.</li> <li>Self-hosted\u00a0- The self-hosted gateway is an optional, containerized version of the default managed gateway. It's useful for hybrid and multicloud scenarios where there's a requirement to run the gateways off of Azure in the same environments where API backends are hosted. The self-hosted gateway enables customers with hybrid IT infrastructure to manage APIs hosted on-premises and across clouds from a single API Management service in Azure.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#explore-api-management-policies","title":"Explore API Management policies","text":"<p>In Azure API Management, policies allow the publisher to change the behavior of the API through configuration. Policies are a collection of Statements that are executed sequentially on the request or response of an API.</p> <p>Policies are applied inside the gateway that sits between the API consumer and the managed API. The gateway receives all requests and usually forwards them unaltered to the underlying API. However a policy can apply changes to both the inbound request and outbound response. Policy expressions can be used as attribute values or text values in any of the API Management policies, unless the policy specifies otherwise.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#understanding-policy-configuration","title":"Understanding policy configuration","text":"<p>The policy definition is a simple XML document that describes a sequence of inbound and outbound statements. The XML can be edited directly in the definition window.</p> <p>The configuration is divided into\u00a0<code>inbound</code>,\u00a0<code>backend</code>,\u00a0<code>outbound</code>, and\u00a0<code>on-error</code>. The series of specified policy statements is executed in order for a request and a response.</p> <pre><code>&lt;policies&gt;\n  &lt;inbound&gt;\n    &lt;!-- statements to be applied to the request go here --&gt;\n  &lt;/inbound&gt;\n  &lt;backend&gt;\n    &lt;!-- statements to be applied before the request is forwarded to \n         the backend service go here --&gt;\n  &lt;/backend&gt;\n  &lt;outbound&gt;\n    &lt;!-- statements to be applied to the response go here --&gt;\n  &lt;/outbound&gt;\n  &lt;on-error&gt;\n    &lt;!-- statements to be applied if there is an error condition go here --&gt;\n  &lt;/on-error&gt;\n&lt;/policies&gt;\n</code></pre> <p>If there's an error during the processing of a request, any remaining steps in the\u00a0<code>inbound</code>,\u00a0<code>backend</code>, or\u00a0<code>outbound</code>\u00a0sections are skipped and execution jumps to the statements in the\u00a0<code>on-error</code>\u00a0section. By placing policy statements in the\u00a0<code>on-error</code>\u00a0section you can review the error by using the\u00a0<code>context.LastError</code>\u00a0property, inspect and customize the error response using the\u00a0<code>set-body</code>\u00a0policy, and configure what happens if an error occurs.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#policy-expressions","title":"Policy expressions","text":"<p>Unless the policy specifies otherwise, policy expressions can be used as attribute values or text values in any of the API Management policies. A policy expression is either:</p> <ul> <li>a single C# statement enclosed in\u00a0<code>@(expression)</code>, or</li> <li>a multi-statement C# code block, enclosed in\u00a0<code>@{expression}</code>, that returns a value</li> </ul> <p>Each expression has access to the implicitly provided\u00a0<code>context</code>\u00a0variable and an allowed subset of .NET Framework types.</p> <p>Policy expressions\u00a0provide a sophisticated means to control traffic and modify API behavior without requiring you to write specialized code or modify backend services.</p> <p>The following example uses policy expressions and the set-header policy to add user data to the incoming request. The added header includes the user ID associated with the subscription key in the request, and the region where the gateway processing the request is hosted.</p> <pre><code>&lt;policies&gt;\n    &lt;inbound&gt;\n        &lt;base /&gt;\n        &lt;set-header name=\"x-request-context-data\" exists-action=\"override\"&gt;\n            &lt;value&gt;@(context.User.Id)&lt;/value&gt;\n            &lt;value&gt;@(context.Deployment.Region)&lt;/value&gt;\n      &lt;/set-header&gt;\n    &lt;/inbound&gt;\n&lt;/policies&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#apply-policies-specified-at-different-scopes","title":"Apply policies specified at different scopes","text":"<p>If you have a policy at the global level and a policy configured for an API, then whenever that particular API is used both policies are applied. API Management allows for deterministic ordering of combined policy statements via the base element.</p> <pre><code>&lt;policies&gt;\n    &lt;inbound&gt;\n        &lt;cross-domain /&gt;\n        &lt;base /&gt;\n        &lt;find-and-replace from=\"xyz\" to=\"abc\" /&gt;\n    &lt;/inbound&gt;\n&lt;/policies&gt;\n</code></pre> <p>In the previous example policy definition, The\u00a0<code>cross-domain</code>\u00a0statement would execute first. The\u00a0<code>find-and-replace</code>\u00a0policy would execute after any policies at a broader scope.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#filter-response-content","title":"Filter response content","text":"<p>The policy defined in following example demonstrates how to filter data elements from the response payload based on the product associated with the request.</p> <p>The snippet assumes that response content is formatted as JSON and contains root-level properties named \"minutely\", \"hourly\", \"daily\", \"flags\".</p> <pre><code>&lt;policies&gt;\n  &lt;inbound&gt;\n    &lt;base /&gt;\n  &lt;/inbound&gt;\n  &lt;backend&gt;\n    &lt;base /&gt;\n  &lt;/backend&gt;\n  &lt;outbound&gt;\n    &lt;base /&gt;\n    &lt;choose&gt;\n      &lt;when condition=\"@(context.Response.StatusCode == 200 &amp;&amp; context.Product.Name.Equals(\"Starter\"))\"&gt;\n        &lt;!-- NOTE that we are not using preserveContent=true when deserializing response body stream into a JSON object since we don't intend to access it again. See details on /azure/api-management/api-management-transformation-policies#SetBody --&gt;\n        &lt;set-body&gt;\n          @{\n            var response = context.Response.Body.As&lt;JObject&gt;();\n            foreach (var key in new [] {\"minutely\", \"hourly\", \"daily\", \"flags\"}) {\n            response.Property (key).Remove ();\n           }\n          return response.ToString();\n          }\n    &lt;/set-body&gt;\n      &lt;/when&gt;\n    &lt;/choose&gt;    \n  &lt;/outbound&gt;\n  &lt;on-error&gt;\n    &lt;base /&gt;\n  &lt;/on-error&gt;\n&lt;/policies&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#create-advanced-policies","title":"Create advanced policies","text":"<p>This unit provides a reference for the following API Management policies:</p> <ul> <li>Control flow - Conditionally applies policy statements based on the results of the evaluation of Boolean expressions.</li> <li>Forward request - Forwards the request to the backend service.</li> <li>Limit concurrency - Prevents enclosed policies from executing by more than the specified number of requests at a time.</li> <li>Log to Event Hub - Sends messages in the specified format to an Event Hub defined by a Logger entity.</li> <li>Mock response - Aborts pipeline execution and returns a mocked response directly to the caller.</li> <li>Retry - Retries execution of the enclosed policy statements, if and until the condition is met. Execution will repeat at the specified time intervals and up to the specified retry count.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#control-flow","title":"Control flow","text":"<p>The\u00a0<code>choose</code>\u00a0policy applies enclosed policy statements based on the outcome of evaluation of boolean expressions, similar to an if-then-else or a switch construct in a programming language.</p> <pre><code>&lt;choose&gt;\n    &lt;when condition=\"Boolean expression | Boolean constant\"&gt;\n        &lt;!\u2014 one or more policy statements to be applied if the above condition is true  --&gt;\n    &lt;/when&gt;\n    &lt;when condition=\"Boolean expression | Boolean constant\"&gt;\n        &lt;!\u2014 one or more policy statements to be applied if the above condition is true  --&gt;\n    &lt;/when&gt;\n    &lt;otherwise&gt;\n        &lt;!\u2014 one or more policy statements to be applied if none of the above conditions are true  --&gt;\n    &lt;/otherwise&gt;\n&lt;/choose&gt;\n</code></pre> <p>The control flow policy must contain at least one\u00a0<code>&lt;when/&gt;</code>\u00a0element. The\u00a0<code>&lt;otherwise/&gt;</code>\u00a0element is optional. Conditions in\u00a0<code>&lt;when/&gt;</code>\u00a0elements are evaluated in order of their appearance within the policy. Policy statement(s) enclosed within the first\u00a0<code>&lt;when/&gt;</code>\u00a0element with condition attribute equals true will be applied. Policies enclosed within the\u00a0<code>&lt;otherwise/&gt;</code>\u00a0element, if present, will be applied if all of the\u00a0<code>&lt;when/&gt;</code>\u00a0element condition attributes are false.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#forward-request","title":"Forward request","text":"<p>The\u00a0<code>forward-request</code>\u00a0policy forwards the incoming request to the backend service specified in the request context. The backend service URL is specified in the API settings and can be changed using the set backend service policy.</p> <p>Removing this policy results in the request not being forwarded to the backend service and the policies in the outbound section are evaluated immediately upon the successful completion of the policies in the inbound section.</p> <pre><code>&lt;forward-request timeout=\"time in seconds\" follow-redirects=\"true | false\"/&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#limit-concurrency","title":"Limit concurrency","text":"<p>The\u00a0<code>limit-concurrency</code>\u00a0policy prevents enclosed policies from executing by more than the specified number of requests at any time. Upon exceeding that number, new requests will fail immediately with a\u00a0429 Too Many Requests\u00a0status code.</p> <pre><code>&lt;limit-concurrency key=\"expression\" max-count=\"number\"&gt;\n    &lt;!\u2014 nested policy statements --&gt;\n&lt;/limit-concurrency&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#log-to-event-hub","title":"Log to Event Hub","text":"<p>The\u00a0<code>log-to-eventhub</code>\u00a0policy sends messages in the specified format to an Event Hub defined by a Logger entity. As its name implies, the policy is used for saving selected request or response context information for online or offline analysis.</p> <pre><code>&lt;log-to-eventhub logger-id=\"id of the logger entity\" partition-id=\"index of the partition where messages are sent\" partition-key=\"value used for partition assignment\"&gt;\n  Expression returning a string to be logged\n&lt;/log-to-eventhub&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#mock-response","title":"Mock response","text":"<p>The\u00a0<code>mock-response</code>, as the name implies, is used to mock APIs and operations. It aborts normal pipeline execution and returns a mocked response to the caller. The policy always tries to return responses of highest fidelity. It prefers response content examples, whenever available. It generates sample responses from schemas, when schemas are provided and examples are not. If neither examples or schemas are found, responses with no content are returned.</p> <pre><code>&lt;mock-response status-code=\"code\" content-type=\"media type\"/&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#retry","title":"Retry","text":"<p>The\u00a0<code>retry</code>\u00a0policy executes its child policies once and then retries their execution until the retry\u00a0<code>condition</code>\u00a0becomes\u00a0<code>false</code>\u00a0or retry\u00a0<code>count</code>\u00a0is exhausted.</p> <pre><code>&lt;retry\n    condition=\"boolean expression or literal\"\n    count=\"number of retry attempts\"\n    interval=\"retry interval in seconds\"\n    max-interval=\"maximum retry interval in seconds\"\n    delta=\"retry interval delta in seconds\"\n    first-fast-retry=\"boolean expression or literal\"&gt;\n        &lt;!-- One or more child policies. No restrictions --&gt;\n&lt;/retry&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#return-response","title":"Return response","text":"<p>The\u00a0<code>return-response</code>\u00a0policy aborts pipeline execution and returns either a default or custom response to the caller. Default response is\u00a0<code>200 OK</code>\u00a0with no body. Custom response can be specified via a context variable or policy statements. When both are provided, the response contained within the context variable is modified by the policy statements before being returned to the caller.</p> <pre><code>&lt;return-response response-variable-name=\"existing context variable\"&gt;\n  &lt;set-header/&gt;\n  &lt;set-body/&gt;\n  &lt;set-status/&gt;\n&lt;/return-response&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#additional-resources","title":"Additional resources","text":"<ul> <li>Visit\u00a0API Management policies\u00a0for more policy examples.</li> <li>Error handling in API Management policies</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#secure-apis-by-using-subscriptions","title":"Secure APIs by using subscriptions","text":"<p>When you publish APIs through API Management, it's easy and common to secure access to those APIs by using subscription keys. Developers who need to consume the published APIs must include a valid subscription key in HTTP requests when they make calls to those APIs. Otherwise, the calls are rejected immediately by the API Management gateway. They aren't forwarded to the back-end services.</p> <p>To get a subscription key for accessing APIs, a subscription is required. A subscription is essentially a named container for a pair of subscription keys. Developers who need to consume the published APIs can get subscriptions. And they don't need approval from API publishers. API publishers can also create subscriptions directly for API consumers.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#subscriptions-and-keys","title":"Subscriptions and Keys","text":"<p>A subscription key is a unique auto-generated key that can be passed through in the headers of the client request or as a query string parameter. The key is directly related to a subscription, which can be scoped to different areas. Subscriptions give you granular control over permissions and policies.</p> <p>The three main subscription scopes are:</p> Scope Details All APIs Applies to every API accessible from the gateway Single API This scope applies to a single imported API and all of its endpoints Product A product is a collection of one or more APIs that you configure in API Management. You can assign APIs to more than one product. Products can have different access rules, usage quotas, and terms of use. <p>Applications that call a protected API must include the key in every request.</p> <p>You can regenerate these subscription keys at any time, for example, if you suspect that a key has been shared with unauthorized users.</p> <p></p> <p>Every subscription has two keys, a primary and a secondary. Having two keys makes it easier when you do need to regenerate a key. For example, if you want to change the primary key and avoid downtime, use the secondary key in your apps.</p> <p>For products where subscriptions are enabled, clients must supply a key when making calls to APIs in that product. Developers can obtain a key by submitting a subscription request. If you approve the request, you must send them the subscription key securely, for example, in an encrypted message. This step is a core part of the API Management workflow.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#call-an-api-with-the-subscription-key","title":"Call an API with the subscription key","text":"<p>Applications must include a valid key in all HTTP requests when they make calls to API endpoints that are protected by a subscription. Keys can be passed in the request header, or as a query string in the URL.</p> <p>The default header name is\u00a0Ocp-Apim-Subscription-Key, and the default query string is\u00a0subscription-key.</p> <p>To test out your API calls, you can use the developer portal, or command-line tools, such as\u00a0curl. Here's an example of a\u00a0<code>GET</code>\u00a0request using the developer portal, which shows the subscription key header:</p> <p></p> <p>Here's how you can pass a key in the request header using\u00a0curl:</p> <pre><code>curl --header \"Ocp-Apim-Subscription-Key: &lt;key string&gt;\" https://&lt;apim gateway&gt;.azure-api.net/api/path\n</code></pre> <p>Here's an example\u00a0curl\u00a0command that passes a key in the URL as a query string:</p> <pre><code>curl https://&lt;apim gateway&gt;.azure-api.net/api/path?subscription-key=&lt;key string&gt;\n</code></pre> <p>If the key is not passed in the header, or as a query string in the URL, you'll get a\u00a0401 Access Denied\u00a0response from the API gateway.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#secure-apis-by-using-certificates","title":"Secure APIs by using certificates","text":"<p>Certificates can be used to provide Transport Layer Security (TLS) mutual authentication between the client and the API gateway. You can configure the API Management gateway to allow only requests with certificates containing a specific thumbprint. The authorization at the gateway level is handled through inbound policies.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#transport-layer-security-client-authentication","title":"Transport Layer Security client authentication","text":"<p>With TLS client authentication, the API Management gateway can inspect the certificate contained within the client request and check for properties like:</p> Property Description Certificate Authority (CA) Only allow certificates signed by a particular CA Thumbprint Allow certificates containing a specified thumbprint Subject Only allow certificates with a specified subject Expiration Date Only allow certificates that have not expired <p>These properties are not mutually exclusive and they can be mixed together to form your own policy requirements. For instance, you can specify that the certificate passed in the request is signed by a certain certificate authority and hasn't expired.</p> <p>Client certificates are signed to ensure that they are not tampered with. When a partner sends you a certificate, verify that it comes from them and not an imposter. There are two common ways to verify a certificate:</p> <ul> <li>Check who issued the certificate. If the issuer was a certificate authority that you trust, you can use the certificate. You can configure the trusted certificate authorities in the Azure portal to automate this process.</li> <li>If the certificate is issued by the partner, verify that it came from them. For example, if they deliver the certificate in person, you can be sure of its authenticity. These are known as self-signed certificates.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#accepting-client-certificates-in-the-consumption-tier","title":"Accepting client certificates in the Consumption tier","text":"<p>The Consumption tier in API Management is designed to conform with serverless design principals. If you build your APIs from serverless technologies, such as Azure Functions, this tier is a good fit. In the Consumption tier, you must explicitly enable the use of client certificates, which you can do on the\u00a0Custom domains\u00a0page. This step is not necessary in other tiers.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#certificate-authorization-policies","title":"Certificate Authorization Policies","text":"<p>Create these policies in the inbound processing policy file within the API Management gateway:</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#check-the-thumbprint-of-a-client-certificate","title":"Check the thumbprint of a client certificate","text":"<p>Every client certificate includes a thumbprint, which is a hash, calculated from other certificate properties. The thumbprint ensures that the values in the certificate have not been altered since the certificate was issued by the certificate authority. You can check the thumbprint in your policy. The following example checks the thumbprint of the certificate passed in the request:</p> <pre><code>&lt;choose&gt;\n    &lt;when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Thumbprint != \"desired-thumbprint\")\" &gt;\n        &lt;return-response&gt;\n            &lt;set-status code=\"403\" reason=\"Invalid client certificate\" /&gt;\n        &lt;/return-response&gt;\n    &lt;/when&gt;\n&lt;/choose&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#check-the-thumbprint-against-certificates-uploaded-to-api-management","title":"Check the thumbprint against certificates uploaded to API Management","text":"<p>In the previous example, only one thumbprint would work so only one certificate would be validated. Usually, each customer or partner company would pass a different certificate with a different thumbprint. To support this scenario, obtain the certificates from your partners and use the\u00a0Client certificates\u00a0page in the Azure portal to upload them to the API Management resource. Then add this code to your policy:</p> <pre><code>&lt;choose&gt;\n    &lt;when condition=\"@(context.Request.Certificate == null || !context.Request.Certificate.Verify()  || !context.Deployment.Certificates.Any(c =&gt; c.Value.Thumbprint == context.Request.Certificate.Thumbprint))\" &gt;\n        &lt;return-response&gt;\n            &lt;set-status code=\"403\" reason=\"Invalid client certificate\" /&gt;\n        &lt;/return-response&gt;\n    &lt;/when&gt;\n&lt;/choose&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#check-the-issuer-and-subject-of-a-client-certificate","title":"Check the issuer and subject of a client certificate","text":"<p>This example checks the issuer and subject of the certificate passed in the request:</p> <pre><code>&lt;choose&gt;\n    &lt;when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Issuer != \"trusted-issuer\" || context.Request.Certificate.SubjectName.Name != \"expected-subject-name\")\" &gt;\n        &lt;return-response&gt;\n            &lt;set-status code=\"403\" reason=\"Invalid client certificate\" /&gt;\n        &lt;/return-response&gt;\n    &lt;/when&gt;\n&lt;/choose&gt;\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#exercise-create-a-backend-api","title":"Exercise: Create a backend API","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#create-an-api-management-instance","title":"Create an API management instance","text":"<p>Let's set some variables for the CLI commands to use to reduce the amount of retyping. Replace\u00a0<code>&lt;myLocation&gt;</code>\u00a0with a region that makes sense for you. The APIM name needs to be a globally unique name, and the following script generates a random string. Replace\u00a0<code>&lt;myEmail&gt;</code>\u00a0with an email address you can access.</p> <pre><code>myApiName=az204-apim-$RANDOM\nmyLocation=&lt;myLocation&gt;\nmyEmail=&lt;myEmail&gt;\n</code></pre> <p>Create a resource group. The following commands create a resource group named\u00a0az204-apim-rg.</p> <pre><code>az group create --name az204-apim-rg --location $myLocation\n</code></pre> <p>Create an APIM instance. The\u00a0<code>az apim create</code>\u00a0command is used to create the instance. The\u00a0<code>--sku-name Consumption</code>\u00a0option is used to speed up the process for the walkthrough.</p> <pre><code>az apim create -n $myApiName \\\n    --location $myLocation \\\n    --publisher-email $myEmail  \\\n    --resource-group az204-apim-rg \\\n    --publisher-name AZ204-APIM-Exercise \\\n    --sku-name Consumption \n</code></pre> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#import-a-backend-api","title":"Import a backend API","text":"<p>This section shows how to import and publish an OpenAPI specification backend API.</p> <ol> <li>In the Azure portal, search for and select\u00a0API Management services.</li> <li>On the\u00a0API Management\u00a0screen, select the API Management instance you created.</li> <li> <p>Select\u00a0APIs\u00a0in the\u00a0API management service\u00a0navigation pane.</p> <p></p> </li> <li> <p>Select\u00a0OpenAPI\u00a0from the list and select\u00a0Full\u00a0in the pop-up.</p> <p></p> <p>Use the values from the table below to fill out the form. You can leave any fields not mentioned their default value.</p> </li> </ol> Setting Value Description OpenAPI Specification <code>https://conferenceapi.azurewebsites.net?format=json</code> References the service implementing the API, requests are forwarded to this address. Most of the necessary information in the form is automatically populated after you enter this. Display name Demo Conference API This name is displayed in the Developer portal. Name demo-conference-api Provides a unique name for the API. Description Automatically populated Provide an optional description of the API. API URL suffix conference The suffix is appended to the base URL for the API management service. API Management distinguishes APIs by their suffix and therefore the suffix must be unique for every API for a given publisher."},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#configure-the-backend-settings","title":"Configure the backend settings","text":"<p>The\u00a0Demo Conference API\u00a0is created and a backend needs to be specified.</p> <ol> <li>Select\u00a0Settings\u00a0in the blade to the right and enter\u00a0<code>https://conferenceapi.azurewebsites.net/</code>\u00a0in the\u00a0Web service URL\u00a0field.</li> <li>Deselect the\u00a0Subscription required\u00a0checkbox.     </li> <li>Select\u00a0Save.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#test-the-api","title":"Test the API","text":"<p>Now that the API has been imported and the backend configured it's time to test the API.</p> <ol> <li> <p>Select\u00a0Test.</p> <p></p> </li> <li> <p>Select\u00a0GetSpeakers. The page shows\u00a0Query parameters\u00a0and\u00a0Headers, if any. The\u00a0<code>Ocp-Apim-Subscription-Key</code>\u00a0is filled in automatically for the subscription key associated with this API.</p> </li> <li>Select\u00a0Send.     Backend responds with\u00a0200 OK\u00a0and some data.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2008%20Revision/#clean-up-azure-resources","title":"Clean up Azure resources","text":"<p>When you're finished with the resources you created in this exercise you can use the command below to delete the resource group and all related resources.</p> <pre><code>az group delete --name az204-apim-rg\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/","title":"Develop event-based solutions","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#explore-azure-event-grid","title":"Explore Azure Event Grid","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Describe how Event Grid operates and how it connects to services and event handlers.</li> <li>Explain how Event Grid delivers events and how it handles errors.</li> <li>Implement authentication and authorization.</li> <li>Route custom events to web endpoint by using Azure CLI.</li> </ul> <p>Azure Event Grid is a highly scalable, fully managed Pub Sub message distribution service that offers flexible message consumption patterns using the Hypertext Transfer Protocol (HTTP) and Message Queuing Telemetry Transport (MQTT) protocols. With Azure Event Grid, you can build data pipelines with device data, integrate applications, and build event-driven serverless architectures. Event Grid enables clients to publish and subscribe to messages over the MQTT v3.1.1 and v5.0 protocols to support Internet of Things (IoT) solutions. Through HTTP, Event Grid enables you to build event-driven solutions where a publisher service announces its system state changes (events) to subscriber applications. Event Grid can be configured to send events to subscribers (push delivery) or subscribers can connect to Event Grid to read events (pull delivery). Event Grid supports CloudEvents 1.0 specification to provide interoperability across systems.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#concepts-in-azure-event-grid","title":"Concepts in Azure Event Grid","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#publishers","title":"Publishers","text":"<p>A publisher is the application that sends events to Event Grid. It can be the same application where the events originated, the event source. Azure services publish events to Event Grid to announce an occurrence in their service. You can publish events from your own application. Organizations that host services outside of Azure can publish events through Event Grid too.</p> <p>A\u00a0partner\u00a0is a kind of publisher that sends events from its system to make them available to Azure customers. Partners not only can publish events to Azure Event Grid, but they can also receive events from it. These capabilities are enabled through the Partner Events feature.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#events-and-cloudevents","title":"Events and CloudEvents","text":"<p>An event is the smallest amount of information that fully describes something that happened in a system. Every event has common information like\u00a0<code>source</code>\u00a0of the event, the\u00a0<code>time</code>\u00a0the event took place, and a unique identifier. Every event also has specific information that is only relevant to the specific type of event.</p> <p>Event Grid conforms to Cloud Native Computing Foundation\u2019s open standard\u00a0CloudEvents 1.0\u00a0specification using the\u00a0HTTP protocol binding\u00a0with\u00a0JSON format. It means that your solutions publish and consume event messages using a format like the following example:</p> <pre><code>{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.yourcompany.order.created\",\n    \"source\" : \"https://yourcompany.com/orders/\",\n    \"subject\" : \"O-28964\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"application/json\",\n    \"data\" : {\n       \"orderId\" : \"O-28964\",\n       \"URL\" : \"https://com.yourcompany/orders/O-28964\"\n    }\n}\n</code></pre> <p>The maximum allowed size for an event is 1 MB. Events over 64 KB are charged in 64-KB increments.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#event-sources","title":"Event sources","text":"<p>An event source is where the event happens. Each event source is related to one or more event types. For example, Azure Storage is the event source for blob created events. IoT Hub is the event source for device created events. Your application is the event source for custom events that you define. Event sources are responsible for sending events to Event Grid.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#topics","title":"Topics","text":"<p>A topic holds events that have been published to Event Grid. You typically use a topic resource for a collection of related events. To respond to certain types of events, subscribers (an Azure service or other applications) decide which topics to subscribe to. There are several kinds of topics: custom topics, system topics, and partner topics.</p> <p>System topics\u00a0are built-in topics provided by Azure services. You don't see system topics in your Azure subscription because the publisher owns the topics, but you can subscribe to them. To subscribe, you provide information about the resource you want to receive events from. As long as you have access to the resource, you can subscribe to its events.</p> <p>Custom topics\u00a0are application and third-party topics. When you create or are assigned access to a custom topic, you see that custom topic in your subscription.</p> <p>Partner topics\u00a0are a kind of topic used to subscribe to events published by a partner. The feature that enables this type of integration is called Partner Events. Through that integration, you get a partner topic where events from a partner system are made available. Once you have a partner topic, you create an event subscription as you would do for any other type of topic.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#event-subscriptions","title":"Event subscriptions","text":"<p>A subscription tells Event Grid which events on a topic you're interested in receiving. When creating the subscription, you provide an endpoint for handling the event. You can filter the events that are sent to the endpoint. You can filter by event type, or subject pattern. Set an expiration for event subscriptions that are only needed for a limited time and you don't want to worry about cleaning up those subscriptions.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#event-handlers","title":"Event handlers","text":"<p>From an Event Grid perspective, an event handler is the place where the event is sent. The handler takes some further action to process the event. Event Grid supports several handler types. You can use a supported Azure service or your own webhook as the handler. Depending on the type of handler, Event Grid follows different mechanisms to guarantee the delivery of the event. For HTTP webhook event handlers, the event is retried until the handler returns a status code of\u00a0<code>200 \u2013 OK</code>. For Azure Storage Queue, the events are retried until the Queue service successfully processes the message push into the queue.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#security","title":"Security","text":"<p>Event Grid provides security for subscribing to topics and when publishing events to topics. When subscribing, you must have adequate permissions on the Event Grid topic. If using push delivery, the event handler is an Azure service, and a managed identity is used to authenticate Event Grid, the managed identity should have an appropriate RBAC role. For example, if sending events to Event Hubs, the managed identity used in the event subscription should be a member of the Event Hubs Data Sender role.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#discover-event-schemas","title":"Discover event schemas","text":"<p>Azure Event Grid supports two types of event shemas: Event Grid event schema and Cloud event schema. Events consist of a set of four required string properties. The properties are common to all events from any publisher.</p> <p>The data object has properties that are specific to each publisher. For system topics, these properties are specific to the resource provider, such as Azure Storage or Azure Event Hubs.</p> <p>Event sources send events to Azure Event Grid in an array, which can have several event objects. When posting events to an Event Grid topic, the array can have a total size of up to 1 MB. Each event in the array is limited to 1 MB. If an event or the array is greater than the size limits, you receive the response\u00a0<code>413 Payload Too Large</code>. Operations are charged in 64 KB increments though. So, events over 64 KB incur operations charges as though they were multiple events. For example, an event that is 130 KB would incur charges as though it were three separate events.</p> <p>Event Grid sends the events to subscribers in an array that has a single event. You can find the JSON schema for the Event Grid event and each Azure publisher's data payload in the\u00a0Event Schema store.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#event-schema","title":"Event schema","text":"<p>The following example shows the properties that are used by all event publishers:</p> <pre><code>[\n  {\n    \"topic\": string,\n    \"subject\": string,\n    \"id\": string,\n    \"eventType\": string,\n    \"eventTime\": string,\n    \"data\":{\n      object-unique-to-each-publisher\n    },\n    \"dataVersion\": string,\n    \"metadataVersion\": string\n  }\n]\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#event-properties","title":"Event properties","text":"<p>All events have the same following top-level data:</p> Property Type Required Description topic string No. If not included, Event Grid stamps onto the event. If included, it must match the Event Grid topic Azure Resource Manager ID exactly. Full resource path to the event source. This field isn't writeable. Event Grid provides this value. subject string Yes Publisher-defined path to the event subject. eventType string Yes One of the registered event types for this event source. eventTime string Yes The time the event is generated based on the provider's UTC time. id string Yes Unique identifier for the event. data object No Event data specific to the resource provider. dataVersion string No. If not included, it's stamped with an empty value. The schema version of the data object. The publisher defines the schema version. metadataVersion string No. If not included, Event Grid stamps onto the event. If included, must match the Event Grid Schema\u00a0<code>metadataVersion</code>\u00a0exactly (currently, only\u00a0<code>1</code>). The schema version of the event metadata. Event Grid defines the schema of the top-level properties. Event Grid provides this value. <p>For custom topics, the event publisher determines the data object. The top-level data should have the same fields as standard resource-defined events.</p> <p>When publishing events to custom topics, create subjects for your events that make it easy for subscribers to know whether they're interested in the event. Subscribers use the subject to filter and route events. Consider providing the path for where the event happened, so subscribers can filter by segments of that path. The path enables subscribers to narrowly or broadly filter events. For example, if you provide a three segment path like\u00a0<code>/A/B/C</code>\u00a0in the subject, subscribers can filter by the first segment\u00a0<code>/A</code>\u00a0to get a broad set of events. Those subscribers get events with subjects like\u00a0<code>/A/B/C</code>\u00a0or\u00a0<code>/A/D/E</code>. Other subscribers can filter by\u00a0<code>/A/B</code>\u00a0to get a narrower set of events.</p> <p>Sometimes your subject needs more detail about what happened. For example, the\u00a0Storage Accounts\u00a0publisher provides the subject\u00a0<code>/blobServices/default/containers/&lt;container-name&gt;/blobs/&lt;file&gt;</code>\u00a0when a file is added to a container. A subscriber could filter by the path\u00a0<code>/blobServices/default/containers/testcontainer</code>\u00a0to get all events for that container but not other containers in the storage account. A subscriber could also filter or route by the suffix\u00a0<code>.txt</code>\u00a0to only work with text files.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#cloud-events-schema","title":"Cloud events schema","text":"<p>In addition to its default event schema, Azure Event Grid natively supports events in the JSON implementation of CloudEvents v1.0 and HTTP protocol binding. CloudEvents is an open specification for describing event data.</p> <p>CloudEvents simplifies interoperability by providing a common event schema for publishing, and consuming cloud based events. This schema allows for uniform tooling, standard ways of routing &amp; handling events, and universal ways of deserializing the outer event schema. With a common schema, you can more easily integrate work across platforms.</p> <p>Here's an example of an Azure Blob Storage event in CloudEvents format:</p> <pre><code>{\n    \"specversion\": \"1.0\",\n    \"type\": \"Microsoft.Storage.BlobCreated\",  \n    \"source\": \"/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.Storage/storageAccounts/{storage-account}\",\n    \"id\": \"9aeb0fdf-c01e-0131-0922-9eb54906e209\",\n    \"time\": \"2019-11-18T15:13:39.4589254Z\",\n    \"subject\": \"blobServices/default/containers/{storage-container}/blobs/{new-file}\",\n    \"dataschema\": \"#\",\n    \"data\": {\n        \"api\": \"PutBlockList\",\n        \"clientRequestId\": \"4c5dd7fb-2c48-4a27-bb30-5361b5de920a\",\n        \"requestId\": \"9aeb0fdf-c01e-0131-0922-9eb549000000\",\n        \"eTag\": \"0x8D76C39E4407333\",\n        \"contentType\": \"image/png\",\n        \"contentLength\": 30699,\n        \"blobType\": \"BlockBlob\",\n        \"url\": \"https://gridtesting.blob.core.windows.net/testcontainer/{new-file}\",\n        \"sequencer\": \"000000000000000000000000000099240000000000c41c18\",\n        \"storageDiagnostics\": {\n            \"batchId\": \"681fe319-3006-00a8-0022-9e7cde000000\"\n        }\n    }\n}\n</code></pre> <p>A detailed description of the available fields, their types, and definitions in CloudEvents v1.0 is\u00a0available here.</p> <p>The headers values for events delivered in the CloudEvents schema and the Event Grid schema are the same except for\u00a0<code>content-type</code>. For CloudEvents schema, that header value is\u00a0<code>\"content-type\":\"application/cloudevents+json; charset=utf-8\"</code>. For Event Grid schema, that header value is\u00a0<code>\"content-type\":\"application/json; charset=utf-8\"</code>.</p> <p>You can use Event Grid for both input and output of events in CloudEvents schema. You can use CloudEvents for system events, like Blob Storage events and IoT Hub events, and custom events. It can also transform those events on the wire back and forth.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#explore-event-delivery-durability","title":"Explore event delivery durability","text":"<p>Event Grid provides durable delivery. It tries to deliver each event at least once for each matching subscription immediately. If a subscriber's endpoint doesn't acknowledge receipt of an event or if there's a failure, Event Grid retries delivery based on a fixed retry schedule and retry policy. By default, Event Grid delivers one event at a time to the subscriber, and the payload is an array with a single event.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#retry-schedule","title":"Retry schedule","text":"<p>When Event Grid receives an error for an event delivery attempt, Event Grid decides whether it should: retry the delivery, dead-letter the event, or drop the event based on the type of the error.</p> <p>If the error returned by the subscribed endpoint is a configuration-related error that can't be fixed with retries, Event Grid will either: perform dead-lettering on the event, or drop the event if dead-letter isn't configured.</p> <p>The following table describes the types of endpoints and errors for which retry doesn't happen:</p> Endpoint Type Error codes Azure Resources 400 (Bad request), 413 (Request entity is too large) Webhook 400 (Bad request), 413 (Request entity is too large), 401 (Unauthorized) <p></p> <p>If the error returned by the subscribed endpoint isn't among the previous list, Event Grid waits 30 seconds for a response after delivering a message. After 30 seconds, if the endpoint hasn\u2019t responded, the message is queued for retry. Event Grid uses an exponential backoff retry policy for event delivery.</p> <p>If the endpoint responds within 3 minutes, Event Grid attempts to remove the event from the retry queue on a best effort basis but duplicates might still be received. Event Grid adds a small randomization to all retry steps and might opportunistically skip certain retries if an endpoint is consistently unhealthy, down for a long period, or appears to be overwhelmed.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#retry-policy","title":"Retry policy","text":"<p>You can customize the retry policy when creating an event subscription by using the following two configurations. An event is dropped if either of the limits of the retry policy is reached.</p> <ul> <li>Maximum number of attempts\u00a0- The value must be an integer between 1 and 30. The default value is 30.</li> <li>Event time-to-live (TTL)\u00a0- The value must be an integer between 1 and 1440. The default value is 1440 minutes</li> </ul> <p>The following example shows setting the maximum number of attempts to 18 by using the Azure CLI.</p> <pre><code>az eventgrid event-subscription create \\\n  -g gridResourceGroup \\\n  --topic-name &lt;topic_name&gt; \\\n  --name &lt;event_subscription_name&gt; \\\n  --endpoint &lt;endpoint_URL&gt; \\\n  --max-delivery-attempts 18\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#output-batching","title":"Output batching","text":"<p>You can configure Event Grid to batch events for delivery for improved HTTP performance in high-throughput scenarios. Batching is turned off by default and can be turned on by subscription via the portal, CLI, PowerShell, or SDKs.</p> <p>Batched delivery has two settings:</p> <ul> <li>Max events per batch\u00a0- Maximum number of events Event Grid delivers per batch. This number won't be exceeded, however fewer events might be delivered if no other events are available at the time of publish. Event Grid doesn't delay events to create a batch if fewer events are available. Must be between 1 and 5,000.</li> <li>Preferred batch size in kilobytes\u00a0- Target ceiling for batch size in kilobytes. Similar to max events, the batch size might be smaller if more events aren't available at the time of publish. It's possible that a batch is larger than the preferred batch size\u00a0if\u00a0a single event is larger than the preferred size. For example, if the preferred size is 4 KB and a 10-KB event is pushed to Event Grid, the 10-KB event is delivered in its own batch rather than being dropped.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#delayed-delivery","title":"Delayed delivery","text":"<p>As an endpoint experiences delivery failures, Event Grid begins to delay the delivery and retry of events to that endpoint. For example, if the first 10 events published to an endpoint fail, Event Grid assumes that the endpoint is experiencing issues and delays all subsequent retries, and new deliveries, for some time - in some cases up to several hours.</p> <p>The functional purpose of delayed delivery is to protect unhealthy endpoints and the Event Grid system. Without back-off and delay of delivery to unhealthy endpoints, Event Grid's retry policy and volume capabilities can easily overwhelm a system.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#dead-letter-events","title":"Dead-letter events","text":"<p>When Event Grid can't deliver an event within a certain time period or after trying to deliver the event a specific number of times, it can send the undelivered event to a storage account. This process is known as\u00a0dead-lettering. Event Grid dead-letters an event when\u00a0one of the following\u00a0conditions is met.</p> <ul> <li>Event isn't delivered within the\u00a0time-to-live\u00a0period.</li> <li>The\u00a0number of tries\u00a0to deliver the event exceeds the limit.</li> </ul> <p>If either of the conditions is met, the event is dropped or dead-lettered. By default, Event Grid doesn't turn on dead-lettering. To enable it, you must specify a storage account to hold undelivered events when creating the event subscription. You pull events from this storage account to resolve deliveries.</p> <p>If Event Grid receives a 400 (Bad Request) or 413 (Request Entity Too Large) response code, it immediately schedules the event for dead-lettering. These response codes indicate delivery of the event failed.</p> <p>There's a five-minute delay between the last attempt to deliver an event and delivery to the dead-letter location. This delay is intended to reduce the number of Blob storage operations. If the dead-letter location is unavailable for four hours, the event is dropped.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#custom-delivery-properties","title":"Custom delivery properties","text":"<p>Event subscriptions allow you to set up HTTP headers that are included in delivered events. This capability allows you to set custom headers that are required by a destination. You can set up to 10 headers when creating an event subscription. Each header value shouldn't be greater than 4,096 bytes. You can set custom headers on the events that are delivered to the following destinations:</p> <ul> <li>Webhooks</li> <li>Azure Service Bus topics and queues</li> <li>Azure Event Hubs</li> <li>Relay Hybrid Connections</li> </ul> <p>Before setting the dead-letter location, you must have a storage account with a container. You provide the endpoint for this container when creating the event subscription.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#control-access-to-events","title":"Control access to events","text":"<p>Azure Event Grid allows you to control the level of access given to different users to do various management operations such as list event subscriptions, create new ones, and generate keys. Event Grid uses Azure role-based access control (Azure RBAC).</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#built-in-roles","title":"Built-in roles","text":"<p>Event Grid provides the following built-in roles:</p> Role Description Event Grid Subscription Reader Lets you read Event Grid event subscriptions. Event Grid Subscription Contributor Lets you manage Event Grid event subscription operations. Event Grid Contributor Lets you create and manage Event Grid resources. Event Grid Data Sender Lets you send events to Event Grid topics. <p>The Event Grid Subscription Reader and Event Grid Subscription Contributor roles are for managing event subscriptions. They're important when implementing event domains because they give users the permissions they need to subscribe to topics in your event domain. These roles are focused on event subscriptions and don't grant access for actions such as creating topics.</p> <p>The Event Grid Contributor role allows you to create and manage Event Grid resources.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#permissions-for-event-subscriptions","title":"Permissions for event subscriptions","text":"<p>If you're using an event handler that isn't a WebHook (such as an event hub or queue storage), you need write access to that resource. This permissions check prevents an unauthorized user from sending events to your resource.</p> <p>You must have the\u00a0Microsoft.EventGrid/EventSubscriptions/Write\u00a0permission on the resource that is the event source. You need this permission because you're writing a new subscription at the scope of the resource. The required resource differs based on whether you're subscribing to a system topic or custom topic. Both types are described in this section.</p> Topic Type Description System topics Need permission to write a new event subscription at the scope of the resource publishing the event. The format of the resource is:\u00a0<code>/subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/{resource-provider}/{resource-type}/{resource-name}</code> Custom topics Need permission to write a new event subscription at the scope of the event grid topic. The format of the resource is:\u00a0<code>/subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/Microsoft.EventGrid/topics/{topic-name}</code> ### Receive events by using webhooks <p>Webhooks are one of the many ways to receive events from Azure Event Grid. When a new event is ready, Event Grid service POSTs an HTTP request to the configured endpoint with the event in the request body.</p> <p>Like many other services that support webhooks, Event Grid requires you to prove ownership of your Webhook endpoint before it starts delivering events to that endpoint. This requirement prevents a malicious user from flooding your endpoint with events.</p> <p>When you use any of the following three Azure services, the Azure infrastructure automatically handles this validation:</p> <ul> <li>Azure Logic Apps with Event Grid Connector</li> <li>Azure Automation via webhook</li> <li>Azure Functions with Event Grid Trigger</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#endpoint-validation-with-event-grid-events","title":"Endpoint validation with Event Grid events","text":"<p>If you're using any other type of endpoint, such as an HTTP trigger based Azure function, your endpoint code needs to participate in a validation handshake with Event Grid. Event Grid supports two ways of validating the subscription.</p> <ul> <li>Synchronous handshake: At the time of event subscription creation, Event Grid sends a subscription validation event to your endpoint. The schema of this event is similar to any other Event Grid event. The data portion of this event includes a\u00a0<code>validationCode</code>\u00a0property. Your application verifies that the validation request is for an expected event subscription, and returns the validation code in the response synchronously. This handshake mechanism is supported in all Event Grid versions.</li> <li>Asynchronous handshake: In certain cases, you can't return the ValidationCode in response synchronously. For example, if you use a third-party service (like\u00a0Zapier\u00a0or\u00a0IFTTT), you can't programmatically respond with the validation code.</li> </ul> <p>Starting with version 2018-05-01-preview, Event Grid supports a manual validation handshake. If you're creating an event subscription with an SDK or tool that uses API version 2018-05-01-preview or later, Event Grid sends a\u00a0<code>validationUrl</code>\u00a0property in the data portion of the subscription validation event. To complete the handshake, find that URL in the event data and do a GET request to it. You can use either a REST client or your web browser.</p> <p>The provided URL is valid for\u00a05 minutes. During that time, the provisioning state of the event subscription is\u00a0<code>AwaitingManualAction</code>. If you don't complete the manual validation within 5 minutes, the provisioning state is set to\u00a0<code>Failed</code>. You have to create the event subscription again before starting the manual validation.</p> <p>This authentication mechanism also requires the webhook endpoint to return an HTTP status code of 200 so that it knows that the POST for the validation event was accepted before it can be put in the manual validation mode. In other words, if the endpoint returns 200 but doesn't return back a validation response synchronously, the mode is transitioned to the manual validation mode. If there's a GET on the validation URL within 5 minutes, the validation handshake is considered to be successful.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#filter-events","title":"Filter Events","text":"<p>When creating an event subscription, you have three options for filtering:</p> <ul> <li>Event types</li> <li>Subject begins with or ends with</li> <li>Advanced fields and operators</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#event-type-filtering","title":"Event type filtering","text":"<p>By default, all event types for the event source are sent to the endpoint. You can decide to send only certain event types to your endpoint. For example, you can get notified of updates to your resources, but not notified for other operations like deletions. In that case, filter by the\u00a0<code>Microsoft.Resources.ResourceWriteSuccess</code>\u00a0event type. Provide an array with the event types, or specify\u00a0<code>All</code>\u00a0to get all event types for the event source.</p> <p>The JSON syntax for filtering by event type is:</p> <pre><code>\"filter\": {\n    \"includedEventTypes\": [\n        \"Microsoft.Resources.ResourceWriteFailure\",\n        \"Microsoft.Resources.ResourceWriteSuccess\"\n  ]\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#subject-filtering","title":"Subject filtering","text":"<p>For simple filtering by subject, specify a starting or ending value for the subject. For example, you can specify the subject ends with\u00a0<code>.txt</code>\u00a0to only get events related to uploading a text file to storage account. Or, you can filter the subject begins with\u00a0<code>/blobServices/default/containers/testcontainer</code>\u00a0to get all events for that container but not other containers in the storage account.</p> <p>The JSON syntax for filtering by subject is:</p> <pre><code>\"filter\": {\n  \"subjectBeginsWith\": \"/blobServices/default/containers/mycontainer/log\",\n  \"subjectEndsWith\": \".jpg\"\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#advanced-filtering","title":"Advanced filtering","text":"<p>To filter by values in the data fields and specify the comparison operator, use the advanced filtering option. In advanced filtering, you specify the:</p> <ul> <li>operator type - The type of comparison.</li> <li>key - The field in the event data that you're using for filtering. It can be a number, boolean, or string.</li> <li>value or values - The value or values to compare to the key.</li> </ul> <p>The JSON syntax for using advanced filters is:</p> <pre><code>\"filter\": {\n  \"advancedFilters\": [\n    {\n      \"operatorType\": \"NumberGreaterThanOrEquals\",\n      \"key\": \"Data.Key1\",\n      \"value\": 5\n    },\n    {\n      \"operatorType\": \"StringContains\",\n      \"key\": \"Subject\",\n      \"values\": [\"container1\", \"container2\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#explore-azure-event-hubs","title":"Explore Azure Event Hubs","text":"<p>Azure Event Hubs is a big data streaming platform and event ingestion service. It can receive and process millions of events per second. Data sent to an event hub can be transformed and stored by using any real-time analytics provider or batching/storage adapters.</p> <p>After completing this module, you'll be able to:</p> <ul> <li>Describe the benefits of using Event Hubs and how it captures streaming data.</li> <li>Explain how to process events.</li> <li>Perform common operations with the Event Hubs client library.</li> </ul> <p>Azure Event Hubs is a native data-streaming service in the cloud that can stream millions of events per second, with low latency, from any source to any destination. Event Hubs is compatible with Apache Kafka. It enables you to run existing Kafka workloads without any code changes.</p> <p>With Event Hubs, you can ingest, buffer, store, and process your stream in real time to get actionable insights. Event Hubs uses a partitioned consumer model. It enables multiple applications to process the stream concurrently and lets you control the speed of processing. Event Hubs also integrates with Azure Functions for serverless architectures.</p> <p>A broad ecosystem is available for the industry-standard AMQP 1.0 protocol. SDKs are available in languages like .NET, Java, Python, and JavaScript, so you can start processing your streams from Event Hubs. All supported client languages provide low-level integration.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#key-capabilities","title":"Key capabilities","text":"<p>Learn about the key capabilities of Azure Event Hubs in the following sections.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#apache-kafka-on-azure-event-hubs","title":"Apache Kafka on Azure Event Hubs","text":"<p>Event Hubs is a multi-protocol event streaming engine that natively supports Advanced Message Queuing Protocol (AMQP), Apache Kafka, and HTTPS protocols. Because it supports Apache Kafka, you can bring Kafka workloads to Event Hubs without making any code changes. You don't need to set up, configure, or manage your own Kafka clusters or use a Kafka-as-a-service offering that's not native to Azure.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#schema-registry-in-event-hubs","title":"Schema Registry in Event Hubs","text":"<p>Azure Schema Registry in Event Hubs provides a centralized repository for managing schemas of event streaming applications. Schema Registry comes free with every Event Hubs namespace. It integrates with your Kafka applications or Event Hubs SDK-based applications.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#real-time-processing-of-streaming-events-with-stream-analytics","title":"Real-time processing of streaming events with Stream Analytics","text":"<p>Event Hubs integrates with Azure Stream Analytics to enable real-time stream processing. With the built-in no-code editor, you can develop a Stream Analytics job by using drag-and-drop functionality, without writing any code.</p> <p>Alternatively, developers can use the SQL-based Stream Analytics query language to perform real-time stream processing and take advantage of a wide range of functions for analyzing streaming data.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#key-concepts","title":"Key concepts","text":"<p>Event Hubs contains the following key components:</p> <ul> <li>Producer applications: These applications can ingest data to an event hub by using Event Hubs SDKs or any Kafka producer client.</li> <li>Namespace: The management container for one or more event hubs or Kafka topics. The management tasks such as allocating streaming capacity, configuring network security, and enabling geo-disaster recovery are handled at the namespace level.</li> <li>Event Hubs/Kafka topic: In Event Hubs, you can organize events into an event hub or a Kafka topic. It's an append-only distributed log, which can comprise one or more partitions.</li> <li>Partitions: They're used to scale an event hub. They're like lanes in a freeway. If you need more streaming throughput, you can add more partitions.</li> <li>Consumer applications: These applications can consume data by seeking through the event log and maintaining consumer offset. Consumers can be Kafka consumer clients or Event Hubs SDK clients.</li> <li>Consumer group: This logical group of consumer instances reads data from an event hub or Kafka topic. It enables multiple consumers to read the same streaming data in an event hub independently at their own pace and with their own offsets.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#explore-event-hubs-capture","title":"Explore Event Hubs Capture","text":"<p>Azure Event Hubs enables you to automatically capture the streaming data in Event Hubs in an Azure Blob storage or Azure Data Lake Storage account of your choice, with the added flexibility of specifying a time or size interval. Setting up Capture is fast, there are no administrative costs to run it, and it scales automatically with Event Hubs throughput units in the standard tier or processing units in the premium tier.</p> <p></p> <p>Event Hubs Capture enables you to process real-time and batch-based pipelines on the same stream. This means you can build solutions that grow with your needs over time.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#how-event-hubs-capture-works","title":"How Event Hubs Capture works","text":"<p>Event Hubs is a time-retention durable buffer for telemetry ingress, similar to a distributed log. The key to scaling in Event Hubs is the partitioned consumer model. Each partition is an independent segment of data and is consumed independently. Over time this data ages off, based on the configurable retention period. As a result, a given event hub never gets \"too full.\"</p> <p>Event Hubs Capture enables you to specify your own Azure Blob storage account and container, or Azure Data Lake Store account, which are used to store the captured data. These accounts can be in the same region as your event hub or in another region, adding to the flexibility of the Event Hubs Capture feature.</p> <p>Captured data is written in Apache Avro format: a compact, fast, binary format that provides rich data structures with inline schema. This format is widely used in the Hadoop ecosystem, Stream Analytics, and Azure Data Factory. More information about working with Avro is available later in this article.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#capture-windowing","title":"Capture windowing","text":"<p>Event Hubs Capture enables you to set up a window to control capturing. This window is a minimum size and time configuration with a \"first wins policy,\" meaning that the first trigger encountered causes a capture operation. Each partition captures independently and writes a completed block blob at the time of capture, named for the time at which the capture interval was encountered. The storage naming convention is as follows:</p> <pre><code>{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}\n</code></pre> <p>Note the date values are padded with zeroes; an example filename might be:</p> <pre><code>https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#scaling-to-throughput-units","title":"Scaling to throughput units","text":"<p>Event Hubs traffic is controlled by throughput units. A single throughput unit allows 1 MB per second or 1,000 events per second of ingress and twice that amount of egress. Standard Event Hubs can be configured with 1-20 throughput units, and you can purchase more with a quota increase support request. Usage beyond your purchased throughput units is throttled. Event Hubs Capture copies data directly from the internal Event Hubs storage, bypassing throughput unit egress quotas and saving your egress for other processing readers, such as Stream Analytics or Spark.</p> <p>Once configured, Event Hubs Capture runs automatically when you send your first event, and continues running. To make it easier for your downstream processing to know that the process is working, Event Hubs writes empty files when there's no data. This process provides a predictable cadence and marker that can feed your batch processors.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#scale-your-processing-application","title":"Scale your processing application","text":"<p>To scale your event processing application, you can run multiple instances of the application and have it balance the load among themselves. In the older versions,\u00a0EventProcessorHost\u00a0allowed you to balance the load between multiple instances of your program and checkpoint events when receiving. In the newer versions (5.0 onwards),\u00a0EventProcessorClient\u00a0(.NET and Java), or\u00a0EventHubConsumerClient\u00a0(Python and JavaScript) allows you to do the same.</p> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#example-scenario","title":"Example scenario","text":"<p>As an example scenario, consider a home security company that monitors 100,000 homes. Every minute, it gets data from various sensors such as a motion detector, door/window open sensor, glass break detector, and so on, installed in each home. The company provides a web site for residents to monitor the activity of their home in near real time.</p> <p>Each sensor pushes data to an event hub. The event hub is configured with 16 partitions. On the consuming end, you need a mechanism that can read these events, consolidate them, and dump the aggregate to a storage blob, which is then projected to a user-friendly web page.</p> <p>When designing the consumer in a distributed environment, the scenario must handle the following requirements:</p> <ul> <li>Scale:\u00a0Create multiple consumers, with each consumer taking ownership of reading from a few Event Hubs partitions.</li> <li>Load balance:\u00a0Increase or reduce the consumers dynamically. For example, when a new sensor type (for example, a carbon monoxide detector) is added to each home, the number of events increases. In that case, the operator (a human) increases the number of consumer instances. Then, the pool of consumers can rebalance the number of partitions they own, to share the load with the newly added consumers.</li> <li>Seamless resume on failures:\u00a0If a consumer (consumer A) fails (for example, the virtual machine hosting the consumer suddenly crashes), then other consumers can pick up the partitions owned by\u00a0consumer A\u00a0and continue. Also, the continuation point, called a\u00a0checkpoint\u00a0or\u00a0offset, should be at the exact point at which\u00a0consumer A\u00a0failed, or slightly before that.</li> <li>Consume events:\u00a0While the previous three points deal with the management of the consumer, there must be code to consume the events and do something useful with it. For example, aggregate it and upload it to blob storage.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#event-processor-or-consumer-client","title":"Event processor or consumer client","text":"<p>You don't need to build your own solution to meet these requirements. The Azure Event Hubs SDKs provide this functionality. In .NET or Java SDKs, you use an event processor client (<code>EventProcessorClient</code>), and in Python and JavaScript SDKs, you use\u00a0<code>EventHubConsumerClient</code>.</p> <p>For most production scenarios, we recommend that you use the event processor client for reading and processing events. Event processor clients can work cooperatively within the context of a consumer group for a given event hub. Clients will automatically manage distribution and balancing of work as instances become available or unavailable for the group.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#partition-ownership-tracking","title":"Partition ownership tracking","text":"<p>An event processor instance typically owns and processes events from one or more partitions. Ownership of partitions is evenly distributed among all the active event processor instances associated with an event hub and consumer group combination.</p> <p>Each event processor is given a unique identifier and claims ownership of partitions by adding or updating an entry in a checkpoint store. All event processor instances communicate with this store periodically to update its own processing state and to learn about other active instances. This data is then used to balance the load among the active processors.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#receive-messages","title":"Receive messages","text":"<p>When you create an event processor, you specify the functions that process events and errors. Each call to the function that processes events delivers a single event from a specific partition. It's your responsibility to handle this event. If you want to make sure the consumer processes every message at least once, you need to write your own code with retry logic. But be cautious about poisoned messages.</p> <p>We recommend that you do things relatively fast. That is, do as little processing as possible. If you need to write to storage and do some routing, it's better to use two consumer groups and have two event processors.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#checkpointing","title":"Checkpointing","text":"<p>Checkpointing\u00a0is a process by which an event processor marks or commits the position of the last successfully processed event within a partition. Marking a checkpoint is typically done within the function that processes the events and occurs on a per-partition basis within a consumer group.</p> <p>If an event processor disconnects from a partition, another instance can resume processing the partition at the checkpoint that was previously committed by the last processor of that partition in that consumer group. When the processor connects, it passes the offset to the event hub to specify the location at which to start reading. In this way, you can use checkpointing to both mark events as \"complete\" by downstream applications and to provide resiliency when an event processor goes down. It's possible to return to older data by specifying a lower offset from this checkpointing process.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#thread-safety-and-processor-instances","title":"Thread safety and processor instances","text":"<p>By default, the function that processes the events is called sequentially for a given partition. Subsequent events and calls to this function from the same partition queue up behind the scenes as the event pump continues to run in the background on other threads. Events from different partitions can be processed concurrently and any shared state that is accessed across partitions have to be synchronized.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#control-access-to-events_1","title":"Control access to events","text":"<p>Azure Event Hubs supports both Microsoft Entra ID and shared access signatures (SAS) to handle both authentication and authorization. Azure provides the following Azure built-in roles for authorizing access to Event Hubs data using Microsoft Entra ID and OAuth:</p> <ul> <li>Azure Event Hubs Data Owner: Use this role to give\u00a0complete access\u00a0to Event Hubs resources.</li> <li>Azure Event Hubs Data Sender: Use this role to give\u00a0send access\u00a0to Event Hubs resources.</li> <li>Azure Event Hubs Data Receiver: Use this role to give\u00a0receiving access\u00a0to Event Hubs resources.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#authorize-access-with-managed-identities","title":"Authorize access with managed identities","text":"<p>To authorize a request to Event Hubs service from a managed identity in your application, you need to configure Azure role-based access control settings for that managed identity. Azure Event Hubs defines Azure roles that encompass permissions for sending and reading from Event Hubs. When the Azure role is assigned to a managed identity, the managed identity is granted access to Event Hubs data at the appropriate scope.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#authorize-access-with-microsoft-identity-platform","title":"Authorize access with Microsoft identity platform","text":"<p>A key advantage of using Microsoft Entra ID with Event Hubs is that your credentials no longer need to be stored in your code. Instead, you can request an OAuth 2.0 access token from Microsoft identity platform. Microsoft Entra authenticates the security principal (a user, a group, or service principal) running the application. If authentication succeeds, Microsoft Entra ID returns the access token to the application, and the application can then use the access token to authorize requests to Azure Event Hubs.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#authorize-access-to-event-hubs-publishers-with-shared-access-signatures","title":"Authorize access to Event Hubs publishers with shared access signatures","text":"<p>An event publisher defines a virtual endpoint for an Event Hubs. The publisher can only be used to send messages to an event hub and not receive messages. Typically, an event hub employs one publisher per client. All messages that are sent to any of the publishers of an event hub are enqueued within that event hub. Publishers enable fine-grained access control.</p> <p>Each Event Hubs client is assigned a unique token that is uploaded to the client. A client that holds a token can only send to one publisher, and no other publisher. If multiple clients share the same token, then each of them shares the publisher.</p> <p>All tokens are assigned with shared access signature keys. Typically, all tokens are signed with the same key. Clients aren't aware of the key, which prevents clients from manufacturing tokens. Clients operate on the same tokens until they expire.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#authorize-access-to-event-hubs-consumers-with-shared-access-signatures","title":"Authorize access to Event Hubs consumers with shared access signatures","text":"<p>To authenticate back-end applications that consume from the data generated by Event Hubs producers, Event Hubs token authentication requires its clients to either have the\u00a0manage\u00a0rights or the\u00a0listen privileges assigned to its Event Hubs namespace or event hub instance or topic. Data is consumed from Event Hubs using consumer groups. While SAS policy gives you granular scope, this scope is defined only at the entity level and not at the consumer level. It means that the privileges defined at the namespace level or the event hub instance or topic level are to the consumer groups of that entity.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#perform-common-operations-with-the-event-hubs-client-library","title":"Perform common operations with the Event Hubs client library","text":"<p>This unit contains examples of common operations you can perform with the Event Hubs client library (<code>Azure.Messaging.EventHubs</code>) to interact with an Event Hubs.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#inspect-event-hubs","title":"Inspect Event Hubs","text":"<p>Many Event Hubs operations take place within the scope of a specific partition. Because Event Hubs owns the partitions, their names are assigned at the time of creation. To understand what partitions are available, you query the Event Hubs using one of the Event Hubs clients. For illustration, the\u00a0<code>EventHubProducerClient</code>\u00a0is demonstrated in these examples, but the concept and form are common across clients.</p> <pre><code>var connectionString = \"&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;\";\nvar eventHubName = \"&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    string[] partitionIds = await producer.GetPartitionIdsAsync();\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#publish-events-to-event-hubs","title":"Publish events to Event Hubs","text":"<p>In order to publish events, you need to create an\u00a0<code>EventHubProducerClient</code>. Producers publish events in batches and might request a specific partition, or allow the Event Hubs service to decide which partition events should be published to. We recommended using automatic routing when the publishing of events needs to be highly available or when event data should be distributed evenly among the partitions. Our example takes advantage of automatic routing.</p> <pre><code>var connectionString = \"&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;\";\nvar eventHubName = \"&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    using EventDataBatch eventBatch = await producer.CreateBatchAsync();\n    eventBatch.TryAdd(new EventData(new BinaryData(\"First\")));\n    eventBatch.TryAdd(new EventData(new BinaryData(\"Second\")));\n\n    await producer.SendAsync(eventBatch);\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#read-events-from-an-event-hubs","title":"Read events from an Event Hubs","text":"<p>In order to read events from an Event Hubs, you need to create an\u00a0<code>EventHubConsumerClient</code>\u00a0for a given consumer group. When an Event Hubs is created, it provides a default consumer group that can be used to get started with exploring Event Hubs. In our example, we focus on reading all events published to the Event Hubs using an iterator.</p> <p></p> <pre><code>var connectionString = \"&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;\";\nvar eventHubName = \"&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsAsync(cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the Event Hub. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#read-events-from-an-event-hubs-partition","title":"Read events from an Event Hubs partition","text":"<p>To read from a specific partition, the consumer needs to specify where in the event stream to begin receiving events. In our example, we focus on reading all published events for the first partition of the Event Hubs.</p> <pre><code>var connectionString = \"&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;\";\nvar eventHubName = \"&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    EventPosition startingPosition = EventPosition.Earliest;\n    string partitionId = (await consumer.GetPartitionIdsAsync()).First();\n\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsFromPartitionAsync(partitionId, startingPosition, cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the partition. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2009%20Revision/#process-events-using-an-event-processor-client","title":"Process events using an Event Processor client","text":"<p>For most production scenarios, the recommendation is to use\u00a0<code>EventProcessorClient</code>\u00a0for reading and processing events. Since the\u00a0<code>EventProcessorClient</code>\u00a0has a dependency on Azure Storage blobs for persistence of its state, you need to provide a\u00a0<code>BlobContainerClient</code>\u00a0for the processor, which has been configured for the storage account and container that should be used.</p> <pre><code>var cancellationSource = new CancellationTokenSource();\ncancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\nvar storageConnectionString = \"&lt;&lt; CONNECTION STRING FOR THE STORAGE ACCOUNT &gt;&gt;\";\nvar blobContainerName = \"&lt;&lt; NAME OF THE BLOB CONTAINER &gt;&gt;\";\n\nvar eventHubsConnectionString = \"&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;\";\nvar eventHubName = \"&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;\";\nvar consumerGroup = \"&lt;&lt; NAME OF THE EVENT HUB CONSUMER GROUP &gt;&gt;\";\n\nTask processEventHandler(ProcessEventArgs eventArgs) =&gt; Task.CompletedTask;\nTask processErrorHandler(ProcessErrorEventArgs eventArgs) =&gt; Task.CompletedTask;\n\nvar storageClient = new BlobContainerClient(storageConnectionString, blobContainerName);\nvar processor = new EventProcessorClient(storageClient, consumerGroup, eventHubsConnectionString, eventHubName);\n\nprocessor.ProcessEventAsync += processEventHandler;\nprocessor.ProcessErrorAsync += processErrorHandler;\n\nawait processor.StartProcessingAsync();\n\ntry\n{\n    // The processor performs its work in the background; block until cancellation\n    // to allow processing to take place.\n\n    await Task.Delay(Timeout.Infinite, cancellationSource.Token);\n}\ncatch (TaskCanceledException)\n{\n    // This is expected when the delay is canceled.\n}\n\ntry\n{\n    await processor.StopProcessingAsync();\n}\nfinally\n{\n    // To prevent leaks, the handlers should be removed when processing is complete.\n\n    processor.ProcessEventAsync -= processEventHandler;\n    processor.ProcessErrorAsync -= processErrorHandler;\n}\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/","title":"Develop message-based solutions","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#azure-message-queues","title":"Azure message queues","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Choose the appropriate queue mechanism for your solution.</li> <li>Explain how the messaging entities that form the core capabilities of Service Bus operate.</li> <li>Send and receive message from a Service Bus queue by using .NET.</li> <li>Identify the key components of Azure Queue Storage</li> <li>Create queues and manage messages in Azure Queue Storage by using .NET.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#choosing-a-message-queue-solution","title":"Choosing a message queue solution","text":"<p>Storage queues and Service Bus queues have a slightly different feature set. You can choose either one or both, depending on the needs of your particular solution.</p> <p>When determining which queuing technology fits the purpose of a given solution, solution architects and developers should consider these recommendations.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#consider-using-service-bus-queues","title":"Consider using Service Bus queues","text":"<p>As a solution architect/developer,\u00a0you should consider using Service Bus queues\u00a0when:</p> <ul> <li>Your solution needs to receive messages without having to poll the queue. With Service Bus, you can achieve it by using a long-polling receive operation using the TCP-based protocols that Service Bus supports.</li> <li>Your solution requires the queue to provide a guaranteed first-in-first-out (FIFO) ordered delivery.</li> <li>Your solution needs to support automatic duplicate detection.</li> <li>You want your application to process messages as parallel long-running streams (messages are associated with a stream using the\u00a0session ID\u00a0property on the message). In this model, each node in the consuming application competes for streams, as opposed to messages. When a stream is given to a consuming node, the node can examine the state of the application stream state using transactions.</li> <li>Your solution requires transactional behavior and atomicity when sending or receiving multiple messages from a queue.</li> <li>Your application handles messages that can exceed 64 KB but won't likely approach the 256 KB or 1-MB limit, depending on the chosen service tier (although Service Bus queues can handle messages up to 100 MB).</li> <li>You deal with a requirement to provide a role-based access model to the queues, and different rights/permissions for senders and receivers.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#consider-using-storage-queues","title":"Consider using Storage queues","text":"<p>As a solution architect/developer,\u00a0you should consider using Storage queues\u00a0when:</p> <ul> <li>Your application must store over 80 gigabytes of messages in a queue.</li> <li>Your application wants to track progress for processing a message in the queue. It's useful if the worker processing a message crashes. Another worker can then use that information to continue from where the prior worker left off.</li> <li>You require server side logs of all of the transactions executed against your queues.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#azure-service-bus","title":"Azure Service Bus","text":"<p>Azure Service Bus is a fully managed enterprise message broker with message queues and publish-subscribe topics. Service Bus is used to decouple applications and services. Data is transferred between different applications and services using\u00a0messages. A message is a container decorated with metadata, and contains data. The data can be any kind of information, including structured data encoded with the common formats such as the following ones: JSON, XML, Apache Avro, and Plain Text.</p> <p>Some common messaging scenarios are:</p> <ul> <li>Messaging. Transfer business data, such as sales or purchase orders, journals, or inventory movements.</li> <li>Decouple applications. Improve reliability and scalability of applications and services. Client and service don't have to be online at the same time.</li> <li>Topics and subscriptions. Enable 1:n\u00a0relationships between publishers and subscribers.</li> <li>Message sessions. Implement workflows that require message ordering or message deferral.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#service-bus-tiers","title":"Service Bus tiers","text":"<p>Service Bus offers basic, standard, and premium tiers. The\u00a0premium\u00a0tier of Service Bus Messaging addresses common customer requests around scale, performance, and availability for mission-critical applications. The premium tier is recommended for production scenarios. Although the feature sets are nearly identical, these two tiers of Service Bus Messaging are designed to serve different use cases. For more information on the available tiers, visit\u00a0Service Bus pricing.</p> <p>Some high-level differences between the premium and standard tiers are highlighted in the following table.</p> Premium Standard High throughput Variable throughput Predictable performance Variable latency Fixed pricing Pay as you go variable pricing Ability to scale workload up and down N/A Message size up to 100 MB Message size up to 256 KB"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#advanced-features","title":"Advanced features","text":"<p>Service Bus includes advanced features that enable you to solve more complex messaging problems. The following table describes several of these features.</p> Feature Description Message sessions To create a first-in, first-out (FIFO) guarantee in Service Bus, use sessions. Message sessions enable exclusive, ordered handling of unbounded sequences of related messages. Autoforwarding The autoforwarding feature chains a queue or subscription to another queue or topic that is in the same namespace. Dead-letter queue Service Bus supports a dead-letter queue (DLQ). A DLQ holds messages that can't be delivered to any receiver. Service Bus lets you remove messages from the DLQ and inspect them. Scheduled delivery You can submit messages to a queue or topic for delayed processing. You can schedule a job to become available for processing by a system at a certain time. Message deferral A queue or subscription client can defer retrieval of a message until a later time. The message remains in the queue or subscription, but is set aside. Transactions A transaction groups two or more operations together into an\u00a0execution scope. Service Bus supports grouping operations against a single messaging entity within the scope of a single transaction. A message entity can be a queue, topic, or subscription. Filtering and actions Subscribers can define which messages they want to receive from a topic. These messages are specified in the form of one or more named subscription rules. Autodelete on idle Autodelete on idle enables you to specify an idle interval after which a queue is automatically deleted. The minimum duration is 5 minutes. Duplicate detection An error could cause the client to have a doubt about the outcome of a send operation. Duplicate detection enables the sender to resend the same message, or for the queue or topic to discard any duplicate copies. Security protocols Service Bus supports security protocols such as Shared Access Signatures (SAS), Role Based Access Control (RBAC) and Managed identities for Azure resources. Geo-disaster recovery When Azure regions or datacenters experience downtime, Geo-disaster recovery enables data processing to continue operating in a different region or datacenter. Security Service Bus supports standard AMQP 1.0 and HTTP/REST protocols."},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#compliance-with-standards-and-protocols","title":"Compliance with standards and protocols","text":"<p>The primary wire protocol for Service Bus is\u00a0Advanced Messaging Queueing Protocol (AMQP) 1.0, an open ISO/IEC standard. It allows customers to write applications that work against Service Bus and on-premises brokers such as ActiveMQ or RabbitMQ. The\u00a0AMQP protocol guide\u00a0provides detailed information in case you want to build such an abstraction.</p> <p>Service Bus Premium is fully compliant with the Java/Jakarta EE\u00a0Java Message Service (JMS) 2.0\u00a0API.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#client-libraries","title":"Client libraries","text":"<p>Fully supported Service Bus client libraries are available via the Azure SDK.</p> <ul> <li>Azure Service Bus for .NET</li> <li>Azure Service Bus libraries for Java</li> <li>Azure Service Bus provider for Java JMS 2.0</li> <li>Azure Service Bus Modules for JavaScript and TypeScript</li> <li>Azure Service Bus libraries for Python</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#service-bus-queues-topics-and-subscriptions","title":"Service Bus queues, topics, and subscriptions","text":"<p>The messaging entities that form the core of the messaging capabilities in Service Bus are\u00a0queues,\u00a0topics and subscriptions, and rules/actions.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#queues","title":"Queues","text":"<p>Queues offer\u00a0First In, First Out\u00a0(FIFO) message delivery to one or more competing consumers. That is, receivers typically receive and process messages in the order in which they were added to the queue. And, only one message consumer receives and processes each message. Because messages are stored durably in the queue, producers (senders) and consumers (receivers) don't have to process messages concurrently.</p> <p>A related benefit is\u00a0load-leveling, which enables producers and consumers to send and receive messages at different rates. In many applications, the system load varies over time. However, the processing time required for each unit of work is typically constant. Intermediating message producers and consumers with a queue means that the consuming application only has to be able to handle average load instead of peak load.</p> <p>Using queues to intermediate between message producers and consumers provides an inherent loose coupling between the components. Because producers and consumers aren't aware of each other, a consumer can be upgraded without having any effect on the producer.</p> <p>You can create queues using the Azure portal, PowerShell, CLI, or Resource Manager templates. Then, send and receive messages using clients written in C#, Java, Python, and JavaScript.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#receive-modes","title":"Receive modes","text":"<p>You can specify two different modes in which Service Bus receives messages:\u00a0Receive and delete\u00a0or\u00a0Peek lock.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#receive-and-delete","title":"Receive and delete","text":"<p>In this mode, when Service Bus receives the request from the consumer, it marks the message as consumed and returns it to the consumer application. This mode is the simplest model. It works best for scenarios in which the application can tolerate not processing a message if a failure occurs. For example, consider a scenario in which the consumer issues the receive request and then crashes before processing it. As Service Bus marks the message as consumed, the application begins consuming messages upon restart. It misses the message that it consumed before the crash.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#peek-lock","title":"Peek lock","text":"<p>In this mode, the receive operation becomes two-stage, which makes it possible to support applications that can't tolerate missing messages.</p> <ol> <li>Finds the next message to be consumed,\u00a0locks\u00a0it to prevent other consumers from receiving it, and then, return the message to the application.</li> <li>After the application finishes processing the message, it requests the Service Bus service to complete the second stage of the receive process. Then, the service\u00a0marks the message as consumed.</li> </ol> <p>If the application is unable to process the message for some reason, it can request the Service Bus service to\u00a0abandon\u00a0the message. Service Bus\u00a0unlocks\u00a0the message and makes it available to be received again, either by the same consumer or by another competing consumer. Secondly, there's a\u00a0timeout\u00a0associated with the lock. If the application fails to process the message before the lock timeout expires, Service Bus unlocks the message and makes it available to be received again.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#topics-and-subscriptions","title":"Topics and subscriptions","text":"<p>A queue allows processing of a message by a single consumer. In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a\u00a0publish and subscribe\u00a0pattern. It's useful for scaling to large numbers of recipients. Each published message is made available to each subscription registered with the topic. Publisher sends a message to a topic and one or more subscribers receive a copy of the message.</p> <p>The subscriptions can use more filters to restrict the messages that they want to receive. Publishers send messages to a topic in the same way that they send messages to a queue. But, consumers don't receive messages directly from the topic. Instead, consumers receive messages from subscriptions of the topic. A topic subscription resembles a virtual queue that receives copies of the messages that are sent to the topic. Consumers receive messages from a subscription identically to the way they receive messages from a queue.</p> <p>The message-sending functionality of a queue maps directly to a topic and its message-receiving functionality maps to a subscription. Among other things, this feature means that subscriptions support the same patterns described earlier in this section regarding queues: competing consumer, temporal decoupling, load leveling, and load balancing.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#rules-and-actions","title":"Rules and actions","text":"<p>In many scenarios, messages that have specific characteristics must be processed in different ways. To enable this processing, you can configure subscriptions to find messages that have desired properties and then perform certain modifications to those properties. While Service Bus subscriptions see all messages sent to the topic, you can only copy a subset of those messages to the virtual subscription queue. This filtering is accomplished using subscription filters. Such modifications are called\u00a0filter actions. When a subscription is created, you can supply a filter expression that operates on the properties of the message. The properties can be both the system properties (for example,\u00a0Label) and custom application properties (for example,\u00a0StoreName.) The SQL filter expression is optional in this case. Without a SQL filter expression, any filter action defined on a subscription is performed on all the messages for that subscription.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#service-bus-message-payloads-and-serialization","title":"Service Bus message payloads and serialization","text":"<p>Messages carry a payload and metadata. The metadata is in the form of key-value pair properties, and describes the payload, and gives handling instructions to Service Bus and applications. Occasionally, that metadata alone is sufficient to carry the information that the sender wants to communicate to receivers, and the payload remains empty.</p> <p>The object model of the official Service Bus clients for .NET and Java maps to and from the wire protocols Service Bus supports.</p> <p>A Service Bus message consists of a binary payload section that Service Bus never handles in any form on the service-side, and two sets of properties. The\u00a0broker properties\u00a0are system defined. These predefined properties either control message-level functionality inside the broker, or they map to common and standardized metadata items. The\u00a0user properties\u00a0are a collection of key-value pairs defined and set by the application.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#message-routing-and-correlation","title":"Message routing and correlation","text":"<p>A subset of the broker properties, specifically\u00a0<code>To</code>,\u00a0<code>ReplyTo</code>,\u00a0<code>ReplyToSessionId</code>,\u00a0<code>MessageId</code>,\u00a0<code>CorrelationId</code>, and\u00a0<code>SessionId</code>, help applications route messages to particular destinations. The following patterns describe the routing:</p> <pre><code># MY NOTES\nSimple RR\nPublisher -&gt; Publishes to a queue\nPublisher -&gt; Sets ReplyTo to be a queue.\n\nMulticast RR\nPublisher -&gt; Publishes to a topic\nPublisher -&gt; Sets ReplyTo to be a queue, queues, topic\n\nMultiplexing\nPublisher -&gt; Publishing to a specific queue/subscription (in a topic).\nPublisher -&gt; SessionId determines which receiver receives these messages.\n\nMultiplexing request/reply\nPublishers -&gt; Publishers publish without sessionId, dont care which receiver picks it up.\nPublishers -&gt; ReplyToSessionId, consumer set the SessionID based off of that.\nPublishers -&gt; Only uses sessions on reply.\n</code></pre> <ul> <li>Simple request/reply:\u00a0A publisher sends a message into a queue and expects a reply from the message consumer. The publisher owns a queue to receive the replies. The address of that queue is contained in the\u00a0<code>ReplyTo</code>\u00a0property of the outbound message. When the consumer responds, it copies the\u00a0<code>MessageId</code>\u00a0of the handled message into the\u00a0<code>CorrelationId</code>\u00a0property of the reply message and delivers the message to the destination indicated by the\u00a0<code>ReplyTo</code>\u00a0property. One message can yield multiple replies, depending on the application context.</li> <li>Multicast request/reply:\u00a0As a variation of the prior pattern, a publisher sends the message into a topic and multiple subscribers become eligible to consume the message. Each of the subscribers might respond in the fashion described previously. If\u00a0<code>ReplyTo</code>\u00a0points to a topic, such a set of discovery responses can be distributed to an audience.</li> <li>Multiplexing:\u00a0This session feature enables multiplexing of streams of related messages through a single queue or subscription such that each session (or group) of related messages, identified by matching\u00a0<code>SessionId</code>\u00a0values, are routed to a specific receiver while the receiver holds the session under lock. Learn more about the details of sessions\u00a0here.</li> <li>Multiplexed request/reply:\u00a0This session feature enables multiplexed replies, allowing several publishers to share a reply queue. By setting\u00a0<code>ReplyToSessionId</code>, the publisher can instruct one or more consumers to copy that value into the\u00a0<code>SessionId</code>\u00a0property of the reply message. The publishing queue or topic doesn't need to be session-aware. When the message is sent the publisher can wait for a session with the given\u00a0<code>SessionId</code>\u00a0to materialize on the queue by conditionally accepting a session receiver.</li> </ul> <p>Routing inside of a Service Bus namespace uses autoforward chaining and topic subscription rules. Routing across namespaces can be performed using Azure LogicApps. The\u00a0<code>To</code>\u00a0property is reserved for future use. Applications that implement routing should do so based on user properties and not lean on the\u00a0<code>To</code>\u00a0property; however, doing so now won't cause compatibility issues.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#payload-serialization","title":"Payload serialization","text":"<p>When in transit or stored inside of Service Bus, the payload is always an opaque, binary block. The\u00a0<code>ContentType</code>\u00a0property enables applications to describe the payload, with the suggested format for the property values being a MIME content-type description according to IETF RFC2045; for example,\u00a0<code>application/json;charset=utf-8</code>.</p> <p>Unlike the Java or .NET Standard variants, the .NET Framework version of the Service Bus API supports creating\u00a0<code>BrokeredMessage</code>\u00a0instances by passing arbitrary .NET objects into the constructor.</p> <p>The legacy SBMP protocol serializes objects with the default binary serializer, or with a serializer that is externally supplied. The AMQP protocol serializes objects into an AMQP object. The receiver can retrieve those objects with the\u00a0<code>GetBody&lt;T&gt;()</code>\u00a0method, supplying the expected type. With AMQP, the objects are serialized into an AMQP graph of\u00a0<code>ArrayList</code>\u00a0and\u00a0<code>IDictionary&lt;string,object&gt;</code>\u00a0objects, and any AMQP client can decode them.</p> <p>While this hidden serialization magic is convenient, if applications should take explicit control of object serialization and turn their object graphs into streams before including them into a message, they should do the reverse operation on the receiver side. While AMQP has a powerful binary encoding model, it's tied to the AMQP messaging ecosystem and HTTP clients have trouble decoding such payloads.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2010%20Revision/#azure-queue-storage","title":"Azure Queue Storage","text":"<p>Azure Queue Storage is a service for storing large numbers of messages. You access messages from anywhere in the world via authenticated calls using HTTP or HTTPS. A queue message can be up to 64 KB in size. A queue may contain millions of messages, up to the total capacity limit of a storage account. Queues are commonly used to create a backlog of work to process asynchronously.</p> <p>The Queue service contains the following components:</p> <p></p> <ul> <li>URL format:\u00a0Queues are addressable using the URL format\u00a0<code>https://&lt;storage account&gt;.queue.core.windows.net/&lt;queue&gt;</code>. For example, the following URL addresses a queue in the diagram above\u00a0<code>https://myaccount.queue.core.windows.net/images-to-download</code></li> <li>Storage account:\u00a0All access to Azure Storage is done through a storage account.</li> <li>Queue:\u00a0A queue contains a set of messages. All messages must be in a queue. The queue name must be all lowercase.</li> <li>Message:\u00a0A message, in any format, of up to 64 KB. Before version 2017-07-29, the maximum time-to-live allowed is seven days. For version 2017-07-29 or later, the maximum time-to-live can be any positive number, or -1 indicating that the message doesn't expire. If this parameter is omitted, the default time-to-live is seven days.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/","title":"Troubleshoot solutions by using Application Insights","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#monitoring-app-performance","title":"Monitoring app performance","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Describe how Application Insights works and how it collects events and metrics.</li> <li>Instrument an app for monitoring, and perform availability tests.</li> <li>Use Application Map to help you monitor performance and troubleshoot issues.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#application-insights","title":"Application Insights","text":"<p>Application Insights is an extension of Azure Monitor and provides Application Performance Monitoring (APM) features. APM tools are useful to monitor applications from development, through test, and into production in the following ways:</p> <ul> <li>Proactively understand how an application is performing.</li> <li>Reactively review application execution data to determine the cause of an incident.</li> </ul> <p>In addition to collecting metrics and application telemetry data, which describe application activities and health, Application Insights can also be used to collect and store application trace logging data.</p> <p>The log trace is associated with other telemetry to give a detailed view of the activity. Adding trace logging to existing apps only requires providing a destination for the logs; the logging framework rarely needs to be changed.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#application-insights-feature-overview","title":"Application Insights feature overview","text":"<p>Features include, but not limited to:</p> Feature Description Live Metrics Observe activity from your deployed application in real time with no effect on the host environment. Availability Also known as\u00a0Synthetic Transaction Monitoring, probe your applications external endpoints to test the overall availability and responsiveness over time. GitHub or Azure DevOps integration Create GitHub or Azure DevOps work items in context of Application Insights data. Usage Understand which features are popular with users and how users interact and use your application Smart Detection Automatic failure and anomaly detection through proactive telemetry analysis. Application Map A high level top-down view of the application architecture and at-a-glance visual references to component health and responsiveness. Distributed Tracing Search and visualize an end-to-end flow of a given execution or transaction."},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#what-application-insights-monitors","title":"What Application Insights monitors","text":"<p>Application Insights collects Metrics and application Telemetry data, which describe application activities and health, as well as trace logging data.</p> <ul> <li>Request rates, response times, and failure rates\u00a0- Find out which pages are most popular, at what times of day, and where your users are. See which pages perform best. If your response times and failure rates go high when there are more requests, then perhaps you have a resourcing problem.</li> <li>Dependency rates, response times, and failure rates\u00a0- Find out whether external services are slowing you down.</li> <li>Exceptions\u00a0- Analyze the aggregated statistics, or pick specific instances and drill into the stack trace and related requests. Both server and browser exceptions are reported.</li> <li>Page views and load performance\u00a0- reported by your users' browsers.</li> <li>AJAX calls\u00a0from web pages - rates, response times, and failure rates.</li> <li>User and session counts.</li> <li>Performance counters\u00a0from your Windows or Linux server machines, such as CPU, memory, and network usage.</li> <li>Host diagnostics\u00a0from Docker or Azure.</li> <li>Diagnostic trace logs\u00a0from your app - so that you can correlate trace events with requests.</li> <li>Custom events and metrics\u00a0that you write yourself in the client or server code, to track business events such as items sold or games won.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#getting-started-with-application-insights","title":"Getting started with Application Insights","text":"<p>Application Insights is one of the many services hosted within Microsoft Azure, and telemetry is sent there for analysis and presentation. It's free to sign up, and if you choose the basic pricing plan of Application Insights, there's no charge until your application has grown to have substantial usage.</p> <p>There are several ways to get started monitoring and analyzing app performance:</p> <ul> <li>At run time:\u00a0instrument your web app on the server. Ideal for applications already deployed. Avoids any update to the code.</li> <li>At development time:\u00a0add Application Insights to your code. Allows you to customize telemetry collection and send more telemetry.</li> <li>Instrument your web pages\u00a0for page view, AJAX, and other client-side telemetry.</li> <li>Analyze mobile app usage\u00a0by integrating with Visual Studio App Center.</li> <li>Availability tests\u00a0- ping your website regularly from our servers.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#discover-log-based-metrics","title":"Discover log-based metrics","text":"<p>Application Insights log-based metrics let you analyze the health of your monitored apps, create powerful dashboards, and configure alerts. There are two kinds of metrics:</p> <ul> <li>Log-based metrics\u00a0behind the scene are translated into\u00a0Kusto queries\u00a0from stored events.</li> <li>Standard metrics\u00a0are stored as preaggregated time series.</li> </ul> <p>Since\u00a0standard metrics\u00a0are preaggregated during collection, they have better performance at query time. Standard metrics are a better choice for dashboarding and in real-time alerting. The\u00a0log-based metrics\u00a0have more dimensions, which makes them the superior option for data analysis and ad-hoc diagnostics. Use the\u00a0namespace selector\u00a0to switch between log-based and standard metrics in\u00a0metrics explorer.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#log-based-metrics","title":"Log-based metrics","text":"<p>Developers can use the SDK to send events manually (by writing code that explicitly invokes the SDK) or they can rely on the automatic collection of events from autoinstrumentation. In either case, the Application Insights backend stores all collected events as logs, and the Application Insights blades in the Azure portal act as an analytical and diagnostic tool for visualizing event-based data from logs.</p> <p>Using logs to retain a complete set of events can bring great analytical and diagnostic value. For example, you can get an exact count of requests to a particular URL with the number of distinct users who made these calls. Or you can get detailed diagnostic traces, including exceptions and dependency calls for any user session. Having this type of information can significantly improve visibility into the application health and usage, allowing to cut down the time necessary to diagnose issues with an app.</p> <p>At the same time, collecting a complete set of events may be impractical (or even impossible) for applications that generate a large volume of telemetry. For situations when the volume of events is too high, Application Insights implements several telemetry volume reduction techniques, such as sampling and filtering that reduces the number of collected and stored events. Unfortunately, lowering the number of stored events also lowers the accuracy of the metrics that, behind the scenes, must perform query-time aggregations of the events stored in logs.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#preaggregated-metrics","title":"Preaggregated metrics","text":"<p>The preaggregated metrics aren't stored as individual events with lots of properties. Instead, they're stored as preaggregated time series, and only with key dimensions. This makes the new metrics superior at query time: retrieving data happens faster and requires less compute power. This enables new scenarios such as near real-time alerting on dimensions of metrics, more responsive dashboards, and more.</p> <p></p> <p>The newer SDKs (Application Insights 2.7\u00a0SDK or later for .NET) preaggregate metrics during collection. This applies to\u00a0standard metrics sent by default\u00a0so the accuracy isn't affected by sampling or filtering. It also applies to custom metrics sent using\u00a0GetMetric\u00a0resulting in less data ingestion and lower cost.</p> <p>For the SDKs that don't implement preaggregation the Application Insights backend still populates the new metrics by aggregating the events received by the Application Insights event collection endpoint. While you don't benefit from the reduced volume of data transmitted over the wire, you can still use the preaggregated metrics and experience better performance and support of the near real-time dimensional alerting with SDKs that don't preaggregate metrics during collection.</p> <p>It's worth mentioning that the collection endpoint preaggregates events before ingestion sampling, which means that\u00a0ingestion sampling\u00a0will never impact the accuracy of preaggregated metrics, regardless of the SDK version you use with your application.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#instrument-an-app-for-monitoring","title":"Instrument an app for monitoring","text":"<p>At a basic level, \"instrumenting\" is simply enabling an application to capture telemetry. There are two methods to instrument your application:</p> <ul> <li>Automatic instrumentation (autoinstrumentation)</li> <li>Manual instrumentation</li> </ul> <p>Autoinstrumentation\u00a0enables telemetry collection through configuration without touching the application's code. Although it's more convenient, it tends to be less configurable. It's also not available in all languages. See\u00a0Autoinstrumentation supported environments and languages. When autoinstrumentation is available, it's the easiest way to enable Azure Monitor Application Insights.</p> <p>Manual instrumentation\u00a0is coding against the Application Insights or OpenTelemetry API. In the context of a user, it typically refers to installing a language-specific SDK in an application. This means that you have to manage the updates to the latest package version by yourself. You can use this option if you need to make custom dependency calls or API calls that are not captured by default with autoinstrumentation. There are two options for manual instrumentation:</p> <ul> <li>Application Insights SDKs</li> <li>Azure Monitor OpenTelemetry Distros.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#enabling-via-application-insights-sdks","title":"Enabling via Application Insights SDKs","text":"<p>You only need to install the Application Insights SDK in the following circumstances:</p> <ul> <li>You require custom events and metrics</li> <li>You require control over the flow of telemetry</li> <li>Auto-Instrumentation isn't available (typically due to language or platform limitations)</li> </ul> <p>To use the SDK, you install a small instrumentation package in your app and then instrument the web app, any background components, and JavaScript within the web pages. The app and its components don't have to be hosted in Azure. The instrumentation monitors your app and directs the telemetry data to an Application Insights resource by using a unique token.</p> <p>A list of SDK versions and names is hosted on GitHub. For more information, visit\u00a0SDK Version.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#enable-via-opentelemetry","title":"Enable via OpenTelemetry","text":"<p>Microsoft worked with project stakeholders from two previously popular open-source telemetry projects,\u00a0OpenCensus\u00a0and\u00a0OpenTracing. Together, we helped to create a single project, OpenTelemetry. OpenTelemetry includes contributions from all major cloud and Application Performance Management (APM) vendors and lives within the\u00a0Cloud Native Computing Foundation (CNCF). Microsoft is a Platinum Member of the CNCF.</p> <p>Some legacy terms in Application Insights are confusing because of the industry convergence on OpenTelemetry. The following table highlights these differences. OpenTelemetry terms are replacing Application Insights terms.</p> Application Insights OpenTelemetry Autocollectors Instrumentation libraries Channel Exporter Codeless / Agent-based Autoinstrumentation Traces Logs Requests Server Spans Dependencies Other Span Types (Client, Internal, etc.) Operation ID Trace ID ID or Operation Parent ID Span ID"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#select-an-availability-test","title":"Select an availability test","text":"<p>After you deploy your web app or website, you can set up recurring tests to monitor availability and responsiveness. Application Insights sends web requests to your application at regular intervals from points around the world. It can alert you if your application isn't responding or responds too slowly. You can create up to 100 availability tests per Application Insights resource.</p> <p>Availability tests don't require any changes to the website you're testing and work for any HTTP or HTTPS endpoint that's accessible from the public internet. You can also test the availability of a REST API that your service depends on.</p> <p>You can create up to 100 availability tests per Application Insights resource, and there are three types of availability tests:</p> <ul> <li>Standard test:\u00a0This is a type of availability test that checks the availability of a website by sending a single request, similar to the deprecated URL ping test. In addition to validating whether an endpoint is responding and measuring the performance, Standard tests also include TLS/SSL certificate validity, proactive lifetime check, HTTP request verb (for example,\u00a0<code>GET</code>,<code>HEAD</code>, and\u00a0<code>POST</code>), custom headers, and custom data associated with your HTTP request.</li> <li>Custom TrackAvailability test:\u00a0If you decide to create a custom application to run availability tests, you can use the\u00a0TrackAvailability()\u00a0method to send the results to Application Insights.</li> <li>URL ping test (classic): You can create this test through the portal to validate whether an endpoint is responding and measure performance associated with that response. You can also set custom success criteria coupled with more advanced features, like parsing dependent requests and allowing for retries.</li> </ul> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2011%20Revision/#troubleshoot-app-performance-by-using-application-map","title":"Troubleshoot app performance by using Application Map","text":"<p>Application Map helps you spot performance bottlenecks or failure hotspots across all components of your distributed application. Each node on the map represents an application component or its dependencies; and has health key performance indicator and alerts status. You can select through from any component to more detailed diagnostics, such as Application Insights events. If your app uses Azure services, you can also select through to Azure diagnostics, such as SQL Database Advisor recommendations.</p> <p>Components are independently deployable parts of your distributed/microservices application. Developers and operations teams have code-level visibility or access to telemetry generated by these application components.</p> <ul> <li>Components are different from \"observed\" external dependencies such as SQL, Event Hubs, etc. which your team/organization may not have access to (code or telemetry).</li> <li>Components run on any number of server/role/container instances.</li> <li>Components can be separate Application Insights instrumentation keys (even if subscriptions are different) or different roles reporting to a single Application Insights instrumentation key. The preview map experience shows the components regardless of their configuration.</li> </ul> <p>You can see the full application topology across multiple levels of related application components. Components could be different Application Insights resources, or different roles in a single resource. The app map finds components by following HTTP dependency calls made between servers with the Application Insights SDK installed.</p> <p>This experience starts with progressive discovery of the components. When you first load the application map, a set of queries is triggered to discover the components related to this component. A button at the top-left corner updates with the number of components in your application as they're discovered.</p> <p>Selecting\u00a0Update map components\u00a0refreshes with all components discovered until that point. Depending on the complexity of your application, this may take a minute to load.</p> <p>If all of the components are roles within a single Application Insights resource, then this discovery step isn't required. The initial load for such an application has all its components.</p> <p></p> <p>One of the key objectives with this experience is to be able to visualize complex topologies with hundreds of components. Click on any component to see related insights and go to the performance and failure triage experience for that component.</p> <p></p> <p>Application Map uses the cloud role name property to identify the components on the map. You can manually set or override the cloud role name and change what gets displayed on the Application Map.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/","title":"Caching for solutions","text":""},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#azure-cache-for-redis","title":"Azure Cache for Redis","text":"<p>After completing this module, you'll be able to:</p> <ul> <li>Explain the key scenarios Azure Cache for Redis covers and its service tiers</li> <li>Identify the key parameters for creating an Azure Cache for Redis instance and interact with the cache</li> <li>Connect an app to Azure Cache for Redis by using .NET</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#key-scenarios","title":"Key scenarios","text":"<p>Azure Cache for Redis improves application performance by supporting common application architecture patterns. Some of the most common include the following patterns:</p> Pattern Description Data cache Databases are often too large to load directly into a cache. It's common to use the\u00a0cache-aside\u00a0pattern to load data into the cache only as needed. When the system makes changes to the data, the system can also update the cache, which is then distributed to other clients. Content cache Many web pages are generated from templates that use static content such as headers, footers, banners. These static items shouldn't change often. Using an in-memory cache provides quick access to static content compared to backend datastores. Session store This pattern is commonly used with shopping carts and other user history data that a web application might associate with user cookies. Storing too much in a cookie can have a negative effect on performance as the cookie size grows and is passed and validated with every request. A typical solution uses the cookie as a key to query the data in a database. Using an in-memory cache, like Azure Cache for Redis, to associate information with a user is faster than interacting with a full relational database. Job and message queuing Applications often add tasks to a queue when the operations associated with the request take time to execute. Longer running operations are queued to be processed in sequence, often by another server. This method of deferring work is called task queuing. Distributed transactions Applications sometimes require a series of commands against a backend data-store to execute as a single atomic operation. All commands must succeed, or all must be rolled back to the initial state. Azure Cache for Redis supports executing a batch of commands as a single\u00a0transaction."},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#service-tiers","title":"Service tiers","text":"<p>Azure Cache for Redis is available in these tiers:</p> Tier Description Basic An OSS Redis cache running on a single virtual machine (VM). This tier has no service-level agreement (SLA) and is ideal for development/test and noncritical workloads. Standard An OSS Redis cache running on two VMs in a replicated configuration. Premium High-performance OSS Redis caches. This tier offers higher throughput, lower latency, better availability, and more features. Premium caches are deployed on more powerful VMs compared to the VMs for Basic or Standard caches. Enterprise High-performance caches powered by Redis Labs' Redis Enterprise software. This tier supports Redis modules including RediSearch, RedisBloom, and RedisTimeSeries. Also, it offers even higher availability than the Premium tier. Enterprise Flash Cost-effective large caches powered by Redis Labs' Redis Enterprise software. This tier extends Redis data storage to nonvolatile memory, which is cheaper than DRAM, on a VM. It reduces the overall per-GB memory cost. <p>The\u00a0Azure Cache for Redis Pricing\u00a0provides a detailed comparison of each tier.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#configure-azure-cache-for-redis","title":"Configure Azure Cache for Redis","text":"<p>Go here</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#interact-with-azure-cache-for-redis-by-using-net","title":"Interact with Azure Cache for Redis by using .NET","text":"<p>Go here and here</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#develop-for-storage-on-cdns","title":"Develop for storage on CDNs","text":"<p>A content delivery network (CDN) is a distributed network of servers that can efficiently deliver web content to users. A CDN stores cached content on edge servers in point-of-presence (POP) locations that are close to end users, to minimize latency.</p> <p>After completing this module, you'll be able to:</p> <ul> <li>Explain how the Azure Content Delivery Network works and how it can improve the user experience.</li> <li>Control caching behavior and purge content.</li> <li>Perform actions on Azure CDN by using the Azure CDN Library for .NET.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#azure-content-delivery-networks","title":"Azure Content Delivery Networks","text":"<p>Azure Content Delivery Network (CDN) offers developers a global solution for rapidly delivering high-bandwidth content to users. It caches content at strategically placed physical nodes across the world. Azure CDN can also accelerate dynamic content, which can't be cached, by using various network optimizations using CDN POPs. For example, route optimization to bypass Border Gateway Protocol (BGP).</p> <p>The benefits of using Azure CDN to deliver web site assets include:</p> <ul> <li>Better performance and improved user experience for end users, especially when using applications in which multiple round-trips are required to load content.</li> <li>Large scaling to better handle instantaneous high loads, such as the start of a product launch event.</li> <li>Distribution of user requests and serving of content directly from edge servers so that less traffic is sent to the origin server.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#how-azure-content-delivery-network-works","title":"How Azure Content Delivery Network works","text":"<ol> <li>A user (Alice) requests a file (also called an asset) by using a URL with a special domain name, such as\u00a0<code>&lt;endpoint name&gt;.azureedge.net</code>. This name can be an endpoint hostname or a custom domain. The DNS routes the request to the best performing POP location, which is usually the POP that is geographically closest to the user.</li> <li>If no edge servers in the POP have the file in their cache, the POP requests the file from the origin server. The origin server can be an Azure Web App, Azure Cloud Service, Azure Storage account, or any publicly accessible web server.</li> <li>The origin server returns the file to an edge server in the POP.</li> <li>An edge server in the POP caches the file and returns the file to the original requestor (Alice). The file remains cached on the edge server in the POP until the time-to-live (TTL) specified by its HTTP headers expires. If the origin server didn't specify a TTL, the default TTL is seven days.</li> <li>Other users can then request the same file by using the same URL that Alice used, and can also be directed to the same POP.</li> <li>If the TTL for the file hasn't expired, the POP edge server returns the file directly from the cache. This process results in a faster, more responsive user experience.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#requirements","title":"Requirements","text":"<ul> <li>To use Azure Content Delivery Network, you must own at least one Azure subscription.</li> <li>You also need to create a content delivery network profile, which is a collection of content delivery network endpoints. Every content delivery network endpoint is a specific configuration which users can customize with required content delivery behaviour and access. To organize your content delivery network endpoints by internet domain, web application, or some other criteria, you can use multiple profiles.</li> <li>Since\u00a0Azure Content Delivery Network pricing\u00a0gets applied at the content delivery network profile level. If you want to use a mix of pricing tiers you must create multiple content delivery network profiles.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#limitations","title":"Limitations","text":"<p>Each Azure subscription has default limits for the following resources:</p> <ul> <li>The number of CDN profiles that can be created.</li> <li>The number of endpoints that can be created in a CDN profile.</li> <li>The number of custom domains that can be mapped to an endpoint.</li> </ul> <p>For more information about CDN subscription limits, visit\u00a0CDN limits.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#azure-cdn-features","title":"Azure CDN features","text":"<p>Azure CDN offers the following key features:</p> <ul> <li>Dynamic site acceleration</li> <li>CDN caching rules</li> <li>HTTPS custom domain support</li> <li>Azure diagnostics logs</li> <li>File compression</li> <li>Geo-filtering</li> </ul> <p>For a complete list of features that each Azure CDN product supports, visit\u00a0Compare Azure CDN product features.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#controlling-caching-behavior","title":"Controlling caching behavior","text":"<p>You can use content delivery network caching rules to set or modify default cache expiration behavior. These caching rules can either be global or with custom conditions. Azure Content Delivery Network offers two ways to control how your files get cached:</p> <ul> <li>Caching rules:\u00a0Azure Content Delivery Network provides global and custom types of caching rules.<ul> <li>Global caching rules - You can set one global caching rule for each endpoint in your profile, which affects all requests to the endpoint. The global caching rule overrides any HTTP cache-directive headers, if set.</li> <li>Custom caching rules - You can set one or more custom caching rules for each endpoint in your profile. Custom caching rules match specific paths and file extensions, get processed in order, and override the global caching rule, if set.</li> </ul> </li> <li>Query string caching:\u00a0You can adjust how the Azure content delivery network treats caching for requests with query strings. If the file isn't cacheable, the query string caching setting has no effect, based on caching rules and content delivery network default behaviors.</li> </ul> <p></p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#standard-rules-engine","title":"Standard rules engine","text":"<p>In the Standard rules engine for Azure Content Delivery Network, a rule consists of one or more match conditions and an action. The rules engine is designed to be the final authority on how specific types of requests get processed by Standard Azure Content Delivery Network.</p> <p>Common uses for the rules:</p> <ul> <li>Override or define a custom cache policy.</li> <li>Redirect requests.</li> <li>Modify HTTP request and response headers.</li> </ul> <p>A rule consists of one or more match conditions and an action. The first part of a rule is a match condition or set of match conditions. In the Standard rules engine for Azure Content Delivery Network, each rule can have up to four match conditions. A match condition identifies specific types of requests for which defined actions are performed. If you use multiple match conditions, the match conditions are grouped together by using\u00a0<code>AND</code>\u00a0logic. Following is a table highlighting a few of the available match options.</p> Match condition Description Device type Identifies requests made from a mobile device or desktop device. HTTP version Identifies requests based on the HTTP version of the request. Request cookies Identifies requests based on cookie information in the incoming request. Post argument Identifies requests based on arguments defined for the POST request method that's used in the request. Query string Identifies requests that contain a specific query string parameter. This parameter is set to a value that matches a specific pattern. <p>For a complete list of match conditions, visit\u00a0Match conditions in the Standard rules engine for Azure Content Delivery Network</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#caching-and-time-to-live","title":"Caching and time to live","text":"<p>Files from publicly accessible origin web servers can be cached in Azure Content Delivery Network until their time to live (TTL) elapses. The TTL gets determined by the\u00a0<code>Cache-Control</code>\u00a0header in the HTTP response from the origin server. This article describes how to set\u00a0<code>Cache-Control</code>\u00a0headers for the Web Apps feature of Microsoft Azure App Service, Azure Cloud Services, ASP.NET applications, and Internet Information Services (IIS) sites, all of which are configured similarly. You can set the\u00a0<code>Cache-Control</code>\u00a0header either by using configuration files or programmatically.</p> <p>If you don't set a TTL on a file, Azure CDN sets a default value. However, this default might be overridden if you set up caching rules in Azure. Default TTL values are as follows:</p> <ul> <li>Generalized web delivery optimizations: seven days</li> <li>Large file optimizations: one day</li> <li>Media streaming optimizations: one year</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#content-updating","title":"Content updating","text":"<p>Azure Content Delivery Network edge nodes cache contents until the content's time to live (TTL) expires. After the TTL expires, when a client makes a request for the content from the edge node, the edge node will retrieve a new updated copy of the content to serve to the client. Then the refreshed content in cache of the edge node.</p> <p>The best practice to make sure your users always obtain the latest copy of your assets is to version your assets for each update and publish them as new URLs. Content delivery network will immediately retrieve the new assets for the next client requests. Sometimes you might wish to purge cached content from all edge nodes and force them all to retrieve new updated assets. The reason might be due to updates to your web application, or to quickly update assets that contain incorrect information.</p> <p>You can purge content in several ways.</p> <ul> <li>On an endpoint by endpoint basis, or all endpoints simultaneously should you want to update everything on your CDN at once.</li> <li>Specify a file, by including the path to that file or all assets on the selected endpoint by checking the\u00a0Purge All\u00a0checkbox in the Azure portal.</li> <li>Based on wildcards (*) or using the root (/).</li> </ul> <p>The Azure CLI provides a special purge verb that unpublishes cached assets from an endpoint. This is useful if you have an application scenario where a large amount of data is invalidated and should be updated in the cache. To unpublish assets, you must specify either a file path, a wildcard directory, or both:</p> <pre><code>az cdn endpoint purge \\\n    --content-paths '/css/*' '/js/app.js' \\\n    --name ContosoEndpoint \\\n    --profile-name DemoProfile \\\n    --resource-group ExampleGroup\n</code></pre> <p>You can also preload assets into an endpoint. This is useful for scenarios where your application creates a large number of assets, and you want to improve the user experience by prepopulating the cache before any actual requests occur:</p> <pre><code>az cdn endpoint load \\\n    --content-paths '/img/*' '/js/module.js' \\\n    --name ContosoEndpoint \\\n    --profile-name DemoProfile \\\n    --resource-group ExampleGroup\n</code></pre>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#geo-filtering","title":"Geo-filtering","text":"<p>Geo-filtering enables you to allow or block content in specific countries/regions, based on the country/region code. In the Azure CDN Standard for Microsoft Tier, you can only allow or block the entire site.</p>"},{"location":"Cloud/Azure%20-%20204/Azure%20204%20-%20Module%2012%20Revision/#interact-with-azure-content-delivery-networks-by-using-net","title":"Interact with Azure Content Delivery Networks by using .NET","text":"<p>Go here</p>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/","title":"Exam Notes","text":""},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#app-service-plans","title":"App Service Plans","text":""},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#pricing-tiers","title":"Pricing Tiers","text":"<p>The pricing tier of an App Service plan determines what App Service features you get and how much you pay for the plan. There are a few categories of pricing tiers:</p> <ul> <li>Shared compute: Free and Shared, the two base tiers, runs an app on the same Azure VM as other App Service apps, including apps of other customers. These tiers allocate CPU quotas to each app that runs on the shared resources, and the resources can't scale out.</li> <li>Dedicated compute: The Basic, Standard, Premium, PremiumV2, and PremiumV3 tiers run apps on dedicated Azure VMs. Only apps in the same App Service plan share the same compute resources. The higher the tier, the more VM instances are available to you for scale-out.</li> <li>Isolated: The Isolated and IsolatedV2 tiers run dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.</li> </ul> <p>App Service on Linux isn't supported on Shared pricing tier.</p>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#scaling","title":"Scaling","text":"<ul> <li>An app runs on all the VM instances configured in the App Service plan.</li> <li>If multiple apps are in the same App Service plan, they all share the same VM instances.</li> <li>If you have multiple deployment slots for an app, all deployment slots also run on the same VM instances.</li> <li>If you enable diagnostic logs, perform backups, or run WebJobs, they also use CPU cycles and memory on these VM instances.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#when-to-isolate-your-app-service-plans","title":"When to isolate your app service plans","text":"<p>Isolate your app into a new App Service plan when:</p> <ul> <li>The app is resource-intensive.</li> <li>You want to scale the app independently from the other apps in the existing plan.</li> <li>The app needs resource in a different geographical region.</li> </ul> <p>This way you can allocate a new set of resources for your app and gain greater control of your apps.</p>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#azure-functions","title":"Azure Functions","text":""},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#hosting-plans","title":"Hosting Plans","text":"Hosting option Service Availability Container support Consumption plan Azure Functions Generally available (GA) None Flex Consumption plan Azure Functions Preview None Premium plan Azure Functions GA Linux Dedicated plan Azure Functions GA Linux Container Apps Azure Container Apps GA Linux #### Consumption plan <p>The Consumption plan is the default hosting plan. Pay for compute resources only when your functions are running (pay-as-you-go) with automatic scale. On the Consumption plan, instances of the Functions host are dynamically added and removed based on the number of incoming events.</p>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#flex-consumption-plan","title":"Flex Consumption plan","text":"<p>Get high scalability with compute choices, virtual networking, and pay-as-you-go billing. On the Flex Consumption plan, instances of the Functions host are dynamically added and removed based on the configured per instance concurrency and the number of incoming events.</p> <p>You can reduce cold starts by specifying the number of pre-provisioned (always ready) instances. Scales automatically based on demand.</p>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#premium-plan","title":"Premium plan","text":"<p>Automatically scales based on demand using prewarmed workers, which run applications with no delay after being idle, runs on more powerful instances, and connects to virtual networks.</p> <p>Consider the Azure Functions Premium plan in the following situations:</p> <ul> <li>Your function apps run continuously, or nearly continuously.</li> <li>You want more control of your instances and want to deploy multiple function apps on the same plan with event-driven scaling.</li> <li>You have a high number of small executions and a high execution bill, but low GB seconds in the Consumption plan.</li> <li>You need more CPU or memory options than are provided by consumption plans.</li> <li>Your code needs to run longer than the maximum execution time allowed on the Consumption plan.</li> <li>You require virtual network connectivity.</li> <li>You want to provide a custom Linux image in which to run your functions.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#dedicated-plan","title":"Dedicated plan","text":"<p>Run your functions within an App Service plan at regular App Service plan rates. Best for long-running scenarios where Durable Functions can't be used.</p> <p>Consider an App Service plan in the following situations:</p> <ul> <li>You must have fully predictable billing, or you need to manually scale instances.</li> <li>You want to run multiple web apps and function apps on the same plan</li> <li>You need access to larger compute size choices.</li> <li>Full compute isolation and secure network access provided by an App Service Environment (ASE).</li> <li>High memory usage and high scale (ASE).</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#container-apps","title":"Container Apps","text":"<p>Create and deploy containerized function apps in a fully managed environment hosted by Azure Container Apps.</p> <p>Use the Azure Functions programming model to build event-driven, serverless, cloud native function apps. Run your functions alongside other microservices, APIs, websites, and workflows as container-hosted programs.</p> <p>Consider hosting your functions on Container Apps in the following situations:</p> <ul> <li>You want to package custom libraries with your function code to support line-of-business apps.</li> <li>You need to migration code execution from on-premises or legacy apps to cloud native microservices running in containers.</li> <li>You want to avoid the overhead and complexity of managing Kubernetes clusters and dedicated compute.</li> <li>You need the high-end processing power provided by dedicated CPU compute resources for your functions.</li> </ul>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#function-app-timeouts","title":"Function App Timeouts","text":"<p>The <code>functionTimeout</code> property in the host.json project file specifies the timeout duration for functions in a function app. This property applies specifically to function executions. After the trigger starts function execution, the function needs to return/respond within the timeout duration.</p> <p>The following table shows the default and maximum values (in minutes) for specific plans:</p> Plan Default Maximum - 1 Consumption plan 5 10 Flex Consumption plan 30 Unlimited - 3 Premium plan 30 - 2 Unlimited - 3 Dedicated plan 30 - 2 Unlimited - 3 Container Apps 30 - 5 Unlimited - 3 <ol> <li>Regardless of the function app timeout setting, 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request.</li> <li>The default timeout for version 1.x of the Functions runtime is unlimited.</li> <li>Guaranteed for up to 60 minutes. OS and runtime patching, vulnerability patching, and scale in behaviours can still cancel function executions.</li> <li>In a Flex Consumption plan, the host doesn't enforce an execution time limit. However, there are currently no guarantees because the platform might need to terminate your instances during scale-in, deployments, or to apply updates.</li> <li>When the minimum number of replicas is set to zero, the default timeout depends on the specific triggers used in the app.</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#scaling-azure-functions","title":"Scaling Azure Functions","text":"Plan Scale out Max # instances Consumption plan Event driven. Scales out automatically, even during periods of high load. Functions infrastructure scales CPU and memory resources by adding more instances based on the number of incoming trigger events. Windows: 200  Linux: 100 Flex Consumption plan Per-function scaling. Event-driven scaling decisions are calculated on a per-function basis, which provides a more deterministic way of scaling the functions in your app. Limited only by total memory usage of all instances across a given region. Premium plan Event driven. Scale out automatically based on the number of events that its functions are triggered on. Windows: 100  Linux: 20-100 Dedicated plan Manual/autoscale 10-30  100 (ASE) Container Apps Event driven. Scale out automatically by adding more instances of the Functions host, based on the number of events that its functions are triggered on. 10-300 <ol> <li>During scale-out, there's currently a limit of 500 instances per subscription per hour for Linux 1. apps on a Consumption plan.</li> <li>In some regions, Linux apps on a Premium plan can scale to 100 instances.</li> <li>For specific limits for the various App Service plan options, see the App Service plan limits.</li> <li>On Container Apps, you can set the maximum number of replicas, which is honoured as long as there's enough cores quota available</li> </ol>"},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#azure-storage","title":"Azure Storage","text":""},{"location":"Cloud/Azure%20-%20204/Exam%20Notes/#types-of-storage-accounts","title":"Types of storage accounts","text":"<p>Azure Storage offers two performance levels of storage accounts, standard and premium. Each performance level supports different features and has its own pricing model.</p> <ul> <li>Standard: This is the standard general-purpose v2 account and is recommended for most scenarios using Azure Storage.</li> <li>Premium: Premium accounts offer higher performance by using solid-state drives. If you create a premium account you can choose between three account types, block blobs, page blobs, or file shares.</li> </ul> <p>The following table describes the types of storage accounts recommended by Microsoft for most scenarios using Blob storage.</p> Type of storage account Supported storage services Redundancy options Usage Standard general-purpose v2 Blob Storage (including Data Lake Storage), Queue Storage, Table Storage, and Azure Files Locally redundant storage (LRS) / geo-redundant storage (GRS) / read-access geo-redundant storage (RA-GRS)   Zone-redundant storage (ZRS) / geo-zone-redundant storage (GZRS) / read-access geo-zone-redundant storage (RA-GZRS) Standard storage account type for blobs, file shares, queues, and tables. Recommended for most scenarios using Azure Storage. If you want support for network file system (NFS) in Azure Files, use the premium file shares account type. Premium block blobs Blob Storage (including Data Lake Storage) LRS and ZRS Premium storage account type for block blobs and append blobs. Recommended for scenarios with high transaction rates or that use smaller objects or require consistently low storage latency. Premium file shares Azure Files LRS and ZRS Premium storage account type for file shares only. Recommended for enterprise or high-performance scale applications. Premium page blobs Page blobs only LRS and ZRS Premium storage account type for page blobs only. ### Access tiers for block blob data <p>The available access tiers are:</p> <ul> <li>The Hot access tier, which is optimized for frequent access of objects in the storage account. The Hot tier has the highest storage costs, but the lowest access costs. New storage accounts are created in the hot tier by default.</li> <li>The Cool access tier, which is optimized for storing large amounts of data that is infrequently accessed and stored for a minimum of 30 days. The Cool tier has lower storage costs and higher access costs compared to the Hot tier.</li> <li>The Cold access tier, which is optimized for storing data that is infrequently accessed and stored for a minimum of 90 days. The cold tier has lower storage costs and higher access costs compared to the cool tier.</li> <li>The Archive tier, which is available only for individual block blobs. The archive tier is optimized for data that can tolerate several hours of retrieval latency and remains in the Archive tier for a minimum 180 days. The archive tier is the most cost-effective option for storing data, but accessing that data is more expensive than accessing data in the hot or cool tiers.</li> </ul> <p>If there's a change in the usage pattern of your data, you can switch between these access tiers at any time.</p> <ul> <li>Hot - An online tier optimized for storing data that is accessed frequently.</li> <li>Cool - An online tier optimized for storing data that is infrequently accessed and stored for a minimum of 30 days.</li> <li>Cold tier - An online tier optimized for storing data that is infrequently accessed and stored for a minimum of 90 days. The cold tier has lower storage costs and higher access costs compared to the cool tier.</li> <li>Archive - An offline tier optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.</li> </ul>"},{"location":"Frameworks/ASP.NET/01%20-%20Getting%20Started/","title":"Getting Started","text":"<p>Open a command shell, and enter the following command:</p> <pre><code>dotnet new webapp --output aspnetcoreapp --no-https\n</code></pre> <p>The preceding command creates a new web app project in a directory named <code>aspnetcoreapp</code>. The project doesn't use HTTPS.</p>"},{"location":"Frameworks/ASP.NET/01%20-%20Getting%20Started/#run-the-app","title":"Run the app","text":"<p>Run the following commands:</p> <pre><code>cd aspnetcoreapp\ndotnet run\n</code></pre>"},{"location":"Frameworks/ASP.NET/01%20-%20Getting%20Started/#edit-a-razor-page","title":"Edit a Razor page","text":"<p>Change the home page:</p> <pre><code>@page\n@model IndexModel\n@{\n    ViewData[\"Title\"] = \"Home page\";\n}\n\n&lt;div class=\"text-center\"&gt;\n    &lt;h1 class=\"display-4\"&gt;Welcome&lt;/h1&gt;\n    &lt;p&gt;Hello, world! The time on the server is @DateTime.Now&lt;/p&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"Frameworks/Angular/001%20-%20Quick%20start/","title":"Quick start","text":"<p>To get started with angular, we will need the angular CLI:</p> <pre><code>npm install -g @angular/cli\n</code></pre>"},{"location":"Frameworks/Angular/001%20-%20Quick%20start/#scaffold-your-project","title":"Scaffold your project","text":"<p>In your terminal, run the CLI command <code>ng new</code> with the desired project name.</p> <pre><code>ng new &lt;project-name&gt;\n</code></pre>"},{"location":"Frameworks/Angular/001%20-%20Quick%20start/#run-the-project","title":"Run the project","text":"<p>To run the project, you can change into the project directory and run:</p> <pre><code>npm start\n</code></pre>"},{"location":"Frameworks/Django/001%20-%20Custom%20Admin%20Configuration/","title":"Configuring your admin models","text":"<p>To register an admin interface for your models, you do something similar to the following:</p> <pre><code>\"\"\"Contains the admin configuration for the stores app.\"\"\"\n\nimport logging\n\nfrom django.contrib import admin\nfrom django.contrib.admin import ModelAdmin\n\nfrom stores.models import ShoppingStore as Store\n\nlog = logging.getLogger(__name__)\nlog.info(\"Loading stores admin config...\")\n\n\nclass StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    pass\n\n\nadmin.site.register(Store, StoreAdmin)\n\nlog.info(\"Loaded stores admin config.\")\n</code></pre>"},{"location":"Frameworks/Django/001%20-%20Custom%20Admin%20Configuration/#update-what-gets-displayed-on-the-list-view","title":"Update what gets displayed on the list view","text":"<p>To update what gets displayed on the list view for the models/rows, you can update the <code>list_display</code> value on the admin model, example below:</p> <pre><code>class StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    list_display = [\"name\", \"store_type\", \"description\", \"created_at\", \"updated_at\", \"user\"]\n</code></pre>"},{"location":"Frameworks/Django/001%20-%20Custom%20Admin%20Configuration/#update-what-filters-are-available","title":"Update what filters are available","text":"<p>To update what filtering is possible on the list view for the models/rows, you can update the <code>list_filter</code> value on the admin model, example below:</p> <pre><code>class StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    list_display = [\"name\", \"store_type\", \"description\", \"created_at\", \"updated_at\", \"user\"]\n    list_filter = [\"store_type\", \"user\", \"created_at\", \"updated_at\"]\n</code></pre> <p>You can also use search bar to filter stores, you should specify what fields should be searched, example below:</p> <pre><code>class StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    list_display = [\"name\", \"store_type\", \"description\", \"created_at\", \"updated_at\", \"user\"]\n    list_filter = [\"store_type\", \"user\", \"created_at\", \"updated_at\"]\n    search_fields = [\"name\", \"description\"]\n</code></pre>"},{"location":"Frameworks/Django/001%20-%20Custom%20Admin%20Configuration/#create-actions","title":"Create actions","text":"<p>You can create pre-determined actions for bulk updating your models in the admin page, you can do as in the example below:</p> <pre><code>class StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    list_display = [\"name\", \"store_type\", \"description\", \"created_at\", \"updated_at\", \"user\"]\n    list_filter = [\"store_type\", \"user\", \"created_at\", \"updated_at\"]\n    search_fields = [\"name\", \"description\"]\n    actions = [\"make_online\", \"make_in_store\", \"make_online_and_in_store\"]\n\n    @admin.action(description=\"Mark selected stores as online.\")\n    def make_online(self, request, queryset) -&gt; None:  # type: ignore\n        \"\"\"Update all selected stores to be online.\"\"\"\n        queryset.update(store_type=1)\n\n    @admin.action(description=\"Mark selected stores as in-store.\")\n    def make_in_store(self, request, queryset) -&gt; None:  # type: ignore\n        \"\"\"Update all selected stores to be in-store.\"\"\"\n        queryset.update(store_type=2)\n\n    @admin.action(description=\"Mark selected stores as online and in-store.\")\n    def make_online_and_in_store(self, request, queryset) -&gt; None:  # type: ignore\n        \"\"\"Update all selected stores as online and in-store.\"\"\"\n        queryset.update(store_type=3)\n</code></pre>"},{"location":"Frameworks/Django/002%20-%20Caching%20with%20Django/","title":"Caching with django","text":"<p>You can use django's cache framework to utilize various different caching techniques. The below will just cover a basic setup with redis.</p>"},{"location":"Frameworks/Django/002%20-%20Caching%20with%20Django/#redis-python-package","title":"Redis python package","text":"<p>Ensure you have the python redis package installed that django requires.</p> <pre><code>pip install redis[hiredis]\n</code></pre>"},{"location":"Frameworks/Django/002%20-%20Caching%20with%20Django/#setup-up-caching-in-django-settings","title":"Setup up caching in django settings","text":"<pre><code>CACHES = {\n    \"default\": {\n        \"BACKEND\": \"django.core.cache.backends.redis.RedisCache\",\n        \"LOCATION\": \"redis://127.0.0.1:6379\",\n    }\n}\n</code></pre>"},{"location":"Frameworks/Django/002%20-%20Caching%20with%20Django/#per-view-caching","title":"Per view caching","text":"<pre><code>from django.views.decorators.cache import cache_page\n\n\n@cache_page(60 * 15) # Seconds to cache for, in this example 15 minutes\ndef my_view(request): ...\n</code></pre>"},{"location":"Frameworks/Django/002%20-%20Caching%20with%20Django/#using-the-cache-framework-directly","title":"Using the cache framework directly","text":"<pre><code>from django.core.cache import cache\n\ncache.set(\"my_key\", \"hello, world!\", 30)\ncache.get(\"my_key\")\ncache.delete(\"a\")\ncache.clear()\ncache.touch(\"a\", 10)\ncache.set(\"my_key\", \"hello world!\", version=2)\n</code></pre>"},{"location":"Frameworks/Django%20Ninja/001%20-%20Parsing%20Input/001%20-%20HTTP%20Methods/","title":"HTTP Methods","text":""},{"location":"Frameworks/Django%20Ninja/001%20-%20Parsing%20Input/001%20-%20HTTP%20Methods/#defining-operations","title":"Defining operations","text":"<p>An <code>operation</code> can be one of the following HTTP methods:</p> <ul> <li>GET</li> <li>POST</li> <li>PUT</li> <li>DELETE</li> <li>PATCH</li> </ul> <p>Django Ninja comes with a decorator for each operation:</p> <p><pre><code>@api.get(\"/path\")\ndef get_operation(request):\n    ...\n\n@api.post(\"/path\")\ndef post_operation(request):\n    ...\n\n@api.put(\"/path\")\ndef put_operation(request):\n    ...\n\n@api.delete(\"/path\")\ndef delete_operation(request):\n    ...\n\n@api.patch(\"/path\")\ndef patch_operation(request):\n    ...\n</code></pre> See the operations parameters reference docs for information on what you can pass to any of these decorators.</p>"},{"location":"Frameworks/Django%20Ninja/001%20-%20Parsing%20Input/001%20-%20HTTP%20Methods/#handling-multiple-methods","title":"Handling multiple methods","text":"<p>If you need to handle multiple methods with a single function for a given path, you can use the <code>api_operation</code> decorator:</p> <pre><code>@api.api_operation([\"POST\", \"PATCH\"], \"/path\")\ndef mixed_operation(request):\n    ...\n</code></pre> <p>This feature can also be used to implement other HTTP methods that don't have corresponding Django Ninja methods, such as <code>HEAD</code> or <code>OPTIONS</code>.</p> <pre><code>@api.api_operation([\"HEAD\", \"OPTIONS\"], \"/path\")\ndef mixed_operation(request):\n    ...\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/","title":"Getting Started with Laravel","text":"<p>You will need npm and node for these projects.</p>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#setting-up-node-and-npm","title":"Setting up node and npm","text":"<pre><code>wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash\nnvm install --lts\nnvm use --lts\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#setting-up-laravel","title":"Setting up laravel","text":"<pre><code>/bin/bash -c \"$(curl -fsSL https://php.new/install/linux)\"\ncomposer global require laravel/installer\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#creating-new-application","title":"Creating new application","text":"<p>This will help you setup a very bare-bones basic application:</p> <pre><code>laravel new Play\n</code></pre> <pre><code>   _                               _\n  | |                             | |\n  | |     __ _ _ __ __ ___   _____| |\n  | |    / _` | '__/ _` \\ \\ / / _ \\ |\n  | |___| (_| | | | (_| |\\ V /  __/ |\n  |______\\__,_|_|  \\__,_| \\_/ \\___|_|\n\n\n \u250c Would you like to install a starter kit? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 No starter kit                                               \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n \u250c Which testing framework do you prefer? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Pest                                                         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n \u250c Would you like to initialize a Git repository? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 No                                                           \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n \u250c Which database will your application use? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 SQLite                                                       \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n \u250c Would you like to run the default database migrations? \u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Yes                                                          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#start-the-application","title":"Start the application","text":"<p>Start the application:</p> <pre><code>cd Play\nnpm install &amp;&amp; npm run build\ncomposer run dev\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#docker","title":"Docker","text":"<p>You can also setup up Laravel to run with docker:</p>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#dockerignore","title":".dockerignore","text":"<p>Create a .dockerignore with the following values:</p> <pre><code>vendor\nnode_modules\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#custom-apache-config","title":"Custom Apache Config","text":"<p>You will need to setup a custom apache config file, within the config directory (<code>config/custom.conf</code>):</p> <pre><code>&lt;Directory /var/www/html/public/&gt;\n    Options Indexes FollowSymLinks\n    AllowOverride All\n    Require all granted\n&lt;/Directory&gt;\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM node:20.17.0-alpine3.20 AS build\n\nWORKDIR /var/www/html\n\nCOPY package.json /var/www/html/package.json\nCOPY package-lock.json /var/www/html/package-lock.json\nCOPY postcss.config.js /var/www/html/postcss.config.js\nCOPY tailwind.config.js /var/www/html/tailwind.config.js\nCOPY vite.config.js /var/www/html/vite.config.js\n\nCOPY resources /var/www/html/resources\nCOPY public /var/www/html/public\n\nRUN npm ci &amp;&amp; npm run build\n\nFROM php:8.3.13-apache AS final\n\nWORKDIR /var/www/html\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libzip-dev \\\n    zip \\\n    libcurl4-gnutls-dev\n\nRUN mv \"$PHP_INI_DIR/php.ini-production\" \"$PHP_INI_DIR/php.ini\"\nRUN docker-php-ext-install curl\n\nENV APACHE_DOCUMENT_ROOT /var/www/html/public\nRUN sed -ri -e 's!/var/www/html!${APACHE_DOCUMENT_ROOT}!g' /etc/apache2/sites-available/000-default.conf\nRUN sed -ri -e 's!/var/www/!${APACHE_DOCUMENT_ROOT}!g' /etc/apache2/apache2.conf\n\nCOPY ./config/custom.conf /etc/apache2/conf-available/custom.conf\nRUN a2enmod rewrite\nRUN a2enconf custom\n\nCOPY --from=composer:2.8.2 /usr/bin/composer /usr/bin/composer\n\nCOPY app /var/www/html/app\nCOPY bootstrap /var/www/html/bootstrap\nCOPY config /var/www/html/config\nCOPY database /var/www/html/database\nCOPY routes /var/www/html/routes\nCOPY storage /var/www/html/storage\n\nCOPY artisan /var/www/html/artisan\nCOPY composer.json /var/www/html/composer.json\nCOPY composer.lock /var/www/html/composer.lock\n\nCOPY --from=build /var/www/html/resources /var/www/html/resources\nCOPY --from=build /var/www/html/public /var/www/html/public\n\nCOPY .env /var/www/html/.env\n\nRUN composer install --no-interaction\n\nRUN php artisan optimize\nRUN php artisan config:cache\nRUN php artisan event:cache\nRUN php artisan route:cache\nRUN php artisan view:cache\n\nRUN chown -R www-data:www-data /var/www/html/\nUSER www-data\nEXPOSE 80\n</code></pre>"},{"location":"Frameworks/Laravel/01%20-%20Getting%20Started/#compose-file","title":"Compose file","text":"<p>You can setup your compose.yaml file like so:</p> <pre><code>services:\n  server:\n    build: .\n    ports:\n      - 8000:80\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/001%20-%20Request%20lifecycle/","title":"Lifecycle Overview","text":""},{"location":"Frameworks/Laravel/02%20-%20Architecture/001%20-%20Request%20lifecycle/#first-steps","title":"First Steps","text":"<p>The entry point for all requests to a Laravel application is the <code>public/index.php</code> file. All requests are directed to this file by your web server (Apache / Nginx) configuration. The <code>index.php</code> file doesn't contain much code. Rather, it is a starting point for loading the rest of the framework.</p> <p>The <code>index.php</code> file loads the Composer generated autoloader definition, and then retrieves an instance of the Laravel application from <code>bootstrap/app.php</code>. The first action taken by Laravel itself is to create an instance of the application / service container.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/001%20-%20Request%20lifecycle/#http-console-kernels","title":"HTTP / Console Kernels","text":"<p>Next, the incoming request is sent to either the HTTP kernel or the console kernel, using the <code>handleRequest</code> or <code>handleCommand</code> methods of the application instance, depending on the type of request entering the application. These two kernels serve as the central location through which all requests flow. For now, let's just focus on the HTTP kernel, which is an instance of <code>Illuminate\\Foundation\\Http\\Kernel</code>.</p> <p>The HTTP kernel defines an array of <code>bootstrappers</code> that will be run before the request is executed. These bootstrappers configure error handling, configure logging, detect the application environment, and perform other tasks that need to be done before the request is actually handled. Typically, these classes handle internal Laravel configuration that you do not need to worry about.</p> <p>The HTTP kernel is also responsible for passing the request through the application's middleware stack. These middleware handle reading and writing the HTTP session, determining if the application is in maintenance mode, verifying the CSRF token, and more. We'll talk more about these soon.</p> <p>The method signature for the HTTP kernel's <code>handle</code> method is quite simple: it receives a <code>Request</code> and returns a <code>Response</code>. Think of the kernel as being a big black box that represents your entire application. Feed it HTTP requests and it will return HTTP responses.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/001%20-%20Request%20lifecycle/#service-providers","title":"Service Providers","text":"<p>One of the most important kernel bootstrapping actions is loading the service providers for your application. Service providers are responsible for bootstrapping all of the framework's various components, such as the database, queue, validation, and routing components.</p> <p>Laravel will iterate through this list of providers and instantiate each of them. After instantiating the providers, the <code>register</code> method will be called on all of the providers. Then, once all of the providers have been registered, the <code>boot</code> method will be called on each provider. This is so service providers may depend on every container binding being registered and available by the time their <code>boot</code> method is executed.</p> <p>Essentially every major feature offered by Laravel is bootstrapped and configured by a service provider. Since they bootstrap and configure so many features offered by the framework, service providers are the most important aspect of the entire Laravel bootstrap process.</p> <p>While the framework internally uses dozens of service providers, you also have the option to create your own. You can find a list of the user-defined or third-party service providers that your application is using in the <code>bootstrap/providers.php</code> file.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/001%20-%20Request%20lifecycle/#routing","title":"Routing","text":"<p>Once the application has been bootstrapped and all service providers have been registered, the <code>Request</code> will be handed off to the router for dispatching. The router will dispatch the request to a route or controller, as well as run any route specific middleware.</p> <p>Middleware provide a convenient mechanism for filtering or examining HTTP requests entering your application. For example, Laravel includes a middleware that verifies if the user of your application is authenticated. If the user is not authenticated, the middleware will redirect the user to the login screen. However, if the user is authenticated, the middleware will allow the request to proceed further into the application. Some middleware are assigned to all routes within the application, like <code>PreventRequestsDuringMaintenance</code>, while some are only assigned to specific routes or route groups. You can learn more about middleware by reading the complete middleware documentation.</p> <p>If the request passes through all of the matched route's assigned middleware, the route or controller method will be executed and the response returned by the route or controller method will be sent back through the route's chain of middleware.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/001%20-%20Request%20lifecycle/#finishing-up","title":"Finishing Up","text":"<p>Once the route or controller method returns a response, the response will travel back outward through the route's middleware, giving the application a chance to modify or examine the outgoing response.</p> <p>Finally, once the response travels back through the middleware, the HTTP kernel's <code>handle</code> method returns the response object to the <code>handleRequest</code> of the application instance, and this method calls the <code>send</code> method on the returned response. The <code>send</code> method sends the response content to the user's web browser. We've now completed our journey through the entire Laravel request lifecycle!</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/001%20-%20Request%20lifecycle/#focus-on-service-providers","title":"Focus on Service Providers","text":"<p>Service providers are truly the key to bootstrapping a Laravel application. The application instance is created, the service providers are registered, and the request is handed to the bootstrapped application. It's really that simple!</p> <p>Having a firm grasp of how a Laravel application is built and bootstrapped via service providers is very valuable. Your application's user-defined service providers are stored in the <code>app/Providers</code> directory.</p> <p>By default, the <code>AppServiceProvider</code> is fairly empty. This provider is a great place to add your application's own bootstrapping and service container bindings. For large applications, you may wish to create several service providers, each with more granular bootstrapping for specific services used by your application.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/","title":"Service Container","text":"<p>The Laravel service container is a powerful tool for managing class dependencies and performing dependency injection. Dependency injection is a fancy phrase that essentially means this: class dependencies are \"injected\" into the class via the constructor or, in some cases, \"setter\" methods.</p> <p>Let's look at a simple example:</p> <pre><code>&lt;?php\n\nnamespace App\\Http\\Controllers;\n\nuse App\\Services\\AppleMusic;\nuse Illuminate\\View\\View;\n\nclass PodcastController extends Controller\n{\n    /**\n     * Create a new controller instance.\n     */\n    public function __construct(\n        protected AppleMusic $apple,\n    ) {}\n\n    /**\n     * Show information about the given podcast.\n     */\n    public function show(string $id): View\n    {\n        return view('podcasts.show', [\n            'podcast' =&gt; $this-&gt;apple-&gt;findPodcast($id)\n        ]);\n    }\n}\n</code></pre> <p>In this example, the <code>PodcastController</code> needs to retrieve podcasts from a data source such as Apple Music. So, we will inject a service that is able to retrieve podcasts. Since the service is injected, we are able to easily \"mock\", or create a dummy implementation of the <code>AppleMusic</code> service when testing our application.</p> <p>A deep understanding of the Laravel service container is essential to building a powerful, large application, as well as for contributing to the Laravel core itself.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#zero-configuration-resolution","title":"Zero Configuration Resolution","text":"<p>If a class has no dependencies or only depends on other concrete classes (not interfaces), the container does not need to be instructed on how to resolve that class. For example, you may place the following code in your <code>routes/web.php</code> file:</p> <pre><code>&lt;?php\n\nclass Service\n{\n    // ...\n}\n\nRoute::get('/', function (Service $service) {\n    die($service::class);\n});\n</code></pre> <p>In this example, hitting your application's <code>/</code> route will automatically resolve the <code>Service</code> class and inject it into your route's handler. This is game changing. It means you can develop your application and take advantage of dependency injection without worrying about bloated configuration files.</p> <p>Thankfully, many of the classes you will be writing when building a Laravel application automatically receive their dependencies via the container, including controllers, event listeners, middleware, and more. Additionally, you may type-hint dependencies in the <code>handle</code> method of queued jobs. Once you taste the power of automatic and zero configuration dependency injection it feels impossible to develop without it.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#when-to-utilize-the-container","title":"When to Utilize the Container","text":"<p>Thanks to zero configuration resolution, you will often type-hint dependencies on routes, controllers, event listeners, and elsewhere without ever manually interacting with the container. For example, you might type-hint the <code>Illuminate\\Http\\Request</code> object on your route definition so that you can easily access the current request. Even though we never have to interact with the container to write this code, it is managing the injection of these dependencies behind the scenes:</p> <pre><code>use Illuminate\\Http\\Request;\n\nRoute::get('/', function (Request $request) {\n    // ...\n});\n</code></pre> <p>In many cases, thanks to automatic dependency injection and facades, you can build Laravel applications without ever manually binding or resolving anything from the container. So, when would you ever manually interact with the container? Let's examine two situations.</p> <p>First, if you write a class that implements an interface and you wish to type-hint that interface on a route or class constructor, you must tell the container how to resolve that interface. Secondly, if you are writing a Laravel package that you plan to share with other Laravel developers, you may need to bind your package's services into the container.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding","title":"Binding","text":""},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding-basics","title":"Binding Basics","text":""},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#simple-bindings","title":"Simple Bindings","text":"<p>Almost all of your service container bindings will be registered within service providers, so most of these examples will demonstrate using the container in that context.</p> <p>Within a service provider, you always have access to the container via the <code>$this-&gt;app</code> property. We can register a binding using the <code>bind</code> method, passing the class or interface name that we wish to register along with a closure that returns an instance of the class:</p> <pre><code>use App\\Services\\Transistor;\nuse App\\Services\\PodcastParser;\nuse Illuminate\\Contracts\\Foundation\\Application;\n\n$this-&gt;app-&gt;bind(Transistor::class, function (Application $app) {\n    return new Transistor($app-&gt;make(PodcastParser::class));\n});\n</code></pre> <p>Note that we receive the container itself as an argument to the resolver. We can then use the container to resolve sub-dependencies of the object we are building.</p> <p>As mentioned, you will typically be interacting with the container within service providers; however, if you would like to interact with the container outside of a service provider, you may do so via the <code>App</code> facade:</p> <pre><code>use App\\Services\\Transistor;\nuse Illuminate\\Contracts\\Foundation\\Application;\nuse Illuminate\\Support\\Facades\\App;\n\nApp::bind(Transistor::class, function (Application $app) {\n    // ...\n});\n</code></pre> <p>You may use the <code>bindIf</code> method to register a container binding only if a binding has not already been registered for the given type:</p> <pre><code>$this-&gt;app-&gt;bindIf(Transistor::class, function (Application $app) {\n    return new Transistor($app-&gt;make(PodcastParser::class));\n});\n</code></pre> <p></p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding-a-singleton","title":"Binding a singleton","text":"<p>The <code>singleton</code> method binds a class or interface into the container that should only be resolved one time. Once a singleton binding is resolved, the same object instance will be returned on subsequent calls into the container:</p> <pre><code>use App\\Services\\Transistor;\nuse App\\Services\\PodcastParser;\nuse Illuminate\\Contracts\\Foundation\\Application;\n\n$this-&gt;app-&gt;singleton(Transistor::class, function (Application $app) {\n    return new Transistor($app-&gt;make(PodcastParser::class));\n});\n</code></pre> <p>You may use the <code>singletonIf</code> method to register a singleton container binding only if a binding has not already been registered for the given type:</p> <pre><code>$this-&gt;app-&gt;singletonIf(Transistor::class, function (Application $app) {\n    return new Transistor($app-&gt;make(PodcastParser::class));\n});\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding-scoped-singletons","title":"Binding Scoped Singletons","text":"<p>The <code>scoped</code> method binds a class or interface into the container that should only be resolved one time within a given Laravel request / job lifecycle. While this method is similar to the <code>singleton</code> method, instances registered using the <code>scoped</code> method will be flushed whenever the Laravel application starts a new \"lifecycle\", such as when a Laravel Octane worker processes a new request or when a Laravel queue worker processes a new job:</p> <pre><code>use App\\Services\\Transistor;\nuse App\\Services\\PodcastParser;\nuse Illuminate\\Contracts\\Foundation\\Application;\n\n$this-&gt;app-&gt;scoped(Transistor::class, function (Application $app) {\n    return new Transistor($app-&gt;make(PodcastParser::class));\n});\n</code></pre> <p>You may use the <code>scopedIf</code> method to register a scoped container binding only if a binding has not already been registered for the given type:</p> <pre><code>$this-&gt;app-&gt;scopedIf(Transistor::class, function (Application $app) {\n    return new Transistor($app-&gt;make(PodcastParser::class));\n});\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding-instances","title":"Binding Instances","text":"<p>You may also bind an existing object instance into the container using the <code>instance</code> method. The given instance will always be returned on subsequent calls into the container:</p> <pre><code>use App\\Services\\Transistor;\nuse App\\Services\\PodcastParser;\n\n$service = new Transistor(new PodcastParser);\n\n$this-&gt;app-&gt;instance(Transistor::class, $service);\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding-interfaces-to-implementations","title":"Binding Interfaces to Implementations","text":"<p>A very powerful feature of the service container is its ability to bind an interface to a given implementation. For example, let's assume we have an EventPusher interface and a RedisEventPusher implementation. Once we have coded our RedisEventPusher implementation of this interface, we can register it with the service container like so:</p> <pre><code>use App\\Contracts\\EventPusher;\nuse App\\Services\\RedisEventPusher;\n\n$this-&gt;app-&gt;bind(EventPusher::class, RedisEventPusher::class);\n</code></pre> <p>This statement tells the container that it should inject the <code>RedisEventPusher</code> when a class needs an implementation of <code>EventPusher</code>. Now we can type-hint the <code>EventPusher</code> interface in the constructor of a class that is resolved by the container. Remember, controllers, event listeners, middleware, and various other types of classes within Laravel applications are always resolved using the container:</p> <pre><code>use App\\Contracts\\EventPusher;\n\n/**\n * Create a new class instance.\n */\npublic function __construct(\n    protected EventPusher $pusher,\n) {}\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#contextual-binding","title":"Contextual Binding","text":"<p>Sometimes you may have two classes that utilize the same interface, but you wish to inject different implementations into each class. For example, two controllers may depend on different implementations of the <code>Illuminate\\Contracts\\Filesystem\\Filesystem</code> contract. Laravel provides a simple, fluent interface for defining this behavior:</p> <pre><code>use App\\Http\\Controllers\\PhotoController;\nuse App\\Http\\Controllers\\UploadController;\nuse App\\Http\\Controllers\\VideoController;\nuse Illuminate\\Contracts\\Filesystem\\Filesystem;\nuse Illuminate\\Support\\Facades\\Storage;\n\n$this-&gt;app-&gt;when(PhotoController::class)\n          -&gt;needs(Filesystem::class)\n          -&gt;give(function () {\n              return Storage::disk('local');\n          });\n\n$this-&gt;app-&gt;when([VideoController::class, UploadController::class])\n          -&gt;needs(Filesystem::class)\n          -&gt;give(function () {\n              return Storage::disk('s3');\n          });\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#contextual-attributes","title":"Contextual Attributes","text":"<p>Since contextual binding is often used to inject implementations of drivers or configuration values, Laravel offers a variety of contextual binding attributes that allow to inject these types of values without manually defining the contextual bindings in your service providers.</p> <p>For example, the <code>Storage</code> attribute may be used to inject a specific storage disk:</p> <pre><code>&lt;?php\n\nnamespace App\\Http\\Controllers;\n\nuse Illuminate\\Container\\Attributes\\Storage;\nuse Illuminate\\Contracts\\Filesystem\\Filesystem;\n\nclass PhotoController extends Controller\n{\n    public function __construct(\n        #[Storage('local')] protected Filesystem $filesystem\n    )\n    {\n        // ...\n    }\n}\n</code></pre> <p>In addition to the <code>Storage</code> attribute, Laravel offers <code>Auth</code>, <code>Cache</code>, <code>Config</code>, <code>DB</code>, <code>Log</code>, <code>RouteParameter</code>, and <code>Tag</code> attributes:</p> <pre><code>&lt;?php\n\nnamespace App\\Http\\Controllers;\n\nuse App\\Models\\Photo;\nuse Illuminate\\Container\\Attributes\\Auth;\nuse Illuminate\\Container\\Attributes\\Cache;\nuse Illuminate\\Container\\Attributes\\Config;\nuse Illuminate\\Container\\Attributes\\DB;\nuse Illuminate\\Container\\Attributes\\Log;\nuse Illuminate\\Container\\Attributes\\RouteParameter;\nuse Illuminate\\Container\\Attributes\\Tag;\nuse Illuminate\\Contracts\\Auth\\Guard;\nuse Illuminate\\Contracts\\Cache\\Repository;\nuse Illuminate\\Database\\Connection;\nuse Psr\\Log\\LoggerInterface;\n\nclass PhotoController extends Controller\n{\n    public function __construct(\n        #[Auth('web')] protected Guard $auth,\n        #[Cache('redis')] protected Repository $cache,\n        #[Config('app.timezone')] protected string $timezone,\n        #[DB('mysql')] protected Connection $connection,\n        #[Log('daily')] protected LoggerInterface $log,\n        #[RouteParameter('photo')] protected Photo $photo,\n        #[Tag('reports')] protected iterable $reports,\n    )\n    {\n        // ...\n    }\n}\n</code></pre> <p>Furthermore, Laravel provides a <code>CurrentUser</code> attribute for injecting the currently authenticated user into a given route or class:</p> <pre><code>use App\\Models\\User;\nuse Illuminate\\Container\\Attributes\\CurrentUser;\n\nRoute::get('/user', function (#[CurrentUser] User $user) {\n    return $user;\n})-&gt;middleware('auth');\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#defining-custom-attributes","title":"Defining Custom Attributes","text":"<p>You can create your own contextual attributes by implementing the <code>Illuminate\\Contracts\\Container\\ContextualAttribute</code> contract. The container will call your attribute's <code>resolve</code> method, which should resolve the value that should be injected into the class utilizing the attribute. In the example below, we will re-implement Laravel's built-in <code>Config</code> attribute:</p> <pre><code>&lt;?php\n\nnamespace App\\Attributes;\n\nuse Illuminate\\Contracts\\Container\\ContextualAttribute;\n\n#[Attribute(Attribute::TARGET_PARAMETER)]\nclass Config implements ContextualAttribute\n{\n    /**\n     * Create a new attribute instance.\n     */\n    public function __construct(public string $key, public mixed $default = null)\n    {\n    }\n\n    /**\n     * Resolve the configuration value.\n     *\n     * @param  self  $attribute\n     * @param  \\Illuminate\\Contracts\\Container\\Container  $container\n     * @return mixed\n     */\n    public static function resolve(self $attribute, Container $container)\n    {\n        return $container-&gt;make('config')-&gt;get($attribute-&gt;key, $attribute-&gt;default);\n    }\n}\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding-primitives","title":"Binding Primitives","text":"<p>Sometimes you may have a class that receives some injected classes, but also needs an injected primitive value such as an integer. You may easily use contextual binding to inject any value your class may need:</p> <pre><code>use App\\Http\\Controllers\\UserController;\n\n$this-&gt;app-&gt;when(UserController::class)\n          -&gt;needs('$variableName')\n          -&gt;give($value);\n</code></pre> <p>Sometimes a class may depend on an array of tagged instances. Using the <code>giveTagged</code> method, you may easily inject all of the container bindings with that tag:</p> <pre><code>$this-&gt;app-&gt;when(ReportAggregator::class)\n    -&gt;needs('$reports')\n    -&gt;giveTagged('reports');\n</code></pre> <p>If you need to inject a value from one of your application's configuration files, you may use the <code>giveConfig</code> method:</p> <pre><code>$this-&gt;app-&gt;when(ReportAggregator::class)\n    -&gt;needs('$timezone')\n    -&gt;giveConfig('app.timezone');\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#binding-typed-variadics","title":"Binding Typed Variadics","text":"<p>Occasionally, you may have a class that receives an array of typed objects using a variadic constructor argument:</p> <pre><code>&lt;?php\n\nuse App\\Models\\Filter;\nuse App\\Services\\Logger;\n\nclass Firewall\n{\n    /**\n     * The filter instances.\n     *\n     * @var array\n     */\n    protected $filters;\n\n    /**\n     * Create a new class instance.\n     */\n    public function __construct(\n        protected Logger $logger,\n        Filter ...$filters,\n    ) {\n        $this-&gt;filters = $filters;\n    }\n}\n</code></pre> <p>Using contextual binding, you may resolve this dependency by providing the <code>give</code> method with a closure that returns an array of resolved <code>Filter</code> instances:</p> <pre><code>$this-&gt;app-&gt;when(Firewall::class)\n          -&gt;needs(Filter::class)\n          -&gt;give(function (Application $app) {\n                return [\n                    $app-&gt;make(NullFilter::class),\n                    $app-&gt;make(ProfanityFilter::class),\n                    $app-&gt;make(TooLongFilter::class),\n                ];\n          });\n</code></pre> <p>For convenience, you may also just provide an array of class names to be resolved by the container whenever <code>Firewall</code> needs <code>Filter</code> instances:</p> <pre><code>$this-&gt;app-&gt;when(Firewall::class)\n          -&gt;needs(Filter::class)\n          -&gt;give([\n              NullFilter::class,\n              ProfanityFilter::class,\n              TooLongFilter::class,\n          ]);\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#variadic-tag-dependencies","title":"Variadic Tag Dependencies","text":"<p>Sometimes a class may have a variadic dependency that is type-hinted as a given class (Report ...$reports). Using the needs and giveTagged methods, you may easily inject all of the container bindings with that tag for the given dependency:</p> <pre><code>$this-&gt;app-&gt;when(ReportAggregator::class)\n    -&gt;needs(Report::class)\n    -&gt;giveTagged('reports');\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#tagging","title":"Tagging","text":"<p>Occasionally, you may need to resolve all of a certain \"category\" of binding. For example, perhaps you are building a report analyzer that receives an array of many different <code>Report</code> interface implementations. After registering the <code>Report</code> implementations, you can assign them a tag using the <code>tag</code> method:</p> <pre><code>$this-&gt;app-&gt;bind(CpuReport::class, function () {\n    // ...\n});\n\n$this-&gt;app-&gt;bind(MemoryReport::class, function () {\n    // ...\n});\n\n$this-&gt;app-&gt;tag([CpuReport::class, MemoryReport::class], 'reports');\n</code></pre> <p>Once the services have been tagged, you may easily resolve them all via the container's <code>tagged</code> method:</p> <pre><code>$this-&gt;app-&gt;bind(ReportAnalyzer::class, function (Application $app) {\n    return new ReportAnalyzer($app-&gt;tagged('reports'));\n});\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#extending-bindings","title":"Extending Bindings","text":"<p>The extend method allows the modification of resolved services. For example, when a service is resolved, you may run additional code to decorate or configure the service. The extend method accepts two arguments, the service class you're extending and a closure that should return the modified service. The closure receives the service being resolved and the container instance:</p> <pre><code>$this-&gt;app-&gt;extend(Service::class, function (Service $service, Application $app) {\n    return new DecoratedService($service);\n});\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#resolving","title":"Resolving","text":""},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#the-make-method","title":"The <code>make</code> Method","text":"<p>You may use the <code>make</code> method to resolve a class instance from the container. The <code>make</code> method accepts the name of the class or interface you wish to resolve:</p> <pre><code>use App\\Services\\Transistor;\n\n$transistor = $this-&gt;app-&gt;make(Transistor::class);\n</code></pre> <p>If some of your class's dependencies are not resolvable via the container, you may inject them by passing them as an associative array into the <code>makeWith</code> method. For example, we may manually pass the <code>$id</code> constructor argument required by the <code>Transistor</code> service:</p> <pre><code>use App\\Services\\Transistor;\n\n$transistor = $this-&gt;app-&gt;makeWith(Transistor::class, ['id' =&gt; 1]);\n</code></pre> <p>The <code>bound</code> method may be used to determine if a class or interface has been explicitly bound in the container:</p> <pre><code>if ($this-&gt;app-&gt;bound(Transistor::class)) {\n    // ...\n}\n</code></pre> <p>If you are outside of a service provider in a location of your code that does not have access to the <code>$app</code> variable, you may use the <code>App</code> facade or the <code>app</code> helper to resolve a class instance from the container:</p> <pre><code>use App\\Services\\Transistor;\nuse Illuminate\\Support\\Facades\\App;\n\n$transistor = App::make(Transistor::class);\n\n$transistor = app(Transistor::class);\n</code></pre> <p>If you would like to have the Laravel container instance itself injected into a class that is being resolved by the container, you may type-hint the <code>Illuminate\\Container\\Container</code> class on your class's constructor:</p> <pre><code>use Illuminate\\Container\\Container;\n\n/**\n * Create a new class instance.\n */\npublic function __construct(\n    protected Container $container,\n) {}\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#automatic-injection","title":"Automatic Injection","text":"<p>Alternatively, and importantly, you may type-hint the dependency in the constructor of a class that is resolved by the container, including controllers, event listeners, middleware, and more. Additionally, you may type-hint dependencies in the <code>handle</code> method of queued jobs. In practice, this is how most of your objects should be resolved by the container.</p> <p>For example, you may type-hint a service defined by your application in a controller's constructor. The service will automatically be resolved and injected into the class:</p> <pre><code>&lt;?php\n\nnamespace App\\Http\\Controllers;\n\nuse App\\Services\\AppleMusic;\n\nclass PodcastController extends Controller\n{\n    /**\n     * Create a new controller instance.\n     */\n    public function __construct(\n        protected AppleMusic $apple,\n    ) {}\n\n    /**\n     * Show information about the given podcast.\n     */\n    public function show(string $id): Podcast\n    {\n        return $this-&gt;apple-&gt;findPodcast($id);\n    }\n}\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#method-invocation-and-injection","title":"Method Invocation and Injection","text":"<p>Sometimes you may wish to invoke a method on an object instance while allowing the container to automatically inject that method's dependencies. For example, given the following class:</p> <pre><code>&lt;?php\n\nnamespace App;\n\nuse App\\Services\\AppleMusic;\n\nclass PodcastStats\n{\n    /**\n     * Generate a new podcast stats report.\n     */\n    public function generate(AppleMusic $apple): array\n    {\n        return [\n            // ...\n        ];\n    }\n}\n</code></pre> <p>You may invoke the <code>generate</code> method via the container like so:</p> <pre><code>use App\\PodcastStats;\nuse Illuminate\\Support\\Facades\\App;\n\n$stats = App::call([new PodcastStats, 'generate']);\n</code></pre> <p>The <code>call</code> method accepts any PHP callable. The container's <code>call</code> method may even be used to invoke a closure while automatically injecting its dependencies:</p> <pre><code>use App\\Services\\AppleMusic;\nuse Illuminate\\Support\\Facades\\App;\n\n$result = App::call(function (AppleMusic $apple) {\n    // ...\n});\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#container-events","title":"Container Events","text":"<p>The service container fires an event each time it resolves an object. You may listen to this event using the <code>resolving</code> method:</p> <pre><code>use App\\Services\\Transistor;\nuse Illuminate\\Contracts\\Foundation\\Application;\n\n$this-&gt;app-&gt;resolving(Transistor::class, function (Transistor $transistor, Application $app) {\n    // Called when container resolves objects of type \"Transistor\"...\n});\n\n$this-&gt;app-&gt;resolving(function (mixed $object, Application $app) {\n    // Called when container resolves object of any type...\n});\n</code></pre> <p>As you can see, the object being resolved will be passed to the callback, allowing you to set any additional properties on the object before it is given to its consumer.</p>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#rebinding","title":"Rebinding","text":"<p>The <code>rebinding</code> method allows you to listen for when a service is re-bound to the container, meaning it is registered again or overridden after its initial binding. This can be useful when you need to update dependencies or modify behavior each time a specific binding is updated:</p> <pre><code>use App\\Contracts\\PodcastPublisher;\nuse App\\Services\\SpotifyPublisher;\nuse App\\Services\\TransistorPublisher;\nuse Illuminate\\Contracts\\Foundation\\Application;\n\n$this-&gt;app-&gt;bind(PodcastPublisher::class, SpotifyPublisher::class);\n\n$this-&gt;app-&gt;rebinding(\n    PodcastPublisher::class,\n    function (Application $app, PodcastPublisher $newInstance) {\n        //\n    },\n);\n\n// New binding will trigger rebinding closure...\n$this-&gt;app-&gt;bind(PodcastPublisher::class, TransistorPublisher::class);\n</code></pre>"},{"location":"Frameworks/Laravel/02%20-%20Architecture/002%20-%20Service%20Container/#psr-11","title":"PSR-11","text":"<p>Laravel's service container implements the PSR-11 interface. Therefore, you may type-hint the PSR-11 container interface to obtain an instance of the Laravel container:</p> <pre><code>use App\\Services\\Transistor;\nuse Psr\\Container\\ContainerInterface;\n\nRoute::get('/', function (ContainerInterface $container) {\n    $service = $container-&gt;get(Transistor::class);\n\n    // ...\n});\n</code></pre> <p>An exception is thrown if the given identifier can't be resolved. The exception will be an instance of <code>Psr\\Container\\NotFoundExceptionInterface</code> if the identifier was never bound. If the identifier was bound but was unable to be resolved, an instance of <code>Psr\\Container\\ContainerExceptionInterface</code> will be thrown.</p>"},{"location":"Frameworks/NestJS/001%20-%20Getting%20Started/","title":"Getting Started with NestJS","text":"<p>Please make sure that Node.js (version &gt;= 16) is installed on your operating system.</p>"},{"location":"Frameworks/NestJS/001%20-%20Getting%20Started/#setup","title":"Setup","text":"<p>Setting up a new project is quite simple with the Nest CLI. With npm installed, you can create a new Nest project with the following commands in your OS terminal:</p> <pre><code>npm i -g @nestjs/cli\nnest new project-name\n</code></pre>"},{"location":"Frameworks/NestJS/001%20-%20Getting%20Started/#running-the-application","title":"Running the application","text":"<p>Once the installation process is complete, you can run the following command at your OS command prompt to start the application listening for inbound HTTP requests:</p> <pre><code>$ npm run start\n</code></pre> <p>This command starts the app with the HTTP server listening on the port defined in the <code>src/main.ts</code> file. Once the application is running, open your browser and navigate to <code>http://localhost:3000/</code>. You should see the <code>Hello World!</code> message.</p> <p>To watch for changes in your files, you can run the following command to start the application:</p> <pre><code>$ npm run start:dev\n</code></pre> <p>This command will watch your files, automatically recompiling and reloading the server.</p>"},{"location":"Frameworks/React/001%20-%20Quick%20start/","title":"Quick start","text":"<p>I would recommend using <code>vite</code> for react applications.</p>"},{"location":"Frameworks/React/001%20-%20Quick%20start/#scaffolding-the-project","title":"Scaffolding the project","text":"<pre><code>npm create vite@latest\n</code></pre> <p>You can then use the following command to run the project, once done setting up:</p> <pre><code>npm run dev\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/01%20-%20Hello%20World/","title":"01 - Hello World","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/01%20-%20Hello%20World/#hello-world","title":"Hello World","text":"<p>The C programming language is a general purpose programming language, which relates closely to the way machines work. Understanding how computer memory works is an important aspect of the C programming language. Although C can be considered as \"hard to learn\", C is in fact a very simple language, with very powerful capabilities.</p> <p>C is a very common language, and it is the language of many applications such as Windows, the Python interpreter, Git, and many many more.</p> <p>C is a compiled language - which means that in order to run it, the compiler (for example, GCC or Visual Studio) must take the code that we wrote, process it, and then create an executable file. This file can then be executed, and will do what we intended for the program to do.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/01%20-%20Hello%20World/#our-first-program","title":"Our first program","text":"<p>Every C program uses libraries, which give the ability to execute necessary functions. For example, the most basic function called <code>printf</code>, which prints to the screen, is defined in the <code>stdio.h</code> header file.</p> <p>To add the ability to run the <code>printf</code> command to our program, we must add the following include directive to our first line of the code:</p> <pre><code>#include &lt;stdio.h&gt;\n</code></pre> <p>The second part of the code is the actual code which we are going to write. The first code which will run will always reside in the <code>main</code> function.</p> <pre><code>int main() {\n  ... our code goes here\n}\n</code></pre> <p>The <code>int</code> keyword indicates that the function <code>main</code> will return an integer - a simple number. The number which will be returned by the function indicates whether the program that we wrote worked correctly. If we want to say that our code was run successfully, we will return the number 0. A number greater than 0 will mean that the program that we wrote failed.</p> <p>For this tutorial, we will return 0 to indicate that our program was successful:</p> <pre><code>return 0;\n</code></pre> <p>Notice that every statement in C must end with a semicolon, so that the compiler knows that a new statement has started.</p> <p>Last but not least, we will need to call the function <code>printf</code> to print our sentence.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/01%20-%20Hello%20World/#exercise","title":"Exercise","text":"<p>Change the program at the bottom so that it prints to the output \"Hello, World!\".</p>"},{"location":"Languages/C/01%20-%20Fundamentals/01%20-%20Hello%20World/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  printf(\"Goodbye, World!\");\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/01%20-%20Hello%20World/#solution","title":"Solution","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  printf(\"Hello, World!\\n\");\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/02%20-%20Variables%20and%20Types/","title":"02 - Variables and Types","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#variables-and-types","title":"Variables and Types","text":""},{"location":"Languages/C/01%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#data-types","title":"Data types","text":"<p>C has several types of variables, but there are a few basic types:</p> <ul> <li>Integers - whole numbers which can be either positive or negative. Defined using <code>char</code>, <code>int</code>, <code>short</code>, <code>long</code> or <code>long long</code>.</li> <li>Unsigned integers - whole numbers which can only be positive. Defined using <code>unsigned char</code>, <code>unsigned int</code>, <code>unsigned short</code>, <code>unsigned long</code> or <code>unsigned long long</code>.</li> <li>Floating point numbers - real numbers (numbers with fractions). Defined using <code>float</code> and <code>double</code>.</li> <li>Structures - will be explained later, in the Structures section.</li> </ul> <p>The different types of variables define their bounds. A <code>char</code> can range only from -128 to 127, whereas a <code>long</code> can range from -2,147,483,648 to 2,147,483,647 (<code>long</code> and other numeric data types may have another range on different computers, for example - from \u20139,223,372,036,854,775,808 to 9,223,372,036,854,775,807 on 64-bit computer).</p> <p>Note that C does not have a boolean type. Usually, it is defined using the following notation:</p> <pre><code>#define BOOL char\n#define FALSE 0\n#define TRUE 1\n</code></pre> <p>C uses arrays of characters to define strings, and will be explained in the Strings section.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#defining-variables","title":"Defining variables","text":"<p>For numbers, we will usually use the type <code>int</code>. On most computers today, it is a 32-bit number, which means the number can range from -2,147,483,648 to 2,147,483,647.</p> <p>To define the variables <code>foo</code> and <code>bar</code>, we need to use the following syntax:</p> <pre><code>int foo;\nint bar = 1;\n</code></pre> <p>The variable <code>foo</code> can be used, but since we did not initialise it, we don't know what's in it. The variable <code>bar</code> contains the number 1.</p> <p>Now, we can do some maths. Assuming <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code>, and <code>e</code> are variables, we can simply use plus, minus and multiplication operators in the following notation, and assign a new value to <code>a</code>:</p> <pre><code>int a = 0, b = 1, c = 2, d = 3, e = 4;\na = b - c + d * e;\nprintf(\"%d\", a); /* will print 1-2+3*4 = 11 */\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#exercise","title":"Exercise","text":"<p>In the next exercise, you will need to create a program which prints out the sum of the numbers <code>a</code>, <code>b</code>, and <code>c</code>.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  int a = 3;\n  float b = 4.5;\n  double c = 5.25;\n  float sum;\n\n  /* Your code goes here */\n\n  printf(\"The sum of a, b, and c is %f.\", sum);\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  int a = 3;\n  float b = 4.5;\n  double c = 5.25;\n  float sum;\n\n  /* Your code goes here */\n  sum = a + b + c;\n\n  printf(\"The sum of a, b, and c is %f.\\n\", sum);\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/03%20-%20Arrays/","title":"03 - Arrays","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/03%20-%20Arrays/#arrays","title":"Arrays","text":"<p>Arrays are special variables which can hold more than one value under the same variable name, organised with an index. Arrays are defined using a very straightforward syntax:</p> <pre><code>/* defines an array of 10 integers */\nint numbers[10];\n</code></pre> <p>Accessing a number from the array is done using the same syntax. Notice that arrays in C are zero-based, which means that if we defined an array of size 10, then the array cells 0 through 9 (inclusive) are defined. <code>numbers[10]</code> is not an actual value.</p> <pre><code>int numbers[10];\n\n/* populate the array */\nnumbers[0] = 10;\nnumbers[1] = 20;\nnumbers[2] = 30;\nnumbers[3] = 40;\nnumbers[4] = 50;\nnumbers[5] = 60;\nnumbers[6] = 70;\n\n/* print the 7th number from the array, which has an index of 6 */\nprintf(\"The 7th number in the array is %d\", numbers[6]);\n</code></pre> <p>Arrays can only have one type of variable, because they are implemented as a sequence of values in the computer's memory. Because of that, accessing a specific array cell is very efficient.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/03%20-%20Arrays/#exercise","title":"Exercise","text":"<ul> <li>The code below does not compile, because the <code>grades</code> variable is missing.</li> <li>One of the grades is missing. Can you define it so the grade average will be 85?</li> </ul>"},{"location":"Languages/C/01%20-%20Fundamentals/03%20-%20Arrays/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  /* TODO: define the grades variable here */\n  int average;\n\n  grades[0] = 80;\n  /* TODO: define the missing grade so that the average will sum to 85. */\n  grades[2] = 90;\n\n  average = (grades[0] + grades[1] + grades[2]) / 3;\n  printf(\"The average of the 3 grades is: %d\", average);\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/03%20-%20Arrays/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  // When declaring arrays, need to specify the size of the array.\n  int grades[3];\n\n  // Due to no size being specified, this will just be an integer.\n  int average;\n\n  // Populate array indices with grades.\n  grades[0] = 80;\n  grades[1] = 85;\n  grades[2] = 90;\n\n  // Some calculation and display.\n  average = (grades[0] + grades[1] + grades[2]) / 3;\n  printf(\"The average of the 3 grades is: %d\\n\", average);\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/","title":"04 - Multidimensional Arrays","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/#multidimensional-arrays","title":"Multidimensional Arrays","text":"<p>In the previous tutorials on Arrays, we covered, well, arrays and how they work. The arrays we looked at were all one-dimensional, but C can create and use multi-dimensional arrays. Here is the general form of a multidimensional array declaration:</p> <pre><code>type name[size1][size2]...[sizeN];\n</code></pre> <p>For example, here's a basic one for you to look at -</p> <pre><code>int foo[1][2][3];\n</code></pre> <p>or maybe this one -</p> <pre><code>char vowels[1][5] = {\n    {'a', 'e', 'i', 'o', 'u'}\n};\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/#two-dimensional-arrays","title":"Two-dimensional Arrays","text":"<p>The simplest form of multidimensional array is the two-dimensional array. A two-dimensional array is pretty much a list of one-dimensional arrays. To declare a two-dimensional integer array of size [x] and [y], you would write something like this \u2212</p> <pre><code>type arrayName [x][y];\n</code></pre> <p>Where type can be any C data type (int, char, long, long long, double, etc.) and arrayName will be a valid C identifier, or variable. A two-dimensional array can be considered as a table which will have [ x ] number of rows and [ y ] number of columns. A two-dimensional array a, which contains three rows and four columns can be shown and thought about like this \u2212</p> <p></p> <p>In this sense, every element in the array a is identified by an element name in the form a[i][j], where 'a' is the name of the array, and 'i' and 'j' are the indexes that uniquely identify, or show, each element in 'a'.</p> <p>And honestly, you really don't have to put in a [ x ] value really, because if you did something like this -</p> <pre><code>char vowels[][5] = {\n    {'A', 'E', 'I', 'O', 'U'},\n    {'a', 'e', 'i', 'o', 'u'}\n};\n</code></pre> <p>the compiler would already know that there are two \"dimensions\" you could say, but, you NEED a [ y ] value!! The compiler may be smart, but it will not know how many integers, characters, floats, whatever you're using you have in the dimensions. Keep that in mind.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/#initialising-two-dimensional-arrays","title":"Initialising Two-Dimensional Arrays","text":"<p>Multidimensional arrays may be used by specifying bracketed[] values for each row. Below is an array with 3 rows and each row has 4 columns. To make it easier, you can forget the 3 and keep it blank, it'll still work.</p> <pre><code>int a[3][4] = {  \n   {0, 1, 2, 3} ,   /*  initializers for row indexed by 0 */\n   {4, 5, 6, 7} ,   /*  initializers for row indexed by 1 */\n   {8, 9, 10, 11}   /*  initializers for row indexed by 2 */\n};\n</code></pre> <p>The inside braces, which indicates the wanted row, are optional. The following initialisation is the same to the previous example \u2212</p> <pre><code>int a[3][4] = {0,1,2,3,4,5,6,7,8,9,10,11};\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/#accessing-two-dimensional-array-elements","title":"Accessing Two-Dimensional Array Elements","text":"<p>An element in a two-dimensional array is accessed by using the subscripts, i.e., row index and column index of the array. For example \u2212</p> <pre><code>int val = a[2][3];\n</code></pre> <p>The above statement will take the 4th element from the 3rd row of the array.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/#exercise","title":"Exercise","text":"<p>Let us try to find out the average marks of a group of five students for two subjects, Mathematics and Physics. To do this, we use a two-dimensional array called <code>grades</code>. The marks corresponding to Mathematics would be stored in the first row (<code>grades[0]</code>), whereas those corresponding to Physics would be stored in the second row (<code>grades[1]</code>). Complete the following steps so that you can execute this program.</p> <ul> <li>Declare grades as a two-dimensional array of integers</li> <li>Complete the for loops by specifying their terminating conditions</li> <li>Compute the average marks obtained in each subject</li> </ul>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    /* TODO: declare the 2D array grades here */\n    float average;\n    int i;\n    int j;\n\n    grades[0][0] = 80;\n    grades[0][1] = 70;\n    grades[0][2] = 65;\n    grades[0][3] = 89;\n    grades[0][4] = 90;\n\n    grades[1][0] = 85;\n    grades[1][1] = 80;\n    grades[1][2] = 80;\n    grades[1][3] = 82;\n    grades[1][4] = 87;\n\n    /* TODO: complete the for loop with appropriate terminating conditions */\n    for (i = 0; i &lt; ; i++) {\n        average = 0;\n        for (j = 0; j &lt; ; j++) {\n            average += grades[i][j];\n        }\n\n        /* TODO: compute the average marks for subject i */\n        printf(\"The average marks obtained in subject %d is: %.2f\\n\", i, average);\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/04%20-%20Multidimensional%20Arrays/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  // 2D array to store the grades of five students in two subjects.\n  int grades[2][5];\n\n  // Table representation of the grades array if populated:\n  // Student | Subject 1 | Subject 2\n  // --------------------------------\n  // 1       | 80        | 85\n  // 2       | 70        | 80\n  // 3       | 65        | 80\n  // 4       | 89        | 82\n  // 5       | 90        | 87\n\n  // Other variables\n  float average;\n  int i;\n  int j;\n\n  // Populate the grades array for subject 1.\n  grades[0][0] = 80;\n  grades[0][1] = 70;\n  grades[0][2] = 65;\n  grades[0][3] = 89;\n  grades[0][4] = 90;\n\n  // Populate the grades array for subject 2.\n  grades[1][0] = 85;\n  grades[1][1] = 80;\n  grades[1][2] = 80;\n  grades[1][3] = 82;\n  grades[1][4] = 87;\n\n  // Loop [0, 1]\n  for (i = 0; i &lt; 2; i++) {\n    average = 0;\n\n    // Loop [0, 1, 2, 3, 4]\n    for (j = 0; j &lt; 5; j++) {\n      average += grades[i][j];\n    }\n\n    // Calculate average\n    average = average / 5;\n    printf(\"The average marks obtained in subject %d is: %.2f\\n\", i, average);\n  }\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/05%20-%20Conditions/","title":"05 - Conditions","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/05%20-%20Conditions/#conditions","title":"Conditions","text":""},{"location":"Languages/C/01%20-%20Fundamentals/05%20-%20Conditions/#decision-making","title":"Decision Making","text":"<p>In life, we all have to make decisions. In order to make a decision we weigh out our options and so do our programs.</p> <p>Here is the general form of the decision making structures found in C.</p> <pre><code>int target = 10;\nif (target == 10) {\n    printf(\"Target is equal to 10\");\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/05%20-%20Conditions/#the-if-statement","title":"The <code>if</code> statement","text":"<p>The <code>if</code> statement allows us to check if an expression is <code>true</code> or <code>false</code>, and execute different code according to the result.</p> <p>To evaluate whether two variables are equal, the == operator is used, just like in the first example.</p> <p>Inequality operators can also be used to evaluate expressions. For example:</p> <pre><code>int foo = 1;\nint bar = 2;\n\nif (foo &lt; bar) {\n    printf(\"foo is smaller than bar.\");\n}\n\nif (foo &gt; bar) {\n    printf(\"foo is greater than bar.\");\n}\n</code></pre> <p>We can use the <code>else</code> keyword to execute code when our expression evaluates to <code>false</code>.</p> <pre><code>int foo = 1;\nint bar = 2;\n\nif (foo &lt; bar) {\n    printf(\"foo is smaller than bar.\");\n} else {\n    printf(\"foo is greater than bar.\");\n}\n</code></pre> <p>Sometimes we will have more than two outcomes to choose from. In these cases, we can \"chain\" multiple <code>if</code> <code>else</code> statements together.</p> <pre><code>int foo = 1;\nint bar = 2;\n\nif (foo &lt; bar) {\n    printf(\"foo is smaller than bar.\");\n} else if (foo == bar) {\n    printf(\"foo is equal to bar.\");\n} else {\n    printf(\"foo is greater than bar.\");\n}\n</code></pre> <p>You can also nest <code>if</code> <code>else</code> statements if you like.</p> <pre><code>int peanuts_eaten = 22;\nint peanuts_in_jar = 100;\nint max_peanut_limit = 50;\n\nif (peanuts_in_jar &gt; 80) {\n    if (peanuts_eaten &lt; max_peanut_limit) {\n        printf(\"Take as many peanuts as you want!\\n\");\n    }\n} else {\n    if (peanuts_eaten &gt; peanuts_in_jar) {\n        printf(\"You can't have anymore peanuts!\\n\");\n    }\n    else {\n        printf(\"Alright, just one more peanut.\\n\");\n    }\n}\n</code></pre> <p>Two or more expressions can be evaluated together using logical operators to check if two expressions evaluate to <code>true</code> together, or at least one of them. To check if two expressions both evaluate to <code>true</code>, use the AND operator <code>&amp;&amp;</code>. To check if at least one of the expressions evaluate to <code>true</code>, use the OR operator <code>||</code>.</p> <pre><code>int foo = 1;\nint bar = 2;\nint moo = 3;\n\nif (foo &lt; bar &amp;&amp; moo &gt; bar) {\n    printf(\"foo is smaller than bar AND moo is larger than bar.\");\n}\n\nif (foo &lt; bar || moo &gt; bar) {\n    printf(\"foo is smaller than bar OR moo is larger than bar.\");\n}\n</code></pre> <p>The NOT operator <code>!</code> can also be used likewise:</p> <pre><code>int target = 9;\nif (target != 10) {\n    printf(\"Target is not equal to 10\");\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/05%20-%20Conditions/#exercise","title":"Exercise","text":"<p>In this exercise, you must construct an <code>if</code> statement inside the <code>guessNumber</code> function statement that checks if the number <code>guess</code> is equal to 555. If that is the case, the function must print out using <code>printf</code> \"Correct. You guessed it!\". If <code>guess</code> is less than 555, the function must print out using <code>printf</code> \"Your guess is too low.\" If <code>guess</code> is greater than 555, the function must print out using <code>printf</code> \"Your guess is too high.\"</p> <ul> <li>Important: Don't forget to add a newline character <code>\\n</code> at the end of the printf string.</li> </ul>"},{"location":"Languages/C/01%20-%20Fundamentals/05%20-%20Conditions/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nvoid guessNumber(int guess) {\n    // TODO: write your code here\n}\n\nint main() {\n    guessNumber(500);\n    guessNumber(600);\n    guessNumber(555);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/05%20-%20Conditions/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nvoid guessNumber(int guess) {\n  if (guess == 555) {\n    printf(\"Correct. You guessed it!\\n\");\n  } else if (guess &gt; 555) {\n    printf(\"Your guess is too high.\\n\");\n  } else {\n    printf(\"Your guess is too low.\\n\");\n  }\n}\n\nint main() {\n    guessNumber(500);\n    guessNumber(600);\n    guessNumber(555);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/","title":"06 - Strings","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#strings","title":"Strings","text":""},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#defining-strings","title":"Defining strings","text":"<p>Strings in C are actually arrays of characters. Although using pointers in C is an advanced subject, fully explained later on, we will use pointers to a character array to define simple strings, in the following manner:</p> <pre><code>char * name = \"John Smith\";\n</code></pre> <p>This method creates a string which we can only use for reading. If we wish to define a string which can be manipulated, we will need to define it as a local character array:</p> <pre><code>char name[] = \"John Smith\";\n</code></pre> <p>This notation is different because it allocates an array variable so we can manipulate it. The empty brackets notation <code>[]</code> tells the compiler to calculate the size of the array automatically. This is in fact the same as allocating it explicitly, adding one to the length of the string:</p> <pre><code>char name[] = \"John Smith\";\n/* is the same as */\nchar name[11] = \"John Smith\";\n</code></pre> <p>The reason that we need to add one, although the string <code>John Smith</code> is exactly 10 characters long, is for the string termination: a special character (equal to 0) which indicates the end of the string. The end of the string is marked because the program does not know the length of the string - only the compiler knows it according to the code.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#string-formatting-with-printf","title":"String formatting with printf","text":"<p>We can use the <code>printf</code> command to format a string together with other strings, in the following manner:</p> <pre><code>char * name = \"John Smith\";\nint age = 27;\n\n/* prints out 'John Smith is 27 years old.' */\nprintf(\"%s is %d years old.\\n\", name, age);\n</code></pre> <p>Notice that when printing strings, we must add a newline (<code>\\n</code>) character so that our next <code>printf</code> statement will print in a new line.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#string-length","title":"String Length","text":"<p>The function 'strlen' returns the length of the string which has to be passed as an argument:</p> <pre><code>char * name = \"Nikhil\";\nprintf(\"%d\\n\",strlen(name));\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#string-comparison","title":"String comparison","text":"<p>The function <code>strncmp</code> compares between two strings, returning the number 0 if they are equal, or a different number if they are different. The arguments are the two strings to be compared, and the maximum comparison length. There is also an unsafe version of this function called <code>strcmp</code>, but it is not recommended to use it. For example:</p> <pre><code>char * name = \"John\";\n\nif (strncmp(name, \"John\", 4) == 0) {\n    printf(\"Hello, John!\\n\");\n} else {\n    printf(\"You are not John. Go away.\\n\");\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#string-concatenation","title":"String Concatenation","text":"<p>The function 'strncat' appends first n characters of src string to the destination string where n is min(n,length(src)); The arguments passed are destination string, source string, and n - maximum number of characters to be appended. For Example:</p> <pre><code>char dest[20]=\"Hello\";\nchar src[20]=\"World\";\nstrncat(dest,src,3);\nprintf(\"%s\\n\",dest);\nstrncat(dest,src,20);\nprintf(\"%s\\n\",dest);\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#exercise","title":"Exercise","text":"<p>Define the string <code>first_name</code> with the value <code>John</code> using the pointer notation, and define the string <code>last_name</code> with the value <code>Doe</code> using the local array notation.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\nint main() {\n  /* define first_name */\n  /* define last_name */\n  char name[100];\n\n  last_name[0] = 'B';\n  sprintf(name, \"%s %s\", first_name, last_name);\n  if (strncmp(name, \"John Boe\", 100) == 0) {\n      printf(\"Done!\\n\");\n  }\n  name[0]='\\0';\n  strncat(name,first_name,4);\n  strncat(name,last_name,20);\n  printf(\"%s\\n\",name);\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/06%20-%20Strings/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nint main() {\n  // Pointer annotation for strings\n  char * first_name = \"John\";\n\n  // Array annotation for strings\n  char last_name[] = \"Doe\";\n  // Same as\n  // char last_name[4] = \"Doe\"; Do not forget the extra number due to the terminator character.\n\n  // Other code ...\n  char name[100];\n\n  last_name[0] = 'B';\n  sprintf(name, \"%s %s\", first_name, last_name);\n  if (strncmp(name, \"John Boe\", 100) == 0) {\n    printf(\"Done!\\n\");\n  }\n  name[0]='\\0';\n  strncat(name,first_name,4);\n  strncat(name,last_name,20);\n  printf(\"%s\\n\",name);\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/07%20-%20For%20loops/","title":"07 - For loops","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/07%20-%20For%20loops/#for-loops","title":"For loops","text":"<p>For loops in C are straightforward. They supply the ability to create a loop - a code block that runs multiple times. For loops require an iterator variable, usually notated as <code>i</code>.</p> <p>For loops give the following functionality:</p> <ul> <li>Initialise the iterator variable using an initial value</li> <li>Check if the iterator has reached its final value</li> <li>Increases the iterator</li> </ul> <p>For example, if we wish to iterate on a block for 10 times, we write:</p> <pre><code>int i;\nfor (i = 0; i &lt; 10; i++) {\n    printf(\"%d\\n\", i);\n}\n</code></pre> <p>This block will print the numbers 0 through 9 (10 numbers in total).</p> <p>For loops can iterate on array values. For example, if we would want to sum all the values of an array, we would use the iterator <code>i</code> as the array index:</p> <pre><code>int array[10] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\nint sum = 0;\nint i;\n\nfor (i = 0; i &lt; 10; i++) {\n    sum += array[i];\n}\n\n/* sum now contains a[0] + a[1] + ... + a[9] */\nprintf(\"Sum of the array is %d\\n\", sum);\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/07%20-%20For%20loops/#exercise","title":"Exercise","text":"<p>Calculate the factorial (multiplication of all items <code>array[0]</code> to <code>array[9]</code>, inclusive), of the variable <code>array</code>.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/07%20-%20For%20loops/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  int array[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n  int factorial = 1;\n  int i;\n\n  /* calculate the factorial using a for loop  here*/\n\n  printf(\"10! is %d.\\n\", factorial);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/07%20-%20For%20loops/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  int array[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n  int factorial = 1;\n  int i;\n\n  // For loop to calculate the factorial of 10\n  for (i = 0; i &lt; 10; i++) {\n    factorial = factorial * array[i];\n    // Other ways of doing the same thing...\n    // factorial *= array[i];\n  };\n\n  printf(\"10! is %d.\\n\", factorial);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/08%20-%20While%20loops/","title":"08 - While loops","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/08%20-%20While%20loops/#while-loops","title":"While loops","text":"<p>While loops are similar to for loops, but have less functionality. A while loop continues executing the while block as long as the condition in the while remains true. For example, the following code will execute exactly ten times:</p> <pre><code>int n = 0;\nwhile (n &lt; 10) {\n    n++;\n}\n</code></pre> <p>While loops can also execute infinitely if a condition is given which always evaluates as true (non-zero):</p> <pre><code>while (1) {\n   /* do something */\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/08%20-%20While%20loops/#loop-directives","title":"Loop directives","text":"<p>There are two important loop directives that are used in conjunction with all loop types in C - the <code>break</code> and <code>continue</code> directives.</p> <p>The <code>break</code> directive halts a loop after ten loops, even though the while loop never finishes:</p> <pre><code>int n = 0;\nwhile (1) {\n    n++;\n    if (n == 10) {\n        break;\n    }\n}\n</code></pre> <p>In the following code, the <code>continue</code> directive causes the <code>printf</code> command to be skipped, so that only even numbers are printed out:</p> <pre><code>int n = 0;\nwhile (n &lt; 10) {\n    n++;\n\n    /* check that n is odd */\n    if (n % 2 == 1) {\n        /* go back to the start of the while block */\n        continue;\n    }\n\n    /* we reach this code only if n is even */\n    printf(\"The number %d is even.\\n\", n);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/08%20-%20While%20loops/#exercise","title":"Exercise","text":"<p>The <code>array</code> variable consists of a sequence of ten numbers. Inside the while loop, you must write two <code>if</code> conditions, which change the flow of the loop in the following manner (without changing the <code>printf</code> command):</p> <ul> <li>If the current number which is about to printed is less than 5, don't print it.</li> <li>If the current number which is about to printed is greater than 10, don't print it and stop the loop.</li> </ul> <p>Notice that if you do not advance the iterator variable <code>i</code> and use the <code>continue</code> derivative, you will get stuck in an infinite loop.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/08%20-%20While%20loops/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int array[] = {1, 7, 4, 5, 9, 3, 5, 11, 6, 3, 4};\n    int i = 0;\n\n    while (i &lt; 10) {\n        /* your code goes here */\n\n        printf(\"%d\\n\", array[i]);\n        i++;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/08%20-%20While%20loops/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int array[] = {1, 7, 4, 5, 9, 3, 5, 11, 6, 3, 4};\n    int i = 0;\n\n    while (i &lt; 10) {\n        if (array[i] &lt; 5) {\n          i++;\n          continue;\n        } else if (array[i] &gt; 10) {\n          break;\n        }\n\n        printf(\"%d\\n\", array[i]);\n        i++;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/09%20-%20Functions/","title":"09 - Functions","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/09%20-%20Functions/#functions","title":"Functions","text":"<p>C functions are simple, but because of how C works, the power of functions is a bit limited.</p> <ul> <li>Functions receive either a fixed or variable amount of arguments.</li> <li>Functions can only return one value, or return no value.</li> </ul> <p>In C, arguments are copied by value to functions, which means that we cannot change the arguments to affect their value outside of the function. To do that, we must use pointers, which are taught later on.</p> <p>Functions are defined using the following syntax:</p> <pre><code>int foo(int bar) {\n    /* do something */\n    return bar * 2;\n}\n\nint main() {\n    foo(1);\n}\n</code></pre> <p>The function <code>foo</code> we defined receives one argument, which is <code>bar</code>. The function receives an integer, multiplies it by two, and returns the result.</p> <p>To execute the function <code>foo</code> with 1 as the argument <code>bar</code>, we use the following syntax:</p> <pre><code>foo(1);\n</code></pre> <p>In C, functions must be first defined before they are used in the code. They can be either declared first and then implemented later on using a header file or in the beginning of the C file, or they can be implemented in the order they are used (less preferable).</p> <p>The correct way to use functions is as follows:</p> <pre><code>/* function declaration */\nint foo(int bar);\n\nint main() {\n    /* calling foo from main */\n    printf(\"The value of foo is %d\", foo(1));\n}\n\nint foo(int bar) {\n    return bar + 1;\n}\n</code></pre> <p>We can also create functions that do not return a value by using the keyword <code>void</code>:</p> <pre><code>void moo() {\n    /* do something and don't return a value */\n}\n\nint main() {\n    moo();\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/09%20-%20Functions/#exercise","title":"Exercise","text":"<p>Write a function called <code>print_big</code> which receives one argument (an integer) and prints the line <code>x is big</code> (where x is the argument) if the argument given to the function is a number bigger than 10.</p> <ul> <li>Important: Don't forget to add a newline character <code>\\n</code> at the end of the printf string.</li> </ul>"},{"location":"Languages/C/01%20-%20Fundamentals/09%20-%20Functions/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\n/* function declaration */\nvoid print_big(int number);\n\nint main() {\n  int array[] = { 1, 11, 2, 22, 3, 33 };\n  int i;\n  for (i = 0; i &lt; 6; i++) {\n    print_big(array[i]);\n  }\n  return 0;\n}\n\n/* write your function here */\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/09%20-%20Functions/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\n/* function declaration */\nvoid print_big(int number);\n\nint main() {\n  int array[] = { 1, 11, 2, 22, 3, 33 };\n  int i;\n  for (i = 0; i &lt; 6; i++) {\n    print_big(array[i]);\n  }\n  return 0;\n}\n\n/* write your function here */\nvoid print_big(int number) {\n  if (number &gt; 10) {\n    printf(\"%d is big\\n\", number);\n  }\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/","title":"10 - Static","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/#static","title":"Static","text":"<p><code>static</code> is a keyword in the C programming language. It can be used with variables and functions.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/#what-is-a-static-variable","title":"What is a static variable?","text":"<p>By default, variables are local to the scope in which they are defined. Variables can be declared as static to increase their scope up to file containing them. As a result, these variables can be accessed anywhere inside a file.</p> <p>Consider the following scenario \u2013 where we want to count the runners participating in a race:</p> <pre><code>#include&lt;stdio.h&gt;\nint runner() {\n    int count = 0;\n    count++;\n    return count;\n}\n\nint main()\n{\n    printf(\"%d \", runner());\n    printf(\"%d \", runner());\n    return 0;\n}\n</code></pre> <p>We will see that <code>count</code> is not updated because it is removed from memory as soon as the function completes. If <code>static</code> is used however, we get the desired result:</p> <pre><code>#include&lt;stdio.h&gt;\nint runner()\n{\n    static int count = 0;\n    count++;\n    return count;\n}\n\nint main()\n{\n    printf(\"%d \", runner());\n    printf(\"%d \", runner());\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/#what-is-a-static-function","title":"What is a static function?","text":"<p>By default, functions are global in C. If we declare a function with <code>static</code>, the scope of that function is reduced to the file containing it.</p> <p>The syntax looks like this:</p> <pre><code>static void fun(void) {\n   printf(\"I am a static function.\");\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/#static-vs-global","title":"Static vs Global?","text":"<p>While static variables have scope over the file containing them making them accessible only inside a given file, global variables can be accessed outside the file too.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/#exercise","title":"Exercise","text":"<p>In this exercise, try to find the sum of some numbers by using the static keyword. Do not pass any variable representing the running total to the <code>sum()</code> function.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\nint sum (int num) {\n   /**\n   * find sum to n numbers\n   */\n}\n\nint main() {\n   printf(\"%d \",sum(55));\n   printf(\"%d \",sum(45));\n   printf(\"%d \",sum(50));\n   return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/10%20-%20Static/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\nint sum (int num) {\n  static int sum = 0;\n  sum += num;\n  return sum;\n}\n\nint main() {\n  printf(\"%d \",sum(55));\n  printf(\"%d \",sum(45));\n  printf(\"%d \",sum(50));\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/","title":"11 - Pointers","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#pointers","title":"Pointers","text":"<p>Pointers are also variables and play a very important role in C programming language. They are used for several reasons, such as:</p> <ul> <li>Strings</li> <li>Dynamic memory allocation</li> <li>Sending function arguments by reference</li> <li>Building complicated data structures</li> <li>Pointing to functions</li> <li>Building special data structures (i.e. Tree, Tries, etc...)</li> </ul> <p>And many more.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#what-is-a-pointer","title":"What is a pointer?","text":"<p>A pointer is essentially a simple integer variable which holds a memory address that points to a value, instead of holding the actual value itself.</p> <p>The computer's memory is a sequential store of data, and a pointer points to a specific part of the memory. Our program can use pointers in such a way that the pointers point to a large amount of memory - depending on how much we decide to read from that point on.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#strings-as-pointers","title":"Strings as pointers","text":"<p>We've already discussed strings, but now we can dive in a bit deeper and understand what strings in C really are (which are called C-Strings to differentiate them from other strings when mixed with C++)</p> <p>The following line:</p> <pre><code>char * name = \"John\";\n</code></pre> <p>does three things:</p> <ol> <li>It allocates a local (stack) variable called <code>name</code>, which is a pointer to a single character.</li> <li>It causes the string \"John\" to appear somewhere in the program memory (after it is compiled and executed, of course).</li> <li>It initialises the <code>name</code> argument to point to where the <code>J</code> character resides at (which is followed by the rest of the string in the memory).</li> </ol> <p>If we try to access the <code>name</code> variable as an array, it will work, and will return the ordinal value of the character <code>J</code>, since the <code>name</code> variable actually points exactly to the beginning of the string.</p> <p>Since we know that the memory is sequential, we can assume that if we move ahead in the memory to the next character, we'll receive the next letter in the string, until we reach the end of the string, marked with a null terminator (the character with the ordinal value of 0, noted as <code>\\0</code>).</p>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#dereferencing","title":"Dereferencing","text":"<p>Dereferencing is the act of referring to where the pointer points, instead of the memory address. We are already using dereferencing in arrays - but we just didn't know it yet. The brackets operator - <code>[0]</code> for example, accesses the first item of the array. And since arrays are actually pointers, accessing the first item in the array is the same as dereferencing a pointer. Dereferencing a pointer is done using the asterisk operator <code>*</code>.</p> <p>If we want to create an array that will point to a different variable in our stack, we can write the following code:</p> <pre><code>/* define a local variable a */\nint a = 1;\n\n/* define a pointer variable, and point it to a using the &amp; operator */\nint * pointer_to_a = &amp;a;\n\nprintf(\"The value a is %d\\n\", a);\nprintf(\"The value of a is also %d\\n\", *pointer_to_a);\n</code></pre> <p>Notice that we used the <code>&amp;</code> operator to point at the variable <code>a</code>, which we have just created.</p> <p>We then referred to it using the dereferencing operator. We can also change the contents of the dereferenced variable:</p> <pre><code>int a = 1;\nint * pointer_to_a = &amp;a;\n\n/* let's change the variable a */\na += 1;\n\n/* we just changed the variable again! */\n*pointer_to_a += 1;\n\n/* will print out 3 */\nprintf(\"The value of a is now %d\\n\", a);\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#exercise","title":"Exercise","text":"<p>Create a pointer to the local variable <code>n</code> called <code>pointer_to_n</code>, and use it to increase the value of <code>n</code> by one.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  int n = 10;\n\n  /* your code goes here */\n\n  /* testing code */\n  if (pointer_to_n != &amp;n) return 1;\n  if (*pointer_to_n != 11) return 1;\n\n  printf(\"Done!\\n\");\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  int n = 10;\n\n  /* your code goes here */\n  int * pointer_to_n = &amp;n;\n  *pointer_to_n += 1;\n\n  /* testing code */\n  if (pointer_to_n != &amp;n) return 1;\n  if (*pointer_to_n != 11) return 1;\n\n  printf(\"Done!\\n\");\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#pointers-can-do-the-exact-same-job","title":"Pointers - Can do the exact same job","text":""},{"location":"Languages/C/01%20-%20Fundamentals/11%20-%20Pointers/#pointers-modify-original-or-dont","title":"Pointers - Modify original or don't?","text":""},{"location":"Languages/C/01%20-%20Fundamentals/12%20-%20Structures/","title":"12 - Structures","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/12%20-%20Structures/#structures","title":"Structures","text":"<p>C structures are special, large variables which contain several named variables inside. Structures are the basic foundation for objects and classes in C. Structures are used for:</p> <ul> <li>Serialisation of data</li> <li>Passing multiple arguments in and out of functions through a single argument</li> <li>Data structures such as linked lists, binary trees, and more</li> </ul> <p>The most basic example of structures are points, which are a single entity that contains two variables - <code>x</code> and <code>y</code>. Let's define a point:</p> <pre><code>struct point {\n    int x;\n    int y;\n};\n</code></pre> <p>Now, let's define a new point, and use it. Assume the function <code>draw</code> receives a point and draws it on a screen. Without structs, using it would require two arguments - each for every coordinate:</p> <pre><code>/* draws a point at 10, 5 */\nint x = 10;\nint y = 5;\ndraw(x, y);\n</code></pre> <p>Using structs, we can pass a point argument:</p> <pre><code>/* draws a point at 10, 5 */\nstruct point p;\np.x = 10;\np.y = 5;\ndraw(p);\n</code></pre> <p>To access the point's variables, we use the dot <code>.</code> operator.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/12%20-%20Structures/#typedefs","title":"Typedefs","text":"<p>Typedefs allow us to define types with a different name - which can come in handy when dealing with structs and pointers. In this case, we'd want to get rid of the long definition of a point structure. We can use the following syntax to remove the <code>struct</code> keyword from each time we want to define a new point:</p> <pre><code>typedef struct {\n    int x;\n    int y;\n} point;\n</code></pre> <p>This will allow us to define a new point like this:</p> <pre><code>point p;\n</code></pre> <p>Structures can also hold pointers - which allows them to hold strings, or pointers to other structures as well - which is their real power. For example, we can define a vehicle structure in the following manner:</p> <pre><code>typedef struct {\n    char * brand;\n    int model;\n} vehicle;\n</code></pre> <p>Since brand is a char pointer, the vehicle type can contain a string (which, in this case, indicates the brand of the vehicle).</p> <pre><code>vehicle mycar;\nmycar.brand = \"Ford\";\nmycar.model = 2007;\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/12%20-%20Structures/#exercise","title":"Exercise","text":"<p>Define a new data structure, named \"person\", which contains a string (pointer to char) called <code>name</code>, and an integer called <code>age</code>.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/12%20-%20Structures/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\n/* define the person struct here using the typedef syntax */\n\nint main() {\n    person john;\n\n    /* testing code */\n    john.name = \"John\";\n    john.age = 27;\n    printf(\"%s is %d years old.\", john.name, john.age);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/12%20-%20Structures/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\n/* define the person struct here using the typedef syntax */\ntypedef struct {\n  char * name;\n  int age;\n} person;\n\nint main() {\n    person john;\n\n    /* testing code */\n    john.name = \"John\";\n    john.age = 27;\n    printf(\"%s is %d years old.\", john.name, john.age);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/13%20-%20Function%20arguments%20by%20reference/","title":"13 - Function arguments by reference","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/13%20-%20Function%20arguments%20by%20reference/#function-arguments-by-reference","title":"Function arguments by reference","text":"<p>Assuming you now understand pointers and functions, you are aware that function arguments are passed by value, by which means they are copied in and out of functions. But what if we pass pointers to values instead of the values themselves? This will allow us to give functions control over the variables and structures of the parent functions and not just a copy of them, thus directly reading and writing the original object.</p> <p>Let's say we want to write a function which increments a number by one, called <code>addone</code>. This will not work:</p> <pre><code>void addone(int n) {\n    // n is local variable which only exists within the function scope\n    n++; // therefore incrementing it has no effect\n}\n\nint n;\nprintf(\"Before: %d\\n\", n);\naddone(n);\nprintf(\"After: %d\\n\", n);\n</code></pre> <p>However, this will work:</p> <pre><code>void addone(int *n) {\n    // n is a pointer here which point to a memory-adress outside the function scope\n    (*n)++; // this will effectively increment the value of n\n}\n\nint n;\nprintf(\"Before: %d\\n\", n);\naddone(&amp;n);\nprintf(\"After: %d\\n\", n);\n</code></pre> <p>The difference is that the second version of <code>addone</code> receives a pointer to the variable <code>n</code> as an argument, and then it can manipulate it, because it knows where it is in the memory.</p> <p>Notice that when calling the <code>addone</code> function, we must pass a reference to the variable <code>n</code>, and not the variable itself - this is done so that the function knows the address of the variable, and won't just receive a copy of the variable itself.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/13%20-%20Function%20arguments%20by%20reference/#pointers-to-structures","title":"Pointers to structures","text":"<p>Let's say we want to create a function which moves a point forward in both <code>x</code> and <code>y</code> directions, called <code>move</code>. Instead of sending two pointers, we can now send only one pointer to the function of the point structure:</p> <pre><code>void move(point * p) {\n    (*p).x++;\n    (*p).y++;\n}\n</code></pre> <p>However, if we wish to dereference a structure and access one of it's internal members, we have a shorthand syntax for that, because this operation is widely used in data structures. We can rewrite this function using the following syntax:</p> <pre><code>void move(point * p) {\n    p-&gt;x++;\n    p-&gt;y++;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/13%20-%20Function%20arguments%20by%20reference/#exercise","title":"Exercise","text":"<p>Write a function called <code>birthday</code>, which adds one to the <code>age</code> of a <code>person</code>.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/13%20-%20Function%20arguments%20by%20reference/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\ntypedef struct {\n  char * name;\n  int age;\n} person;\n\n/* function declaration */\nvoid birthday(person * p);\n\n/* write your function here */\n\nint main() {\n  person john;\n  john.name = \"John\";\n  john.age = 27;\n\n  printf(\"%s is %d years old.\\n\", john.name, john.age);\n  birthday(&amp;john);\n  printf(\"Happy birthday! %s is now %d years old.\\n\", john.name, john.age);\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/13%20-%20Function%20arguments%20by%20reference/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\ntypedef struct {\n  char * name;\n  int age;\n} person;\n\n/* function declaration */\nvoid birthday(person * p);\n\n/* write your function here */\nvoid birthday(person * p) {\n  p -&gt; age++;\n}\n\nint main() {\n  person john;\n  john.name = \"John\";\n  john.age = 27;\n\n  printf(\"%s is %d years old.\\n\", john.name, john.age);\n  birthday(&amp;john);\n  printf(\"Happy birthday! %s is now %d years old.\\n\", john.name, john.age);\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/14%20-%20Dynamic%20allocation/","title":"14 - Dynamic allocation","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/14%20-%20Dynamic%20allocation/#dynamic-allocation","title":"Dynamic allocation","text":"<p>Dynamic allocation of memory is a very important subject in C. It allows building complex data structures such as linked lists. Allocating memory dynamically helps us to store data without initially knowing the size of the data in the time we wrote the program.</p> <p>To allocate a chunk of memory dynamically, we need to have a pointer ready to store the location of the newly allocated memory. We can access memory that was allocated to us using that same pointer, and we can use that pointer to free the memory again, once we have finished using it.</p> <p>Let's assume we want to dynamically allocate a person structure. The person is defined like this:</p> <pre><code>typedef struct {\n    char * name;\n    int age;\n} person;\n</code></pre> <p>To allocate a new person in the <code>myperson</code> argument, we use the following syntax:</p> <pre><code>person * myperson = (person *) malloc(sizeof(person));\n</code></pre> <p>This tells the compiler that we want to dynamically allocate just enough to hold a person struct in memory and then return a pointer of type <code>person</code> to the newly allocated data. The memory allocation function <code>malloc()</code> reserves the specified memory space. In this case, this is the size of <code>person</code> in bytes.</p> <p>The reason we write <code>(person *)</code> before the call to <code>malloc()</code> is that <code>malloc()</code> returns a \"void pointer,\" which is a pointer that doesn't have a type. Writing <code>(person *)</code> in front of it is called typecasting, and changes the type of the pointer returned from <code>malloc()</code> to be <code>person</code>. However, it isn't strictly necessary to write it like this as C will implicitly convert the type of the returned pointer to that of the pointer it is assigned to (in this case, <code>myperson</code>) if you don't typecast it.</p> <p>Note that <code>sizeof</code> is not an actual function, because the compiler interprets it and translates it to the actual memory size of the person struct.</p> <p>To access the person's members, we can use the <code>-&gt;</code> notation:</p> <pre><code>myperson-&gt;name = \"John\";\nmyperson-&gt;age = 27;\n</code></pre> <p>After we are done using the dynamically allocated struct, we can release it using <code>free</code>:</p> <pre><code>free(myperson);\n</code></pre> <p>Note that the free does not delete the <code>myperson</code> variable itself, it simply releases the data that it points to. The <code>myperson</code> variable will still point to somewhere in the memory - but after calling <code>myperson</code> we are not allowed to access that area anymore. We must not use that pointer again until we allocate new data using it.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/14%20-%20Dynamic%20allocation/#exercise","title":"Exercise","text":"<p>Use <code>malloc</code> to dynamically allocate a point structure.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/14%20-%20Dynamic%20allocation/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n  int x;\n  int y;\n} point;\n\nint main() {\n  point * mypoint = NULL;\n\n  /* Dynamically allocate a new point\n     struct which mypoint points to here */\n\n  mypoint-&gt;x = 10;\n  mypoint-&gt;y =5 ;\n  printf(\"mypoint coordinates: %d, %d\\n\", mypoint-&gt;x, mypoint-&gt;y);\n\n  free(mypoint);\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/14%20-%20Dynamic%20allocation/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n  int x;\n  int y;\n} point;\n\nint main() {\n  point * mypoint = NULL;\n\n  /* Dynamically allocate a new point struct which mypoint points to here */\n  mypoint = (point *) malloc(sizeof(point));\n\n  mypoint-&gt;x = 10;\n  mypoint-&gt;y =5 ;\n  printf(\"mypoint coordinates: %d, %d\\n\", mypoint-&gt;x, mypoint-&gt;y);\n\n  free(mypoint);\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/15%20-%20Arrays%20and%20pointers/","title":"15 - Arrays and pointers","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/15%20-%20Arrays%20and%20pointers/#arrays-and-pointers","title":"Arrays and Pointers","text":"<p>In a previous tutorial on Pointers, you learned that a pointer to a given data type can store the address of any variable of that particular data type. For example, in the following code, the pointer variable <code>pc</code> stores the address of the character variable <code>c</code>.</p> <pre><code>char c = 'A';\nchar *pc = &amp;c;\n</code></pre> <p>Here, <code>c</code> is a scalar variable that can store only a single value. However, you are already familiar with arrays that can hold multiple values of the same data type in a contiguously allocated memory block. So, you might wonder, can we have pointers to arrays too? Indeed, we can.</p> <p>Let us start with an example code and look at its output. We will discuss its behaviour subsequently.</p> <pre><code>char vowels[] = {'A', 'E', 'I', 'O', 'U'};\nchar *pvowels = vowels;\nint i;\n\n// Print the addresses\nfor (i = 0; i &lt; 5; i++) {\n    printf(\"&amp;vowels[%d]: %p, pvowels + %d: %p, vowels + %d: %p\\n\", i, &amp;vowels[i], i, pvowels + i, i, vowels + i);\n}\n\n// Print the values\nfor (i = 0; i &lt; 5; i++) {\n    printf(\"vowels[%d]: %c, *(pvowels + %d): %c, *(vowels + %d): %c\\n\", i, vowels[i], i, *(pvowels + i), i, *(vowels + i));\n}\n</code></pre> <p>A typical output of the above code is shown below.</p> <p>&amp;vowels[0]: 0x7ffee146da17, pvowels + 0: 0x7ffee146da17, vowels + 0: 0x7ffee146da17</p> <p>&amp;vowels[1]: 0x7ffee146da18, pvowels + 1: 0x7ffee146da18, vowels + 1: 0x7ffee146da18</p> <p>&amp;vowels[2]: 0x7ffee146da19, pvowels + 2: 0x7ffee146da19, vowels + 2: 0x7ffee146da19</p> <p>&amp;vowels[3]: 0x7ffee146da1a, pvowels + 3: 0x7ffee146da1a, vowels + 3: 0x7ffee146da1a</p> <p>&amp;vowels[4]: 0x7ffee146da1b, pvowels + 4: 0x7ffee146da1b, vowels + 4: 0x7ffee146da1b</p> <p>vowels[0]: A, (pvowels + 0): A, (vowels + 0): A</p> <p>vowels[1]: E, (pvowels + 1): E, (vowels + 1): E</p> <p>vowels[2]: I, (pvowels + 2): I, (vowels + 2): I</p> <p>vowels[3]: O, (pvowels + 3): O, (vowels + 3): O</p> <p>vowels[4]: U, (pvowels + 4): U, (vowels + 4): U</p> <p>As you rightly guessed, <code>&amp;vowels[i]</code> gives the memory location of the _i_th element of the array <code>vowels</code>. Moreover, since this is a character array, each element occupies one byte so that the consecutive memory addresses are separated by a single byte. We also created a pointer, <code>pvowels</code>, and assigned the address of the array <code>vowels</code> to it. <code>pvowels + i</code> is a valid operation; although in general, this may not always be meaningful (explored further in Pointer Arithmetics ). In particular, the output shown above indicates that <code>&amp;vowels[i]</code> and <code>pvowels + i</code> are equivalent. Feel free to alter the data types of the array and pointer variables to test this out.</p> <p>If you look carefully at the previous code, you will notice that we also used another apparently surprising notation: <code>vowels + i</code>. Moreover, <code>pvowels + i</code> and <code>vowels + i</code> returns the same thing \u2014 address of the _i_th element of the array <code>vowels</code>. On the other hand, <code>*(pvowels + i)</code> and <code>*(vowels + i)</code> both return the _i_th element of the array <code>vowels</code>. Why is that so?</p> <p>This is because the name of an array itself is a (constant) pointer to the first element of the array. In other words, the notations <code>vowels</code>, <code>&amp;vowels[0]</code>, and <code>vowels + 0</code> all point to the same location.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/15%20-%20Arrays%20and%20pointers/#dynamic-memory-allocation-for-arrays","title":"Dynamic Memory Allocation for Arrays","text":"<p>By now we know that we can traverse an array using pointers. Moreover, we also know that we can dynamically allocate (contiguous) memory using blocks pointers. These two aspects can be combined to dynamically allocate memory for an array. This is illustrated in the following code.</p> <pre><code>// Allocate memory to store five characters\nint n = 5;\nchar *pvowels = (char *) malloc(n * sizeof(char));\nint i;\n\npvowels[0] = 'A';\npvowels[1] = 'E';\n*(pvowels + 2) = 'I';\npvowels[3] = 'O';\n*(pvowels + 4) = 'U';\n\nfor (i = 0; i &lt; n; i++) {\n    printf(\"%c \", pvowels[i]);\n}\n\nprintf(\"\\n\");\n\nfree(pvowels);\n</code></pre> <p>In the above code, we allocated five contiguous bytes of memory to store five characters. Subsequently, we used array notations to traverse the blocks of memory as if <code>pvowels</code> is an array. However, remember that <code>pvowels</code> actually is a pointer. Pointers and arrays, in general, are not the same thing.</p> <p>So when is this useful? Remember that while declaring an array, the number of elements that it would contain must be known beforehand. Therefore, in some scenarios it might happen that the space allocated for an array is either less than the desired space or more. However, by using dynamic memory allocation, one can allocate just as much memory as required by a program. Moreover, unused memory can be freed as soon as it is no longer required by invoking the <code>free()</code> function. On the down side, with dynamic memory allocation, one must responsibly call <code>free()</code> wherever relevant. Otherwise, memory leaks would occur.</p> <p>We conclude this tutorial by looking at dynamic memory allocation for a two-dimensional array. This can be generalised to n-dimensions in a similar way. Unlike one-dimensional arrays, where we used a pointer, in this case we require a pointer to a pointer, as shown below.</p> <pre><code>int nrows = 2;\nint ncols = 5;\nint i, j;\n\n// Allocate memory for nrows pointers\nchar **pvowels = (char **) malloc(nrows * sizeof(char *));\n\n// For each row, allocate memory for ncols elements\npvowels[0] = (char *) malloc(ncols * sizeof(char));\npvowels[1] = (char *) malloc(ncols * sizeof(char));\n\npvowels[0][0] = 'A';\npvowels[0][1] = 'E';\npvowels[0][2] = 'I';\npvowels[0][3] = 'O';\npvowels[0][4] = 'U';\n\npvowels[1][0] = 'a';\npvowels[1][1] = 'e';\npvowels[1][2] = 'i';\npvowels[1][3] = 'o';\npvowels[1][4] = 'u';\n\nfor (i = 0; i &lt; nrows; i++) {\n    for(j = 0; j &lt; ncols; j++) {\n        printf(\"%c \", pvowels[i][j]);\n    }\n\n    printf(\"\\n\");\n}\n\n// Free individual rows\nfree(pvowels[0]);\nfree(pvowels[1]);\n\n// Free the top-level pointer\nfree(pvowels);\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/15%20-%20Arrays%20and%20pointers/#exercise","title":"Exercise","text":"<p>The first seven rows of Pascal's triangle are shown below. Note that row i contains i elements. Therefore, to store the numbers from the first three rows, one would require 1 + 2 + 3 = 6 memory slots.</p> <p>1</p> <p>1 1</p> <p>1 2 1</p> <p>1 3 3 1</p> <p>1 4 6 4 1</p> <p>1 5 10 10 5 1</p> <p>1 6 15 20 15 6 1</p> <p>Complete the skeleton code given below to store the numbers from the first three rows of Pascal's triangle in a two-dimensional \"array\" using dynamic memory allocation. Note that you must allocate exactly six memory slots to store those six numbers. No extra memory should be allocated. At the end of your program, free all the memory blocks used in this program.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/15%20-%20Arrays%20and%20pointers/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n    int i, j;\n    /* TODO: define the 2D pointer variable here */\n\n    /* TODO: complete the following line to allocate memory for holding three rows */\n    pnumbers = (int **) malloc();\n\n    /* TODO: allocate memory for storing the individual elements in a row */\n    pnumbers[0] = (int *) malloc(1 * sizeof(int));\n\n    pnumbers[0][0] = 1;\n    pnumbers[1][0] = 1;\n    pnumbers[1][1] = 1;\n    pnumbers[2][0] = 1;\n    pnumbers[2][1] = 2;\n    pnumbers[2][2] = 1;\n\n    for (i = 0; i &lt; 3; i++) {\n        for (j = 0; j &lt;= i; j++) {\n            printf(\"%d\", pnumbers[i][j]);\n        }\n        printf(\"\\n\");\n    }\n\n    for (i = 0; i &lt; 3; i++) {\n        /* TODO: free memory allocated for each row */\n    }\n\n    /* TODO: free the top-level pointer */\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/15%20-%20Arrays%20and%20pointers/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n    int i, j;\n\n    // 2D Pointer Initialization\n    int ** pnumbers;\n\n    // Allocate memory for three int pointer rows\n    pnumbers = (int **) malloc(3 * sizeof(int *));\n\n    // Allocate memory for each row\n    // ROW 0 -&gt; SIZE 1\n    // ROW 1 -&gt; SIZE 2\n    // ROW 2 -&gt; SIZE 3\n    pnumbers[0] = (int *) malloc(1 * sizeof(int));\n    pnumbers[1] = (int *) malloc(2 * sizeof(int));\n    pnumbers[2] = (int *) malloc(3 * sizeof(int));\n\n    pnumbers[0][0] = 1;\n    pnumbers[1][0] = 1;\n    pnumbers[1][1] = 1;\n    pnumbers[2][0] = 1;\n    pnumbers[2][1] = 2;\n    pnumbers[2][2] = 1;\n\n    for (i = 0; i &lt; 3; i++) {\n        for (j = 0; j &lt;= i; j++) {\n            printf(\"%d\", pnumbers[i][j]);\n        }\n        printf(\"\\n\");\n    }\n\n    for (i = 0; i &lt; 3; i++) {\n      // Free memory allocated for each row\n      free(pnumbers[i]);\n    }\n\n    // Free table/top-level pointer\n    free(pnumbers);\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/16%20-%20Recursion/","title":"16 - Recursion","text":"<p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/16%20-%20Recursion/#recursion","title":"Recursion","text":"<p>Recursion occurs when a function contains within it a call to itself. Recursion can result in very neat, elegant code that is intuitive to follow. It can also result in a very large amount of memory being used if the recursion gets too deep.</p> <p>Common examples of where recursion is used :</p> <ul> <li>Walking recursive data structures such as linked lists, binary trees, etc.</li> <li>Exploring possible scenarios in games such as chess</li> </ul> <p>Recursion always consists of two main parts. A terminating case that indicates when the recursion will finish and a call to itself that must make progress towards the terminating case.</p> <p>For example, this function will perform multiplication by recursively adding :</p> <pre><code>#include &lt;stdio.h&gt;\n\nunsigned int multiply(unsigned int x, unsigned int y)\n{\n    if (x == 1)\n    {\n        /* Terminating case */\n        return y;\n    }\n    else if (x &gt; 1)\n    {\n        /* Recursive step */\n        return y + multiply(x-1, y);\n    }\n\n    /* Catch scenario when x is zero */\n    return 0;\n}\n\nint main() {\n    printf(\"3 times 5 is %d\", multiply(3, 5));\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/16%20-%20Recursion/#exercise","title":"Exercise","text":"<p>Define a new function called <code>factorial()</code> that will compute the factorial by recursive multiplication (5! = 5 x 4 x 3 x 2 x 1). Note that by convention, the factorial of 0 is equal to 1 (0! = 1).</p>"},{"location":"Languages/C/01%20-%20Fundamentals/16%20-%20Recursion/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    /* testing code */\n    printf(\"0! = %i\\n\", factorial(0));\n    printf(\"1! = %i\\n\", factorial(1));\n    printf(\"3! = %i\\n\", factorial(3));\n    printf(\"5! = %i\\n\", factorial(5));\n}\n\n/* define your function here (don't forget to declare it) */\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/16%20-%20Recursion/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint factorial(int n);\n\nint main() {\n  /* testing code */\n  printf(\"0! = %i\\n\", factorial(0));\n  printf(\"1! = %i\\n\", factorial(1));\n  printf(\"3! = %i\\n\", factorial(3));\n  printf(\"5! = %i\\n\", factorial(5));\n}\n\nint factorial(int n) {\n  if (n == 0) {\n    return 1;\n  } else {\n    return n * factorial(n - 1);\n  }\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/","title":"17 - Linked Lists","text":"<p>Related issues: - Learning C - Content - Learning C - Exercise</p> <p>Related PRs: - Learning C - Content - PR - Learning C - Exercise - PR</p>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#linked-lists","title":"Linked lists","text":""},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#introduction","title":"Introduction","text":"<p>Linked lists are the best and simplest example of a dynamic data structure that uses pointers for its implementation. However, understanding pointers is crucial to understanding how linked lists work, so if you've skipped the pointers tutorial, you should go back and redo it. You must also be familiar with dynamic memory allocation and structures.</p> <p>Essentially, linked lists function as an array that can grow and shrink as needed, from any point in the array.</p> <p>Linked lists have a few advantages over arrays:</p> <ol> <li>Items can be added or removed from the middle of the list</li> <li>There is no need to define an initial size</li> </ol> <p>However, linked lists also have a few disadvantages:</p> <ol> <li>There is no \"random\" access - it is impossible to reach the nth item in the array without first iterating over all items up until that item. This means we have to start from the beginning of the list and count how many times we advance in the list until we get to the desired item.</li> <li>Dynamic memory allocation and pointers are required, which complicates the code and increases the risk of memory leaks and segment faults.</li> <li>Linked lists have a much larger overhead over arrays, since linked list items are dynamically allocated (which is less efficient in memory usage) and each item in the list also must store an additional pointer.</li> </ol>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#what-is-a-linked-list","title":"What is a linked list?","text":"<p>A linked list is a set of dynamically allocated nodes, arranged in such a way that each node contains one value and one pointer. The pointer always points to the next member of the list. If the pointer is NULL, then it is the last node in the list.</p> <p>A linked list is held using a local pointer variable which points to the first item of the list. If that pointer is also NULL, then the list is considered to be empty.</p> <pre><code>    ------------------------------              ------------------------------\n    |              |             |            \\ |              |             |\n    |     DATA     |     NEXT    |--------------|     DATA     |     NEXT    |\n    |              |             |            / |              |             |\n    ------------------------------              ------------------------------\n</code></pre> <p>Let's define a linked list node:</p> <pre><code>typedef struct node {\n    int val;\n    struct node * next;\n} node_t;\n</code></pre> <p>Notice that we are defining the struct in a recursive manner, which is possible in C. Let's name our node type <code>node_t</code>.</p> <p>Now we can use the nodes. Let's create a local variable which points to the first item of the list (called <code>head</code>).</p> <pre><code>node_t * head = NULL;\nhead = (node_t *) malloc(sizeof(node_t));\nif (head == NULL) {\n    return 1;\n}\n\nhead-&gt;val = 1;\nhead-&gt;next = NULL;\n</code></pre> <p>We've just created the first variable in the list. We must set the value, and the next item to be empty, if we want to finish populating the list. Notice that we should always check if malloc returned a NULL value or not.</p> <p>To add a variable to the end of the list, we can just continue advancing to the next pointer:</p> <pre><code>node_t * head = NULL;\nhead = (node_t *) malloc(sizeof(node_t));\nhead-&gt;val = 1;\nhead-&gt;next = (node_t *) malloc(sizeof(node_t));\nhead-&gt;next-&gt;val = 2;\nhead-&gt;next-&gt;next = NULL;\n</code></pre> <p>This can go on and on, but what we should actually do is advance to the last item of the list, until the <code>next</code> variable will be <code>NULL</code>.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#iterating-over-a-list","title":"Iterating over a list","text":"<p>Let's build a function that prints out all the items of a list. To do this, we need to use a <code>current</code> pointer that will keep track of the node we are currently printing. After printing the value of the node, we set the <code>current</code> pointer to the next node, and print again, until we've reached the end of the list (the next node is NULL).</p> <pre><code>void print_list(node_t * head) {\n    node_t * current = head;\n\n    while (current != NULL) {\n        printf(\"%d\\n\", current-&gt;val);\n        current = current-&gt;next;\n    }\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#adding-an-item-to-the-end-of-the-list","title":"Adding an item to the end of the list","text":"<p>To iterate over all the members of the linked list, we use a pointer called <code>current</code>. We set it to start from the head and then in each step, we advance the pointer to the next item in the list, until we reach the last item.</p> <pre><code>void push(node_t * head, int val) {\n    node_t * current = head;\n    while (current-&gt;next != NULL) {\n        current = current-&gt;next;\n    }\n\n    /* now we can add a new variable */\n    current-&gt;next = (node_t *) malloc(sizeof(node_t));\n    current-&gt;next-&gt;val = val;\n    current-&gt;next-&gt;next = NULL;\n}\n</code></pre> <p>The best use cases for linked lists are stacks and queues, which we will now implement:</p>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#adding-an-item-to-the-beginning-of-the-list-pushing-to-the-list","title":"Adding an item to the beginning of the list (pushing to the list)","text":"<p>To add to the beginning of the list, we will need to do the following:</p> <ol> <li>Create a new item and set its value</li> <li>Link the new item to point to the head of the list</li> <li>Set the head of the list to be our new item</li> </ol> <p>This will effectively create a new head to the list with a new value, and keep the rest of the list linked to it.</p> <p>Since we use a function to do this operation, we want to be able to modify the head variable. To do this, we must pass a pointer to the pointer variable (a double pointer) so we will be able to modify the pointer itself.</p> <pre><code>void push(node_t ** head, int val) {\n    node_t * new_node;\n    new_node = (node_t *) malloc(sizeof(node_t));\n\n    new_node-&gt;val = val;\n    new_node-&gt;next = *head;\n    *head = new_node;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#removing-the-first-item-popping-from-the-list","title":"Removing the first item (popping from the list)","text":"<p>To pop a variable, we will need to reverse this action:</p> <ol> <li>Take the next item that the head points to and save it</li> <li>Free the head item</li> <li>Set the head to be the next item that we've stored on the side</li> </ol> <p>Here is the code:</p> <pre><code>int pop(node_t ** head) {\n    int retval = -1;\n    node_t * next_node = NULL;\n\n    if (*head == NULL) {\n        return -1;\n    }\n\n    next_node = (*head)-&gt;next;\n    retval = (*head)-&gt;val;\n    free(*head);\n    *head = next_node;\n\n    return retval;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#removing-the-last-item-of-the-list","title":"Removing the last item of the list","text":"<p>Removing the last item from a list is very similar to adding it to the end of the list, but with one big exception - since we have to change one item before the last item, we actually have to look two items ahead and see if the next item is the last one in the list:</p> <pre><code>int remove_last(node_t * head) {\n    int retval = 0;\n    /* if there is only one item in the list, remove it */\n    if (head-&gt;next == NULL) {\n        retval = head-&gt;val;\n        free(head);\n        return retval;\n    }\n\n    /* get to the second to last node in the list */\n    node_t * current = head;\n    while (current-&gt;next-&gt;next != NULL) {\n        current = current-&gt;next;\n    }\n\n    /* now current points to the second to last item of the list, so let's remove current-&gt;next */\n    retval = current-&gt;next-&gt;val;\n    free(current-&gt;next);\n    current-&gt;next = NULL;\n    return retval;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#removing-a-specific-item","title":"Removing a specific item","text":"<p>To remove a specific item from the list, either by its index from the beginning of the list or by its value, we will need to go over all the items, continuously looking ahead to find out if we've reached the node before the item we wish to remove. This is because we need to change the location to where the previous node points to as well.</p> <p>Here is the algorithm:</p> <ol> <li>Iterate to the node before the node we wish to delete</li> <li>Save the node we wish to delete in a temporary pointer</li> <li>Set the previous node's next pointer to point to the node after the node we wish to delete</li> <li>Delete the node using the temporary pointer</li> </ol> <p>There are a few edge cases we need to take care of, so make sure you understand the code.</p> <pre><code>int remove_by_index(node_t ** head, int n) {\n    int i = 0;\n    int retval = -1;\n    node_t * current = *head;\n    node_t * temp_node = NULL;\n\n    if (n == 0) {\n        return pop(head);\n    }\n\n    for (i = 0; i &lt; n-1; i++) {\n        if (current-&gt;next == NULL) {\n            return -1;\n        }\n        current = current-&gt;next;\n    }\n\n    if (current-&gt;next == NULL) {\n        return -1;\n    }\n\n    temp_node = current-&gt;next;\n    retval = temp_node-&gt;val;\n    current-&gt;next = temp_node-&gt;next;\n    free(temp_node);\n\n    return retval;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#complete-example-from-above-content","title":"Complete example from above content","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// Structs\ntypedef struct node {\n  int val;\n  struct node * next;\n} node_t;\n\n// Function declarations\nvoid print_list(node_t * head);\nvoid push_end(node_t * head, int val);\nvoid push(node_t ** head, int val);\nint pop(node_t ** head);\nint remove_last(node_t * head);\nint remove_by_index(node_t ** head, int n);\n\n// Entrypoint\nint main() {\n  // create head and allocate memory for it.\n  node_t * head = NULL;\n  head = (node_t *) malloc(sizeof(node_t));\n\n  // If it is still NULL, exit with code 1.\n  if (head == NULL) {\n    return 1;\n  }\n\n  // Assign head intial value.\n  head-&gt;val = 1;\n  head-&gt;next = NULL;\n\n  // Add more items to linked list\n  // START\n  head-&gt;val = 1;\n  head-&gt;next = (node_t *) malloc(sizeof(node_t));\n\n  // Check that allocation did not fail...\n  if (head-&gt;next == NULL) {\n    return 1;\n  }\n\n  head-&gt;next-&gt;val = 2;\n  head-&gt;next-&gt;next = NULL;\n  // END\n\n  // Print list after initialization\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Push new node to the end of the linked list\n  push_end(head, 3);\n\n  // Print list after pushing to it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Push new node to start of the the linked list\n  push(&amp;head, 4);\n\n  // Print list after pushing to it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Pop the first item from the linked list.\n  pop(&amp;head);\n\n  // Print list after popping first item from it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Pop the last item from the linked list.\n  remove_last(head);\n\n  // Print list after popping last item from it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Repopulate with some values\n  push_end(head, 3);\n  push_end(head, 4);\n  push_end(head, 5);\n\n  // Print list after populating it.\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Remove by index.\n  remove_by_index(&amp;head, 2);\n\n  // Print list after removing the 2nd index value.\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Return exit code 0\n  return 0;\n}\n\n// Print the list of nodes\nvoid print_list(node_t * head) {\n  node_t * current = head;\n\n  while (current != NULL) {\n    printf(\"%d\\n\", current-&gt;val);\n    current = current-&gt;next;\n  }\n}\n\n// Add to end of linked list\nvoid push_end(node_t * head, int val) {\n  node_t * current = head;\n  while (current-&gt;next != NULL) {\n    current = current-&gt;next;\n  }\n\n  /* now we can add a new variable */\n  current-&gt;next = (node_t *) malloc(sizeof(node_t));\n  current-&gt;next-&gt;val = val;\n  current-&gt;next-&gt;next = NULL;\n}\n\n// Add to start of linked list\nvoid push(node_t ** head, int val) {\n  node_t * new_node;\n  new_node = (node_t *) malloc(sizeof(node_t));\n\n  new_node-&gt;val = val;\n  new_node-&gt;next = *head;\n  *head = new_node;\n}\n\n// Remove the item at the start of the linked list\nint pop(node_t ** head) {\n  int retval = -1;\n  node_t * next_node = NULL;\n\n  if (*head == NULL) {\n    return retval;\n  }\n\n  next_node = (*head)-&gt;next;\n  retval = (*head)-&gt;val;\n  free(*head);\n  *head = next_node;\n\n  return retval;\n}\n\n// Remove the last item from the linked list\nint remove_last(node_t * head) {\n  int retval = 0;\n\n  /* if there is only one item in the list, remove it */\n  if (head-&gt;next == NULL) {\n    retval = head-&gt;val;\n    free(head);\n    return retval;\n  }\n\n  /* get to the second to last node in the list */\n  node_t * current = head;\n  while (current-&gt;next-&gt;next != NULL) {\n    current = current-&gt;next;\n  }\n\n  /* now current points to the second to last item of the list, so let's remove current-&gt;next */\n  retval = current-&gt;next-&gt;val;\n  free(current-&gt;next);\n  current-&gt;next = NULL;\n  return retval;\n}\n\nint remove_by_index(node_t ** head, int n) {\n  int i = 0;\n  int retval = -1;\n  node_t * current = *head;\n  node_t * temp_node = NULL;\n\n  if (n == 0) {\n    return pop(head);\n  }\n\n  for (i = 0; i &lt; n-1; i++) {\n    if (current-&gt;next == NULL) {\n      return -1;\n    }\n    current = current-&gt;next;\n  }\n\n  if (current-&gt;next == NULL) {\n    return -1;\n  }\n\n  temp_node = current-&gt;next;\n  retval = temp_node-&gt;val;\n  current-&gt;next = temp_node-&gt;next;\n  free(temp_node);\n\n  return retval;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#exercise","title":"Exercise","text":"<p>You must implement the function <code>remove_by_value</code> which receives a double pointer to the head and removes the first item in the list which has the value <code>val</code>.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#provided-code","title":"Provided code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct node {\n    int val;\n    struct node * next;\n} node_t;\n\nvoid print_list(node_t * head) {\n    node_t * current = head;\n\n    while (current != NULL) {\n        printf(\"%d\\n\", current-&gt;val);\n        current = current-&gt;next;\n    }\n}\n\nint pop(node_t ** head) {\n    int retval = -1;\n    node_t * next_node = NULL;\n\n    if (*head == NULL) {\n        return -1;\n    }\n\n    next_node = (*head)-&gt;next;\n    retval = (*head)-&gt;val;\n    free(*head);\n    *head = next_node;\n\n    return retval;\n}\n\nint remove_by_value(node_t ** head, int val) {\n    /* TODO: fill in your code here */\n}\n\nint main() {\n\n    node_t * test_list = (node_t *) malloc(sizeof(node_t));\n    test_list-&gt;val = 1;\n    test_list-&gt;next = (node_t *) malloc(sizeof(node_t));\n    test_list-&gt;next-&gt;val = 2;\n    test_list-&gt;next-&gt;next = (node_t *) malloc(sizeof(node_t));\n    test_list-&gt;next-&gt;next-&gt;val = 3;\n    test_list-&gt;next-&gt;next-&gt;next = (node_t *) malloc(sizeof(node_t));\n    test_list-&gt;next-&gt;next-&gt;next-&gt;val = 4;\n    test_list-&gt;next-&gt;next-&gt;next-&gt;next = NULL;\n\n    remove_by_value(&amp;test_list, 3);\n\n    print_list(test_list);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#solution-code","title":"Solution code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct node {\n  int val;\n  struct node * next;\n} node_t;\n\nvoid print_list(node_t * head) {\n  node_t * current = head;\n\n  while (current != NULL) {\n    printf(\"%d\\n\", current-&gt;val);\n    current = current-&gt;next;\n  }\n}\n\nint pop(node_t ** head) {\n  int retval = -1;\n  node_t * next_node = NULL;\n\n  if (*head == NULL) {\n      return -1;\n  }\n\n  next_node = (*head)-&gt;next;\n  retval = (*head)-&gt;val;\n  free(*head);\n  *head = next_node;\n\n  return retval;\n}\n\nint remove_by_value(node_t ** head, int val) {\n  // Default return value (-1 for not found.)\n  int retval = -1;\n\n  // Create pointer to same location as head.\n  node_t * current = *head;\n\n  // Assign space for temporary node that will hold the node that should be removed.\n  node_t * temp_node = (node_t *) malloc(sizeof(node_t));\n\n  // If head/current/first node val is equal to val to be removed then...\n  if (current-&gt;val == val) {\n    // Tell head to advance to next, due to first being freed.\n    *head = current-&gt;next;\n\n    // Create temp node and gets its value for return. \n    temp_node = current;\n    retval = temp_node-&gt;val;\n\n    // Free the temp node.\n    temp_node-&gt;next = NULL;\n    free(temp_node);\n    return retval;\n  }\n\n  // Loop until we are one node before the node that should be removed.\n  while (current-&gt;next-&gt;val != val)\n  {\n    current = current-&gt;next;\n  }\n\n  // Create temp node that will be freed and get its value for return.\n  temp_node = current-&gt;next;\n  retval = temp_node-&gt;val;\n\n  // Point current next node to the node after the node that is going to be freed.\n  current-&gt;next = temp_node-&gt;next;\n\n  // Free temp node.\n  temp_node-&gt;next = NULL;\n  free(temp_node);\n  return retval;\n}\n\nint main() {\n  node_t * test_list = (node_t *) malloc(sizeof(node_t));\n  test_list-&gt;val = 1;\n  test_list-&gt;next = (node_t *) malloc(sizeof(node_t));\n  test_list-&gt;next-&gt;val = 2;\n  test_list-&gt;next-&gt;next = (node_t *) malloc(sizeof(node_t));\n  test_list-&gt;next-&gt;next-&gt;val = 3;\n  test_list-&gt;next-&gt;next-&gt;next = (node_t *) malloc(sizeof(node_t));\n  test_list-&gt;next-&gt;next-&gt;next-&gt;val = 4;\n  test_list-&gt;next-&gt;next-&gt;next-&gt;next = NULL;\n\n  remove_by_value(&amp;test_list, 3);\n\n  print_list(test_list);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/17%20-%20Linked%20Lists/#excalidraw","title":"Excalidraw","text":""},{"location":"Languages/C/01%20-%20Fundamentals/18%20-%20Binary%20Trees/","title":"18 - Binary Trees","text":"<p>Related issues: Issue</p> <p>Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/18%20-%20Binary%20Trees/#binary-trees","title":"Binary Trees","text":"<p>A Binary Tree is a type of data structure in which each node has at most two children (left child and right child). Binary trees are used to implement binary search trees and binary heaps, and are used for efficient searching and sorting. A binary tree is a special case of a K-ary tree, where k is 2. Common operations for binary trees include insertion, deletion, and traversal. The difficulty of performing these operations varies if the tree is balanced and also whether the nodes are leaf nodes or branch nodes. For balanced trees the depth of the left and right subtrees of every node differ by 1 or less. This allows for a predictable depth also known as height. This is the measure of a node from root to leaf, where root is 0 and sebsequent nodes are (1,2..n). This can be expressed by the integer part of log2(n) where n is the number of nodes in the tree.</p> <pre><code>        g                  s                  9\n       / \\                / \\                / \\\n      b   m              f   u              5   13\n     / \\                    / \\                /  \\\n    c   d                  t   y              11  15\n</code></pre> <p>The operations performed on trees requires searching in one of two main ways: Depth First Search and Breadth-first search. Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. One starts at the root and explores as far as possible along each branch before backtracking. There are three types of depth first search traversal: pre-order visit, left, right, in-order left, visit, right, post-order left, right, visit. Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph structures. In level-order, where we visit every node on a level before going to a lower level.</p> <p>DFS - pre-order - Visit - Left - Right</p> <p>DFS - in-order - Left - Visit - Right</p> <p>DFS - post-order - Left - Right - Visit</p>"},{"location":"Languages/C/01%20-%20Fundamentals/18%20-%20Binary%20Trees/#exercise","title":"Exercise","text":"<p>Below is an implementation of a binary tree that has insertion and printing capabilities. This tree is ordered but not balanced. This example maintains its ordering at insertion time.</p> <p>Change the print routine to depth-first search pre-order.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/18%20-%20Binary%20Trees/#provided-code","title":"Provided Code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct node\n{\n  int val;\n  struct node * left;\n  struct node * right;\n} node_t;\n\nvoid insert(node_t * tree,int val);\nvoid print_tree(node_t * current);\nvoid printDFS(node_t * current);\n\nint main()\n{\n  node_t * test_list = (node_t *) malloc(sizeof(node_t));\n  /* set values explicitly, alternative would be calloc() */\n  test_list-&gt;val = 0;\n  test_list-&gt;left = NULL;\n  test_list-&gt;right = NULL;\n\n  insert(test_list,5);\n  insert(test_list,8);\n  insert(test_list,4);\n  insert(test_list,3);\n\n  printDFS(test_list);\n  printf(\"\\n\");\n}\n\nvoid insert(node_t * tree, int val)\n{\n  if (tree-&gt;val == 0)\n  {\n    /* insert on current (empty) position */\n    tree-&gt;val = val;\n  }\n  else\n  {\n    if (val &lt; tree-&gt;val)\n    {\n      /* insert left */\n      if (tree-&gt;left != NULL)\n      {\n        insert(tree-&gt;left, val);\n      }\n      else\n      {\n        tree-&gt;left = (node_t *) malloc(sizeof(node_t));\n        /* set values explicitly, alternative would be calloc() */\n        tree-&gt;left-&gt;val = val;\n        tree-&gt;left-&gt;left = NULL;\n        tree-&gt;left-&gt;right = NULL;\n      }\n    }\n    else\n    {\n      if (val &gt;= tree-&gt;val)\n      {\n        /* insert right */\n        if (tree-&gt;right != NULL)\n        {\n          insert(tree-&gt;right,val);\n        }\n        else\n        {\n          tree-&gt;right = (node_t *) malloc(sizeof(node_t));\n          /* set values explicitly, alternative would be calloc() */\n          tree-&gt;right-&gt;val = val;\n          tree-&gt;right-&gt;left = NULL;\n          tree-&gt;right-&gt;right = NULL;\n        }\n      }\n    }\n  }\n}\n\n/* depth-first search */\nvoid printDFS(node_t * current)\n{\n  /* change the code here */\n  if (current == NULL)         return;   /* security measure */\n  if (current-&gt;left != NULL)   printDFS(current-&gt;left);\n  if (current != NULL)         printf(\"%d \", current-&gt;val);\n  if (current-&gt;right != NULL)  printDFS(current-&gt;right);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/18%20-%20Binary%20Trees/#solution-code","title":"Solution Code","text":"<pre><code>// Libraries\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// TreeNode struct\ntypedef struct node\n{\n  int val;\n  struct node * left;\n  struct node * right;\n} node_t;\n\n// Function declarations.\nvoid insert(node_t * tree,int val);\nvoid print_tree(node_t * current);\nvoid printDFS(node_t * current);\n\n/**\n * Entrypoint for the program.\n */\nint main()\n{\n  node_t * test_list = (node_t *) malloc(sizeof(node_t));\n  /* set values explicitly, alternative would be calloc() */\n  test_list-&gt;val = 0;\n  test_list-&gt;left = NULL;\n  test_list-&gt;right = NULL;\n\n  insert(test_list,5);\n  insert(test_list,8);\n  insert(test_list,4);\n  insert(test_list,3);\n\n  printDFS(test_list);\n  printf(\"\\n\");\n}\n\nvoid insert(node_t * tree, int val)\n{\n  if (tree-&gt;val == 0)\n  {\n    /* insert on current (empty) position */\n    tree-&gt;val = val;\n  }\n  else\n  {\n    if (val &lt; tree-&gt;val)\n    {\n      /* insert left */\n      if (tree-&gt;left != NULL)\n      {\n        insert(tree-&gt;left, val);\n      }\n      else\n      {\n        tree-&gt;left = (node_t *) malloc(sizeof(node_t));\n        /* set values explicitly, alternative would be calloc() */\n        tree-&gt;left-&gt;val = val;\n        tree-&gt;left-&gt;left = NULL;\n        tree-&gt;left-&gt;right = NULL;\n      }\n    }\n    else\n    {\n      if (val &gt;= tree-&gt;val)\n      {\n        /* insert right */\n        if (tree-&gt;right != NULL)\n        {\n          insert(tree-&gt;right,val);\n        }\n        else\n        {\n          tree-&gt;right = (node_t *) malloc(sizeof(node_t));\n          /* set values explicitly, alternative would be calloc() */\n          tree-&gt;right-&gt;val = val;\n          tree-&gt;right-&gt;left = NULL;\n          tree-&gt;right-&gt;right = NULL;\n        }\n      }\n    }\n  }\n}\n\n/* depth-first search */\nvoid printDFS(node_t * current)\n{\n  /* change the code here */\n  if (current == NULL)         return;   /* security measure */\n  if (current != NULL)         printf(\"%d \", current-&gt;val);\n  if (current-&gt;left != NULL)   printDFS(current-&gt;left);\n  if (current-&gt;right != NULL)  printDFS(current-&gt;right);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/19%20-%20Unions/","title":"19 - Unions","text":"<p>Issue Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/19%20-%20Unions/#overview","title":"Overview","text":"<p>C Unions are essentially the same as C Structures, except that instead of containing multiple variables each with their own memory a Union allows for multiple names to the same variable. These names can treat the memory as different types (and the size of the union will be the size of the largest type, + any padding the compiler might decide to give it)</p> <p>So if you wanted to be able to read a variable's memory in different ways, for example read an integer one byte at a time, you could have something like this:</p> <pre><code>union intParts {\n  int theInt;\n  char bytes[sizeof(int)];\n};\n</code></pre> <p>Allowing you to look at each byte individually without casting a pointer and using pointer arithmetic:</p> <pre><code>union intParts parts;\nparts.theInt = 5968145; // arbitrary number &gt; 255 (1 byte)\n\nprintf(\"The int is %i\\nThe bytes are [%i, %i, %i, %i]\\n\",\nparts.theInt, parts.bytes[0], parts.bytes[1], parts.bytes[2], parts.bytes[3]);\n\n// vs\n\nint theInt = parts.theInt;\nprintf(\"The int is %i\\nThe bytes are [%i, %i, %i, %i]\\n\",\ntheInt, *((char*)&amp;theInt+0), *((char*)&amp;theInt+1), *((char*)&amp;theInt+2), *((char*)&amp;theInt+3));\n\n// or with array syntax which can be a tiny bit nicer sometimes\n\nprintf(\"The int is %i\\nThe bytes are [%i, %i, %i, %i]\\n\",\n    theInt, ((char*)&amp;theInt)[0], ((char*)&amp;theInt)[1], ((char*)&amp;theInt)[2], ((char*)&amp;theInt)[3]);\n</code></pre> <p>Combining this with a structure allows you to create a \"tagged\" union which can be used to store multiple different types, one at a time.</p> <p>For example, you might have a \"number\" struct, but you don't want to use something like this:</p> <pre><code>struct operator {\n    int intNum;\n    float floatNum;\n    int type;\n    double doubleNum;\n};\n</code></pre> <p>Because your program has a lot of them and it takes a bit too much memory for all of the variables, so you could use this:</p> <pre><code>struct operator {\n    int type;\n    union {\n      int intNum;\n      float floatNum;\n      double doubleNum;\n    } types;\n};\n</code></pre> <p>Like this the size of the struct is just the size of the int <code>type</code> + the size of the largest type in the union (the double). Not a huge gain, only 8 or 16 bytes, but the concept can be applied to similar structs.</p> <p>use:</p> <pre><code>operator op;\nop.type = 0; // int, probably better as an enum or macro constant\nop.types.intNum = 352;\n</code></pre> <p>Also, if you don't give the union a name then it's members are accessed directly from the struct:</p> <pre><code>struct operator {\n    int type;\n    union {\n        int intNum;\n        float floatNum;\n        double doubleNum;\n    }; // no name!\n};\n\noperator op;\nop.type = 0; // int\n// intNum is part of the union, but since it's not named you access it directly off the struct itself\nop.intNum = 352;\n</code></pre> <p>Another, perhaps more useful feature, is when you always have multiple variables of the same type, and you want to be able to use both names (for readability) and indexes (for ease of iteration), in that case you can do something like this:</p> <pre><code>union Coins {\n    struct {\n        int quarter;\n        int dime;\n        int nickel;\n        int penny;\n    }; // anonymous struct acts the same way as an anonymous union, members are on the outer container\n    int coins[4];\n};\n</code></pre> <p>In that example you can see that there is a struct which contains the four (common) coins in the United States.</p> <p>since the union makes the variables share the same memory the coins array matches with each int in the struct (in order):</p> <pre><code>union Coins change;\nfor(int i = 0; i &lt; sizeof(change) / sizeof(int); ++i)\n{\n    scanf(\"%i\", change.coins + i); // BAD code! input is always suspect!\n}\nprintf(\"There are %i quarters, %i dimes, %i nickels, and %i pennies\\n\",\n    change.quarter, change.dime, change.nickel, change.penny);\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/19%20-%20Unions/#exercise","title":"Exercise","text":"<p>Create a union that stores an array of 21 characters and 6 ints (6 since 21 / 4 == 5, but 5 * 4 == 20 so you need 1 more for the purpose of this exercise), you will set the integers to 6 given values and then print out the character array both as a series of chars and as a string.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/19%20-%20Unions/#given-code","title":"Given Code","text":"<pre><code>#include &lt;stdio.h&gt;\n\n/* define the union here */\n\nint main() {\n  // initializer lists like this are assigned to the first member of the union/struct!\n  // There are 6 ints here so...\n  &lt;union declaration&gt; intCharacters = {{1853169737, 1936876900, 1684955508, 1768838432, 561213039, 0}};\n\n  /* testing code */\n  printf(\"[\");\n  // only go to 18 because 1 byte is for the terminating 0 and we don't print the last in the loop\n  for(int i = 0; i &lt; 19; ++i)\n    printf(\"%c, \", intCharacters.chars[i]);\n  printf(\"%c]\\n\", intCharacters.chars[19]);\n\n  printf(\"%s\\n\", intCharacters.chars);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/19%20-%20Unions/#solution-code","title":"Solution Code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nunion hiddenMessage {\n  int  ints[6];\n  char chars[21];\n};\n\nint main() {\n  union hiddenMessage intCharacters = {{1853169737, 1936876900, 1684955508, 1768838432, 561213039, 0}};\n\n  printf(\"[\");\n\n  // only go to 18 because 1 byte is for the terminating 0 and we don't print the last in the loop\n  for (int i = 0; i &lt; 19; ++i) {\n    printf(\"%c, \", intCharacters.chars[i]);\n  }\n\n  printf(\"%c]\\n\", intCharacters.chars[19]);\n  printf(\"%s\\n\", intCharacters.chars);\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/","title":"Pointer Arithmetics","text":"<p>You previously learned what is a pointer and how to manipulate pointers. In this tutorial you will be learning the arithmetic operations on pointers. There are multiple arithmetic operations that can be applied on C pointers: ++, --, -, +</p>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#incrementing-a-pointer-with","title":"Incrementing a Pointer with (++)","text":"<p>Just like any variable the ++ operation increases the value of that variable. In our case here the variable is a pointer hence when we increase its value we are increasing the address in the memory that pointer points to. Let's combine this operation with an array in our example:</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main()\n{\n    int intarray[5] = {10,20,30,40,50};\n\n    int i;\n    for(i = 0; i &lt; 5; i++)\n        printf(\"intarray[%d] has value %d - and address @ %x\\n\", i, intarray[i], &amp;intarray[i]);\n\n    int *intpointer = &amp;intarray[3]; //point to the 4th element in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the address of the 4th element\n\n    intpointer++; //now increase the pointer's address so it points to the 5th elemnt in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the address of the 5th element\n\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#decreasing-a-pointer-with-","title":"Decreasing a Pointer with (--)","text":"<p>Just like in our previous example we increased the pointer's pointed-to address by one using the ++ operator, we can decrease the address pointed-to by one using the decrement operator (--).</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main()\n{\n    int intarray[5] = {10,20,30,40,50};\n\n    int i;\n    for(i = 0; i &lt; 5; i++)\n        printf(\"intarray[%d] has value %d - and address @ %x\\n\", i, intarray[i], &amp;intarray[i]);\n\n    int *intpointer = &amp;intarray[4]; //point to the 5th element in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the address of the 5th element\n\n    intpointer--; //now decrease the point's address so it points to the 4th element in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the address of the 4th element\n\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#adding-pointers-with","title":"Adding Pointers with (+)","text":"<p>We previously increased a pointer's pointed-to address by one. We can also increase it by an integer value such:</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main()\n{\n    int intarray[5] = {10,20,30,40,50};\n\n    int i;\n    for(i = 0; i &lt; 5; i++)\n        printf(\"intarray[%d] has value: %d - and address @ %x\\n\", i, intarray[i], &amp;intarray[i]);\n\n    int *intpointer = &amp;intarray[1]; //point to the 2nd element in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the address of the 2nd element\n\n    intpointer += 2; //now shift by two the point's address so it points to the 4th element in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the addres of the 4th element\n\n    return 0;\n}\n</code></pre> <p>Note how in the output the address shifted by 8 steps in the memory. You might be wondering why? The answer is simple: Because our pointer is an int-pointer and the size of an int variable is 4 bytes the memory is shift-able by 4 blocks. In our code we shifted by 2 (added +2) to the initial address so that makes them 2 x 4 byte = 8.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#subtracting-pointers-with-","title":"Subtracting Pointers with (-)","text":"<p>Similarly we can subtract:</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main()\n{\n    int intarray[5] = {10,20,30,40,50};\n\n    int i;\n    for(i = 0; i &lt; 5; i++)\n        printf(\"intarray[%d] has value: %d - and address @ %x\\n\", i, intarray[i], &amp;intarray[i]);\n\n    int *intpointer = &amp;intarray[4]; //point to the 5th element in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the address of the 5th element\n\n    intpointer -= 2; //now shift by two the point's address so it points to the 3rd element in the array\n    printf(\"address: %x - has value %d\\n\", intpointer, *intpointer); //print the address of the 3rd element\n\n    return 0;\n}\n</code></pre> <p>again the address is shifted by blocks of 4bytes (in case of int).</p>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#other-operations","title":"Other Operations","text":"<p>There are more operations such as comparison &gt;, &lt;, \\==. The idea is very similar of comparing variables, but in this case we are comparing memory address.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#exercise","title":"Exercise","text":"<p>Copy last three addresses of intarray into parray which is an array of pointers to an int.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#given-code","title":"Given Code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int intarray[5] = {10,20,30,40,50};\n    //-----------------------^\n    int *pointer = &amp;intarray[2];\n\n    // Array of 3 pointers\n    int *parray[3];\n\n    // Copy last three addresses of intarray into parray\n    // Use parray and pointer\n    int i;\n    for (i = 0; i &lt; 3; i++) {\n        // Insert code here\n    }\n\n    // Test code\n    for (i = 0; i &lt; 3; i++) {\n        if (parray[i] == &amp;pointer[i]) {\n            printf(\"Matched!\\n\");\n        } else {\n            printf(\"Fail\\n\");\n        }\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/20%20-%20Pointer%20Arithmetics/#solution-code","title":"Solution Code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int intarray[5] = {10,20,30,40,50};\n    //-----------------------^\n    int *pointer = &amp;intarray[2];\n\n    // Array of 3 pointers\n    int *parray[3];\n\n    // Copy last three addresses of intarray into parray\n    // Use parray and pointer\n    int i;\n    for (i = 0; i &lt; 3; i++) {\n        // Insert code here\n    }\n\n    // Test code\n    for (i = 0; i &lt; 3; i++) {\n        if (parray[i] == &amp;pointer[i]) {\n            printf(\"Matched!\\n\");\n        } else {\n            printf(\"Fail\\n\");\n        }\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/21%20-%20Function%20Pointers/","title":"21 - Function Pointers","text":"<p>Issue Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/21%20-%20Function%20Pointers/#function-pointers","title":"Function Pointers","text":"<p>Remember pointers? We used them to point to an array of chars then make a string out of them. Then things got more interesting when we learned how to control these pointers. Now it is time to do something even more interesting with pointers, using them to point to and call functions.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/21%20-%20Function%20Pointers/#why-point-to-a-function","title":"Why point to a function?","text":"<p>The first question that may come to your mind is why would we use pointers to call a function when we can simply call a function by its name: <code>function();</code> - that's a great question! Now imagine the <code>sort</code> function where you need to sort an array. Sometimes you want to order array elements in an ascending order or descending order. How would you choose? Function pointers!</p>"},{"location":"Languages/C/01%20-%20Fundamentals/21%20-%20Function%20Pointers/#function-pointer-syntax","title":"Function Pointer Syntax","text":"<pre><code>void (*pf)(int);\n</code></pre> <p>I agree with you. This definitely is very complicated, or so you may think. Let's re-read that code and try to understand it point by point. Read it inside-out. <code>*pf</code> is the pointer to a function. <code>void</code> is the return type of that function, and finally <code>int</code> is the argument type of that function. Got it? Good.</p> <p>Let's insert pointers into the function pointer and try to read it again:</p> <pre><code>char* (*pf)(int*)\n</code></pre> <p>Again: 1. <code>*pf</code> is the function pointer. 2. <code>char*</code> is the return type of that function. 3. <code>int*</code> is the type of the argument.</p> <p>Ok enough with theory. Let's get our hands dirty with some real code. See this example:</p> <pre><code>#include &lt;stdio.h&gt;\nvoid someFunction(int arg)\n{\n    printf(\"This is someFunction being called and arg is: %d\\n\", arg);\n    printf(\"Whoops leaving the function now!\\n\");\n}\n\nmain()\n{\n    void (*pf)(int);\n    pf = &amp;someFunction;\n    printf(\"We're about to call someFunction() using a pointer!\\n\");\n    (pf)(5);\n    printf(\"Wow that was cool. Back to main now!\\n\\n\");\n}\n</code></pre> <p>Remember <code>sort()</code> we talked about earlier? We can do the same with it. Instead of ordering a set in an ascending way we can do the opposite using our own comparison function as follows:</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt; //for qsort()\n\nint compare(const void* left, const void* right)\n{\n    return (*(int*)right - *(int*)left);\n    // go back to ref if this seems complicated: http://www.cplusplus.com/reference/cstdlib/qsort/\n}\nmain()\n{\n    int (*cmp) (const void* , const void*);\n    cmp = &amp;compare;\n\n    int iarray[] = {1,2,3,4,5,6,7,8,9};\n    qsort(iarray, sizeof(iarray)/sizeof(*iarray), sizeof(*iarray), cmp);\n\n    int c = 0;\n    while (c &lt; sizeof(iarray)/sizeof(*iarray))\n    {\n        printf(\"%d \\t\", iarray[c]);\n        c++;\n    }\n}\n</code></pre> <p>Let's remember again. Why do we use function pointers? 1. To allow programmers to use libraries for different usages -&gt; \"Flexibility\"</p>"},{"location":"Languages/C/01%20-%20Fundamentals/21%20-%20Function%20Pointers/#exercise","title":"Exercise","text":"<p>Complete the array of pointers to functions and call each function using its pointer from the array. Array of pointers to functions? Yes you can do that!</p>"},{"location":"Languages/C/01%20-%20Fundamentals/21%20-%20Function%20Pointers/#given-code","title":"Given Code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nvoid f1(int var)\n{\n    printf(\"this is f1 and var is: %d\\n\", var);\n}\n\nvoid f2(int var)\n{\n    printf(\"this is f2 and var is: %d\\n\", var);\n}\n\nvoid f3(int var)\n{\n    printf(\"this is f3 and var is: %d\\n\", var);\n}\n\nint main()\n{\n    /* define an array full of function pointers \n    to the above functions, that take an `int` as \n    their only argument */\n\n\n    int c = 0;\n    while(c &lt; 3)\n    {\n        /* call the functions using the function pointers\n        of the array at index `c` with `c` as an argument */\n        ++c;\n    }\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/21%20-%20Function%20Pointers/#solution-code","title":"Solution Code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nvoid f1(int var)\n{\n    printf(\"this is f1 and var is: %d\\n\", var);\n}\n\nvoid f2(int var)\n{\n    printf(\"this is f2 and var is: %d\\n\", var);\n}\n\nvoid f3(int var)\n{\n    printf(\"this is f3 and var is: %d\\n\", var);\n}\n\nint main()\n{\n    /* define an array full of function pointers \n    to the above functions, that take an `int` as \n    their only argument */\n  void (*funcPtrs[])(int) = {&amp;f1, &amp;f2, &amp;f3};\n\n    int c = 0;\n    while(c &lt; 3)\n    {\n    /* call the functions using the function pointers\n        of the array at index `c` with `c` as an argument */\n    funcPtrs[c](c);\n        c++;\n    }\n\n  return 0;\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/","title":"22 - Bitmasks","text":"<p>Issue Issue</p>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#bitmasks","title":"Bitmasks","text":"<p>Bit masking is simply the process of storing data truly as bits, as opposed to storing it as chars/ints/floats. It is incredibly useful for storing certain types of data compactly and efficiently.</p> <p>The idea for bit masking is based on boolean logic. For those not familiar, boolean logic is the manipulation of 'true' (1) and 'false' (0) through logical operations (that take 0s and 1s as their argument). We are concerned with the following operations:</p> <ul> <li>NOT a - the final value is the opposite of the input value (1 -&gt; 0, 0 -&gt; 1)</li> <li>a AND b - if both values are 1, the final value is 1, otherwise the final value is 0</li> <li>a OR b - if either value is 1, the final value is 1, otherwise the final value is 0</li> <li>a XOR b - if one value is 1 and the other value is 0, the final value is 1, otherwise the final value is 0</li> </ul> <p>In computing, one of these true/false values is a bit. Primitives in C (<code>int</code>, <code>float</code>, etc) are made up of some number of bits, where that number is a multiple of 8. For example, an <code>int</code> may be at least 16 bits in size, where a <code>char</code> may be 8 bits. 8 bits is typically referred to as a byte. C guarantees that certain primitives are at least some number of bytes in size. The introduction of <code>stdint.h</code> in C11 allows the programmer to specify integer types that are exactly some number of bytes, which is extremely useful when using masks.</p> <p>Bit masks are often used when setting flags. Flags are values that can be in two states, such as 'on/off' and 'moving/stationary'.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#setting-bit-n","title":"Setting bit n","text":"<p>Setting bit <code>n</code> is as simple as ORing the value of the storage variable with the value <code>2^n</code>.</p> <p><code>storage |= 1 &lt;&lt; n;</code></p> <p>As an example, here is the setting of bit 3 where <code>storage</code> is a char (8 bits):</p> <p><code>01000010 OR 00001000 == 01001010</code></p> <p>The <code>2^n</code> logic places the '1' value at the proper bit in the mask itself, allowing access to that same bit in the storage variable.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#clearing-bit-n","title":"Clearing bit n","text":"<p>Clearing bit <code>n</code> is the result of ANDing the value of the storage variable with the inverse (NOT) of the value <code>2^n</code>:</p> <p><code>storage &amp;= ~(1 &lt;&lt; n);</code></p> <p>Here's the example again:</p> <p><code>01001010 AND 11110111 == 01000010</code></p>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#flipping-bit-n","title":"Flipping bit n","text":"<p>Flipping bit <code>n</code> is the result of XORing the value of the storage variable with <code>2^n</code>:</p> <p><code>storage ^= 1 &lt;&lt; n;</code></p> <p><code>01000010 01001010 XOR XOR 00001000 00001000 == == 01001010 01000010</code></p>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#checking-bit-n","title":"Checking bit n","text":"<p>Checking a bit is ANDing the value of <code>2^n</code> with the bit storage:</p> <p><code>bit = storage &amp; (1 &lt;&lt; n);</code></p> <p><code>01000010 01001010 AND AND 00001000 00001000 == == 00000000 00001000</code></p>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#exercise","title":"Exercise","text":"<p>Use bit masks to manipulate some flags.</p>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#given-code","title":"Given code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;assert.h&gt;\n\n/* Finish initializing the flags */\n\nconst short FLAG_ON          = 1 &lt;&lt; 0; // 1  (0x01)\nconst short FLAG_MOVEMENT    = 1 &lt;&lt; 1; // 2  (0x02)\nconst short FLAG_TRANSPARENT = 1 &lt;&lt; 2; // 4  (0x04)\nconst short FLAG_ALIVE       = ;\nconst short FLAG_BROKEN      = ;\nconst short FLAG_EDIBLE      = 1 &lt;&lt; 5; // 32 (0x20)\n\nint main() {\n  short attributes = 0;\n\n  /* Set the attributes ON, TRANSPARENT, and BROKEN */\n\n  assert(attributes == (FLAG_ON | FLAG_TRANSPARENT | FLAG_BROKEN));\n\n  /* Modify (set/clear/toggle) so the only attributes are ON and ALIVE */\n\n  assert(attributes == (FLAG_ON | FLAG_ALIVE));\n\n  /* Check if the ALIVE flag is set */\n  assert(/* ??? */);\n\n  /* Check if the BROKEN flag is not set */\n  assert(/* ??? */);\n\n  /* Modify so only the EDIBLE attribute is set */\n\n  assert(attributes == FLAG_EDIBLE);\n\n  printf(\"Done!\");\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/22%20-%20Bitmasks/#solution-code","title":"Solution Code","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;assert.h&gt;\n\n/* Finish initializing the flags */\n\nconst short FLAG_ON          = 1 &lt;&lt; 0; // 1  (0x01)\nconst short FLAG_MOVEMENT    = 1 &lt;&lt; 1; // 2  (0x02)\nconst short FLAG_TRANSPARENT = 1 &lt;&lt; 2; // 4  (0x04)\nconst short FLAG_ALIVE       = 1 &lt;&lt; 3; // 8  (0x08)\nconst short FLAG_BROKEN      = 1 &lt;&lt; 4; // 16 (0x10)\nconst short FLAG_EDIBLE      = 1 &lt;&lt; 5; // 32 (0x20)\n\nint main() {\n  short attributes = 0;\n\n  /* Set the attributes ON, TRANSPARENT, and BROKEN */\n  attributes |= FLAG_ON;\n  attributes |= FLAG_TRANSPARENT;\n  attributes |= FLAG_BROKEN;\n  assert(attributes == (FLAG_ON | FLAG_TRANSPARENT | FLAG_BROKEN));\n\n  /* Modify (set/clear/toggle) so the only attributes are ON and ALIVE */\n  // FLIP TRANSPARENT AND BROKEN\n  attributes ^= ~FLAG_TRANSPARENT;\n  attributes ^= ~FLAG_BROKEN;\n\n  // SET ALIVE\n  attributes |= FLAG_ALIVE;\n\n  assert(attributes == (FLAG_ON | FLAG_ALIVE));\n\n  /* Check if the ALIVE flag is set */\n  assert(attributes &amp; FLAG_ALIVE);\n\n  /* Check if the BROKEN flag is not set */\n  assert(!(attributes &amp; FLAG_BROKEN));\n\n  /* Modify so only the EDIBLE attribute is set */\n  // CLEAR EXISTING ATTRIBUTES\n  attributes &amp;= ~FLAG_ON;\n  attributes &amp;= ~FLAG_ALIVE;\n\n  // SET EDIBLE\n  attributes |= FLAG_EDIBLE;\n  assert(attributes == FLAG_EDIBLE);\n\n  printf(\"Done!\");\n}\n</code></pre>"},{"location":"Languages/C/01%20-%20Fundamentals/23%20-%20Exercism/","title":"23 - Exercism","text":"<p>Can find all exercises done here</p>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/","title":"Linked lists","text":""},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#introduction","title":"Introduction","text":"<p>Linked lists are the best and simplest example of a dynamic data structure that uses pointers for its implementation. However, understanding pointers is crucial to understanding how linked lists work, so if you've skipped the pointers tutorial, you should go back and redo it. You must also be familiar with dynamic memory allocation and structures.</p> <p>Essentially, linked lists function as an array that can grow and shrink as needed, from any point in the array.</p> <p>Linked lists have a few advantages over arrays:</p> <ol> <li>Items can be added or removed from the middle of the list</li> <li>There is no need to define an initial size</li> </ol> <p>However, linked lists also have a few disadvantages:</p> <ol> <li>There is no \"random\" access - it is impossible to reach the nth item in the array without first iterating over all items up until that item. This means we have to start from the beginning of the list and count how many times we advance in the list until we get to the desired item.</li> <li>Dynamic memory allocation and pointers are required, which complicates the code and increases the risk of memory leaks and segment faults.</li> <li>Linked lists have a much larger overhead over arrays, since linked list items are dynamically allocated (which is less efficient in memory usage) and each item in the list also must store an additional pointer.</li> </ol>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#what-is-a-linked-list","title":"What is a linked list?","text":"<p>A linked list is a set of dynamically allocated nodes, arranged in such a way that each node contains one value and one pointer. The pointer always points to the next member of the list. If the pointer is NULL, then it is the last node in the list.</p> <p>A linked list is held using a local pointer variable which points to the first item of the list. If that pointer is also NULL, then the list is considered to be empty.</p> <pre><code>    ------------------------------              ------------------------------\n    |              |             |            \\ |              |             |\n    |     DATA     |     NEXT    |--------------|     DATA     |     NEXT    |\n    |              |             |            / |              |             |\n    ------------------------------              ------------------------------\n</code></pre> <p>Let's define a linked list node:</p> <pre><code>typedef struct node {\n    int val;\n    struct node * next;\n} node_t;\n</code></pre> <p>Notice that we are defining the struct in a recursive manner, which is possible in C. Let's name our node type <code>node_t</code>.</p> <p>Now we can use the nodes. Let's create a local variable which points to the first item of the list (called <code>head</code>).</p> <pre><code>node_t * head = NULL;\nhead = (node_t *) malloc(sizeof(node_t));\nif (head == NULL) {\n    return 1;\n}\n\nhead-&gt;val = 1;\nhead-&gt;next = NULL;\n</code></pre> <p>We've just created the first variable in the list. We must set the value, and the next item to be empty, if we want to finish populating the list. Notice that we should always check if malloc returned a NULL value or not.</p> <p>To add a variable to the end of the list, we can just continue advancing to the next pointer:</p> <pre><code>node_t * head = NULL;\nhead = (node_t *) malloc(sizeof(node_t));\nhead-&gt;val = 1;\nhead-&gt;next = (node_t *) malloc(sizeof(node_t));\nhead-&gt;next-&gt;val = 2;\nhead-&gt;next-&gt;next = NULL;\n</code></pre> <p>This can go on and on, but what we should actually do is advance to the last item of the list, until the <code>next</code> variable will be <code>NULL</code>.</p>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#iterating-over-a-list","title":"Iterating over a list","text":"<p>Let's build a function that prints out all the items of a list. To do this, we need to use a <code>current</code> pointer that will keep track of the node we are currently printing. After printing the value of the node, we set the <code>current</code> pointer to the next node, and print again, until we've reached the end of the list (the next node is NULL).</p> <pre><code>void print_list(node_t * head) {\n    node_t * current = head;\n\n    while (current != NULL) {\n        printf(\"%d\\n\", current-&gt;val);\n        current = current-&gt;next;\n    }\n}\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#adding-an-item-to-the-end-of-the-list","title":"Adding an item to the end of the list","text":"<p>To iterate over all the members of the linked list, we use a pointer called <code>current</code>. We set it to start from the head and then in each step, we advance the pointer to the next item in the list, until we reach the last item.</p> <pre><code>void push(node_t * head, int val) {\n    node_t * current = head;\n    while (current-&gt;next != NULL) {\n        current = current-&gt;next;\n    }\n\n    /* now we can add a new variable */\n    current-&gt;next = (node_t *) malloc(sizeof(node_t));\n    current-&gt;next-&gt;val = val;\n    current-&gt;next-&gt;next = NULL;\n}\n</code></pre> <p>The best use cases for linked lists are stacks and queues, which we will now implement:</p>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#adding-an-item-to-the-beginning-of-the-list-pushing-to-the-list","title":"Adding an item to the beginning of the list (pushing to the list)","text":"<p>To add to the beginning of the list, we will need to do the following:</p> <ol> <li>Create a new item and set its value</li> <li>Link the new item to point to the head of the list</li> <li>Set the head of the list to be our new item</li> </ol> <p>This will effectively create a new head to the list with a new value, and keep the rest of the list linked to it.</p> <p>Since we use a function to do this operation, we want to be able to modify the head variable. To do this, we must pass a pointer to the pointer variable (a double pointer) so we will be able to modify the pointer itself.</p> <pre><code>void push(node_t ** head, int val) {\n    node_t * new_node;\n    new_node = (node_t *) malloc(sizeof(node_t));\n\n    new_node-&gt;val = val;\n    new_node-&gt;next = *head;\n    *head = new_node;\n}\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#removing-the-first-item-popping-from-the-list","title":"Removing the first item (popping from the list)","text":"<p>To pop a variable, we will need to reverse this action:</p> <ol> <li>Take the next item that the head points to and save it</li> <li>Free the head item</li> <li>Set the head to be the next item that we've stored on the side</li> </ol> <p>Here is the code:</p> <pre><code>int pop(node_t ** head) {\n    int retval = -1;\n    node_t * next_node = NULL;\n\n    if (*head == NULL) {\n        return -1;\n    }\n\n    next_node = (*head)-&gt;next;\n    retval = (*head)-&gt;val;\n    free(*head);\n    *head = next_node;\n\n    return retval;\n}\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#removing-the-last-item-of-the-list","title":"Removing the last item of the list","text":"<p>Removing the last item from a list is very similar to adding it to the end of the list, but with one big exception - since we have to change one item before the last item, we actually have to look two items ahead and see if the next item is the last one in the list:</p> <pre><code>int remove_last(node_t * head) {\n    int retval = 0;\n    /* if there is only one item in the list, remove it */\n    if (head-&gt;next == NULL) {\n        retval = head-&gt;val;\n        free(head);\n        return retval;\n    }\n\n    /* get to the second to last node in the list */\n    node_t * current = head;\n    while (current-&gt;next-&gt;next != NULL) {\n        current = current-&gt;next;\n    }\n\n    /* now current points to the second to last item of the list, so let's remove current-&gt;next */\n    retval = current-&gt;next-&gt;val;\n    free(current-&gt;next);\n    current-&gt;next = NULL;\n    return retval;\n}\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#removing-a-specific-item","title":"Removing a specific item","text":"<p>To remove a specific item from the list, either by its index from the beginning of the list or by its value, we will need to go over all the items, continuously looking ahead to find out if we've reached the node before the item we wish to remove. This is because we need to change the location to where the previous node points to as well.</p> <p>Here is the algorithm:</p> <ol> <li>Iterate to the node before the node we wish to delete</li> <li>Save the node we wish to delete in a temporary pointer</li> <li>Set the previous node's next pointer to point to the node after the node we wish to delete</li> <li>Delete the node using the temporary pointer</li> </ol> <p>There are a few edge cases we need to take care of, so make sure you understand the code.</p> <pre><code>int remove_by_index(node_t ** head, int n) {\n    int i = 0;\n    int retval = -1;\n    node_t * current = *head;\n    node_t * temp_node = NULL;\n\n    if (n == 0) {\n        return pop(head);\n    }\n\n    for (i = 0; i &lt; n-1; i++) {\n        if (current-&gt;next == NULL) {\n            return -1;\n        }\n        current = current-&gt;next;\n    }\n\n    if (current-&gt;next == NULL) {\n        return -1;\n    }\n\n    temp_node = current-&gt;next;\n    retval = temp_node-&gt;val;\n    current-&gt;next = temp_node-&gt;next;\n    free(temp_node);\n\n    return retval;\n}\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/1%20-%20Linked%20Lists/#complete-example-from-above-content","title":"Complete example from above content","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// Structs\ntypedef struct node {\n  int val;\n  struct node * next;\n} node_t;\n\n// Function declarations\nvoid print_list(node_t * head);\nvoid push_end(node_t * head, int val);\nvoid push(node_t ** head, int val);\nint pop(node_t ** head);\nint remove_last(node_t * head);\nint remove_by_index(node_t ** head, int n);\n\n// Entrypoint\nint main() {\n  // create head and allocate memory for it.\n  node_t * head = NULL;\n  head = (node_t *) malloc(sizeof(node_t));\n\n  // If it is still NULL, exit with code 1.\n  if (head == NULL) {\n    return 1;\n  }\n\n  // Assign head intial value.\n  head-&gt;val = 1;\n  head-&gt;next = NULL;\n\n  // Add more items to linked list\n  // START\n  head-&gt;val = 1;\n  head-&gt;next = (node_t *) malloc(sizeof(node_t));\n\n  // Check that allocation did not fail...\n  if (head-&gt;next == NULL) {\n    return 1;\n  }\n\n  head-&gt;next-&gt;val = 2;\n  head-&gt;next-&gt;next = NULL;\n  // END\n\n  // Print list after initialization\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Push new node to the end of the linked list\n  push_end(head, 3);\n\n  // Print list after pushing to it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Push new node to start of the the linked list\n  push(&amp;head, 4);\n\n  // Print list after pushing to it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Pop the first item from the linked list.\n  pop(&amp;head);\n\n  // Print list after popping first item from it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Pop the last item from the linked list.\n  remove_last(head);\n\n  // Print list after popping last item from it\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Repopulate with some values\n  push_end(head, 3);\n  push_end(head, 4);\n  push_end(head, 5);\n\n  // Print list after populating it.\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Remove by index.\n  remove_by_index(&amp;head, 2);\n\n  // Print list after removing the 2nd index value.\n  printf(\"-----\\n\");\n  print_list(head);\n  printf(\"-----\\n\");\n\n  // Return exit code 0\n  return 0;\n}\n\n// Print the list of nodes\nvoid print_list(node_t * head) {\n  node_t * current = head;\n\n  while (current != NULL) {\n    printf(\"%d\\n\", current-&gt;val);\n    current = current-&gt;next;\n  }\n}\n\n// Add to end of linked list\nvoid push_end(node_t * head, int val) {\n  node_t * current = head;\n  while (current-&gt;next != NULL) {\n    current = current-&gt;next;\n  }\n\n  /* now we can add a new variable */\n  current-&gt;next = (node_t *) malloc(sizeof(node_t));\n  current-&gt;next-&gt;val = val;\n  current-&gt;next-&gt;next = NULL;\n}\n\n// Add to start of linked list\nvoid push(node_t ** head, int val) {\n  node_t * new_node;\n  new_node = (node_t *) malloc(sizeof(node_t));\n\n  new_node-&gt;val = val;\n  new_node-&gt;next = *head;\n  *head = new_node;\n}\n\n// Remove the item at the start of the linked list\nint pop(node_t ** head) {\n  int retval = -1;\n  node_t * next_node = NULL;\n\n  if (*head == NULL) {\n    return retval;\n  }\n\n  next_node = (*head)-&gt;next;\n  retval = (*head)-&gt;val;\n  free(*head);\n  *head = next_node;\n\n  return retval;\n}\n\n// Remove the last item from the linked list\nint remove_last(node_t * head) {\n  int retval = 0;\n\n  /* if there is only one item in the list, remove it */\n  if (head-&gt;next == NULL) {\n    retval = head-&gt;val;\n    free(head);\n    return retval;\n  }\n\n  /* get to the second to last node in the list */\n  node_t * current = head;\n  while (current-&gt;next-&gt;next != NULL) {\n    current = current-&gt;next;\n  }\n\n  /* now current points to the second to last item of the list, so let's remove current-&gt;next */\n  retval = current-&gt;next-&gt;val;\n  free(current-&gt;next);\n  current-&gt;next = NULL;\n  return retval;\n}\n\nint remove_by_index(node_t ** head, int n) {\n  int i = 0;\n  int retval = -1;\n  node_t * current = *head;\n  node_t * temp_node = NULL;\n\n  if (n == 0) {\n    return pop(head);\n  }\n\n  for (i = 0; i &lt; n-1; i++) {\n    if (current-&gt;next == NULL) {\n      return -1;\n    }\n    current = current-&gt;next;\n  }\n\n  if (current-&gt;next == NULL) {\n    return -1;\n  }\n\n  temp_node = current-&gt;next;\n  retval = temp_node-&gt;val;\n  current-&gt;next = temp_node-&gt;next;\n  free(temp_node);\n\n  return retval;\n}\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/2%20-%20Binary%20Trees/","title":"Binary Trees","text":"<p>A Binary Tree is a type of data structure in which each node has at most two children (left child and right child). Binary trees are used to implement binary search trees and binary heaps, and are used for efficient searching and sorting. A binary tree is a special case of a K-ary tree, where k is 2. Common operations for binary trees include insertion, deletion, and traversal. The difficulty of performing these operations varies if the tree is balanced and also whether the nodes are leaf nodes or branch nodes. For balanced trees the depth of the left and right subtrees of every node differ by 1 or less. This allows for a predictable depth also known as height. This is the measure of a node from root to leaf, where root is 0 and sebsequent nodes are (1,2..n). This can be expressed by the integer part of log2(n) where n is the number of nodes in the tree.</p> <pre><code>        g                  s                  9\n       / \\                / \\                / \\\n      b   m              f   u              5   13\n     / \\                    / \\                /  \\\n    c   d                  t   y              11  15\n</code></pre> <p>The operations performed on trees requires searching in one of two main ways: Depth First Search and Breadth-first search. Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. One starts at the root and explores as far as possible along each branch before backtracking. There are three types of depth first search traversal: pre-order visit, left, right, in-order left, visit, right, post-order left, right, visit. Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph structures. In level-order, where we visit every node on a level before going to a lower level.</p> <p>DFS - pre-order - Visit - Left - Right</p> <p>DFS - in-order - Left - Visit - Right</p> <p>DFS - post-order - Left - Right - Visit</p>"},{"location":"Languages/C/02%20-%20Data%20Structures/2%20-%20Binary%20Trees/#python-examples","title":"Python examples","text":""},{"location":"Languages/C/02%20-%20Data%20Structures/2%20-%20Binary%20Trees/#depth-first-search-pre-order","title":"Depth First Search - Pre-Order","text":"<pre><code>\"\"\"Contains example of depth first search with pre-order technique.\"\"\"\n\n\nclass TreeNode:\n    \"\"\"Binary tree node.\"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"Constructor for the binary tree node.\"\"\"\n        self.value: int = value\n        self.left: TreeNode | None = None\n        self.right: TreeNode | None = None\n\n\ndef pre_order_traversal(node: TreeNode | None):\n    \"\"\"Method for pre-order-traversal.\"\"\"\n    if node is not None:\n        print(node.value, end=' ')\n        pre_order_traversal(node.left)\n        pre_order_traversal(node.right)\n\n\n# Example Tree:\n#\n#     1\n#    / \\\n#   2   3\n#  / \\   \\\n# 4   5   6\n\n# Example usage:\nif __name__ == \"__main__\":\n    root = TreeNode(1)\n    root.left = TreeNode(2)\n    root.right = TreeNode(3)\n    root.left.left = TreeNode(4)\n    root.left.right = TreeNode(5)\n    root.right.right = TreeNode(6)\n    pre_order_traversal(root)\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/2%20-%20Binary%20Trees/#depth-first-search-in-order","title":"Depth First Search - In-Order","text":"<pre><code>\"\"\"Contains example of depth first search with in-order technique.\"\"\"\n\n\nclass TreeNode:\n    \"\"\"Binary tree node.\"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"Constructor for the binary tree node.\"\"\"\n        self.value: int = value\n        self.left: TreeNode | None = None\n        self.right: TreeNode | None = None\n\n\ndef in_order_traversal(node: TreeNode | None):\n    \"\"\"Method for in-order traversal.\"\"\"\n    if node is not None:\n        in_order_traversal(node.left)\n        print(node.value, end=' ')\n        in_order_traversal(node.right)\n\n\n# Example Tree:\n# Example Tree:\n#\n#     1\n#    / \\\n#   2   3\n#  / \\   \\\n# 4   5   6\n\n# Example usage:\nif __name__ == \"__main__\":\n    root = TreeNode(1)\n    root.left = TreeNode(2)\n    root.right = TreeNode(3)\n    root.left.left = TreeNode(4)\n    root.left.right = TreeNode(5)\n    root.right.right = TreeNode(6)\n    in_order_traversal(root)\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/2%20-%20Binary%20Trees/#depth-first-search-post-order","title":"Depth First Search - Post-Order","text":"<pre><code>\"\"\"Contains example of depth first search with post-order technique.\"\"\"\n\n\nclass TreeNode:\n    \"\"\"Binary tree node.\"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"Constructor for the binary tree node.\"\"\"\n        self.value: int = value\n        self.left: TreeNode | None = None\n        self.right: TreeNode | None = None\n\n\ndef post_order_traversal(node: TreeNode | None):\n    \"\"\"Method for post-order traversal.\"\"\"\n    if node is not None:\n        post_order_traversal(node.left)\n        post_order_traversal(node.right)\n        print(node.value, end=' ')\n\n\n# Example Tree:\n# Example Tree:\n#\n#     1\n#    / \\\n#   2   3\n#  / \\   \\\n# 4   5   6\n\n# Example usage:\nif __name__ == \"__main__\":\n    root = TreeNode(1)\n    root.left = TreeNode(2)\n    root.right = TreeNode(3)\n    root.left.left = TreeNode(4)\n    root.left.right = TreeNode(5)\n    root.right.right = TreeNode(6)\n    post_order_traversal(root)\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/2%20-%20Binary%20Trees/#breadth-first-search","title":"Breadth First Search","text":"<pre><code>\"\"\"Contains example of breadth first search.\"\"\"\n\nfrom collections import deque\n\n\nclass TreeNode:\n    \"\"\"Binary tree node.\"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"Constructor for the binary tree node.\"\"\"\n        self.value: int = value\n        self.left: TreeNode | None = None\n        self.right: TreeNode | None = None\n\n\ndef bfs_traversal(root: TreeNode | None):\n    \"\"\"Contains method for bfs traversal.\"\"\"\n    if root is None:\n        return\n\n    queue = deque([root])\n\n    while queue:\n        current = queue.popleft()\n        print(current.value, end=' ')\n\n        if current.left is not None:\n            queue.append(current.left)\n        if current.right is not None:\n            queue.append(current.right)\n\n\n# Example Tree:\n#\n#     1\n#    / \\\n#   2   3\n#  / \\   \\\n# 4   5   6\n\n# Example usage:\nif __name__ == \"__main__\":\n    root = TreeNode(1)\n    root.left = TreeNode(2)\n    root.right = TreeNode(3)\n    root.left.left = TreeNode(4)\n    root.left.right = TreeNode(5)\n    root.right.right = TreeNode(6)\n    bfs_traversal(root)\n</code></pre>"},{"location":"Languages/C/02%20-%20Data%20Structures/2%20-%20Binary%20Trees/#c-example-dfs-pre-order","title":"C Example - DFS Pre-Order","text":"<pre><code>// Libraries\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// TreeNode struct\ntypedef struct node\n{\n  int val;\n  struct node * left;\n  struct node * right;\n} node_t;\n\n// Function declarations.\nvoid insert(node_t * tree,int val);\nvoid print_tree(node_t * current);\nvoid printDFS(node_t * current);\n\n/**\n * Entrypoint for the program.\n */\nint main()\n{\n  node_t * test_list = (node_t *) malloc(sizeof(node_t));\n  /* set values explicitly, alternative would be calloc() */\n  test_list-&gt;val = 0;\n  test_list-&gt;left = NULL;\n  test_list-&gt;right = NULL;\n\n  insert(test_list,5);\n  insert(test_list,8);\n  insert(test_list,4);\n  insert(test_list,3);\n\n  printDFS(test_list);\n  printf(\"\\n\");\n}\n\nvoid insert(node_t * tree, int val)\n{\n  if (tree-&gt;val == 0)\n  {\n    /* insert on current (empty) position */\n    tree-&gt;val = val;\n  }\n  else\n  {\n    if (val &lt; tree-&gt;val)\n    {\n      /* insert left */\n      if (tree-&gt;left != NULL)\n      {\n        insert(tree-&gt;left, val);\n      }\n      else\n      {\n        tree-&gt;left = (node_t *) malloc(sizeof(node_t));\n        /* set values explicitly, alternative would be calloc() */\n        tree-&gt;left-&gt;val = val;\n        tree-&gt;left-&gt;left = NULL;\n        tree-&gt;left-&gt;right = NULL;\n      }\n    }\n    else\n    {\n      if (val &gt;= tree-&gt;val)\n      {\n        /* insert right */\n        if (tree-&gt;right != NULL)\n        {\n          insert(tree-&gt;right,val);\n        }\n        else\n        {\n          tree-&gt;right = (node_t *) malloc(sizeof(node_t));\n          /* set values explicitly, alternative would be calloc() */\n          tree-&gt;right-&gt;val = val;\n          tree-&gt;right-&gt;left = NULL;\n          tree-&gt;right-&gt;right = NULL;\n        }\n      }\n    }\n  }\n}\n\n/* depth-first search */\nvoid printDFS(node_t * current)\n{\n  /* change the code here */\n  if (current == NULL)         return;   /* security measure */\n  if (current != NULL)         printf(\"%d \", current-&gt;val);\n  if (current-&gt;left != NULL)   printDFS(current-&gt;left);\n  if (current-&gt;right != NULL)  printDFS(current-&gt;right);\n}\n</code></pre>"},{"location":"Languages/C/03%20-%20Pre-processor/01%20-%20define%20directives/","title":"Define Directives","text":"<p>Reference: devdocs.io</p>"},{"location":"Languages/C/03%20-%20Pre-processor/01%20-%20define%20directives/#syntax","title":"Syntax","text":"Syntax Version Since <code>#define</code> (1) <p>NOTE: You can find other versions within the reference material.</p>"},{"location":"Languages/C/03%20-%20Pre-processor/01%20-%20define%20directives/#explanation-and-examples","title":"Explanation and Examples","text":""},{"location":"Languages/C/03%20-%20Pre-processor/01%20-%20define%20directives/#version-1","title":"Version 1","text":"<p>The <code>#define</code> directives define the identifier as a macro, that is they instruct the compiler to replace all successive occurrences of identifier with replacement-list, which can be optionally additionally processed. If the identifier is already defined as any type of macro, the program is ill-formed unless the definitions are identical.</p> <pre><code>#define ERROR_VALUE -1\n\nint main()\n{\n    return ERROR_VALUE; // Returns -1\n}\n</code></pre>"},{"location":"Languages/C/03%20-%20Pre-processor/01%20-%20define%20directives/#nice-to-know","title":"Nice to know","text":""},{"location":"Languages/C/03%20-%20Pre-processor/01%20-%20define%20directives/#include-guards","title":"Include guards","text":"<p>Define directives are used to create include guards, these prevent multiple inclusions of a header file, which can happen if the file is included more than once in a program.</p> <pre><code>#ifndef COLLATZ_CONJECTURE_H\n#define COLLATZ_CONJECTURE_H\n\n#define ERROR_VALUE -1\n\nint steps(int start);\n\n#endif\n</code></pre>"},{"location":"Languages/C/03%20-%20Pre-processor/01%20-%20define%20directives/#defining-enums-with-the-help-of-the-define-directive","title":"Defining enums with the help of the define directive","text":"<pre><code>#ifndef RESISTOR_COLOR_DUO_H\n#define RESISTOR_COLOR_DUO_H\n#include &lt;stdint.h&gt;\n\n//               0      1     2      3       4      5      6     7       8     9\n#define COLORS BLACK, BROWN, RED, ORANGE, YELLOW, GREEN, BLUE, VIOLET, GREY, WHITE\n\ntypedef enum RESISTOR_BANDS { COLORS } resistor_band_t;\n\nuint16_t color_code(resistor_band_t color[]);\n\n#endif\n</code></pre>"},{"location":"Languages/C/03%20-%20Pre-processor/02%20-%20Conditional%20inclusion/","title":"Conditional inclusion","text":"<p>Reference: devdocs.io</p> <p>The preprocessor supports conditional compilation of parts of source file. This behaviour is controlled by <code>#if</code>, <code>#else</code>, <code>#elif</code>, <code>#ifdef</code>, <code>#ifndef</code>, <code>#elifdef</code>, <code>#elifndef</code>(since C23), and <code>#endif</code> directives.</p>"},{"location":"Languages/C/03%20-%20Pre-processor/02%20-%20Conditional%20inclusion/#syntax","title":"Syntax","text":"<code>#if</code> expression <code>#ifdef</code> identifier <code>#ifndef</code> identifier <code>#elif</code> expression <code>#elifdef</code> identifier (since C23) <code>#elifndef</code> identifier (since C23) <code>#else</code> <code>#endif</code>"},{"location":"Languages/C/03%20-%20Pre-processor/02%20-%20Conditional%20inclusion/#explanation","title":"Explanation","text":"<p>The conditional pre-processing block starts with <code>#if</code>, <code>#ifdef</code> or <code>#ifndef</code> directive, then optionally includes any number of <code>#elif</code>, <code>#elifdef</code>, or <code>#elifndef</code>(since C23) directives, then optionally includes at most one <code>#else</code> directive and is terminated with <code>#endif</code> directive. Any inner conditional pre-processing blocks are processed separately.</p> <p>Each of <code>#if</code>, <code>#ifdef</code>, <code>#ifndef</code>, <code>#elif</code>, <code>#elifdef</code>, <code>#elifndef</code>(since C23), and <code>#else</code> directives control code block until first <code>#elif</code>, <code>#elifdef</code>, <code>#elifndef</code>(since C23), <code>#else</code>, <code>#endif</code> directive not belonging to any inner conditional pre-processing blocks.</p> <p><code>#if</code>, <code>#ifdef</code> and <code>#ifndef</code> directives test the specified condition (see below) and if it evaluates to true, compiles the controlled code block. In that case subsequent <code>#else</code>, <code>#elifdef</code>, <code>#elifndef</code>,(since C23) and <code>#elif</code> directives are ignored. Otherwise, if the specified condition evaluates false, the controlled code block is skipped and the subsequent <code>#else</code>, <code>#elifdef</code>, <code>#elifndef</code>,(since C23) or <code>#elif</code> directive (if any) is processed. If the subsequent directive is <code>#else</code>, the code block controlled by the <code>#else</code> directive is unconditionally compiled. Otherwise, the <code>#elif</code>, <code>#elifdef</code>, or <code>#elifndef</code>(since C23) directive acts as if it was <code>#if</code> directive: checks for condition, compiles or skips the controlled code block based on the result, and in the latter case processes subsequent <code>#elif</code>, <code>#elifdef</code>, <code>#elifndef</code>,(since C23) and <code>#else</code> directives. The conditional pre-processing block is terminated by <code>#endif</code> directive.</p>"},{"location":"Languages/C/03%20-%20Pre-processor/02%20-%20Conditional%20inclusion/#example","title":"Example","text":"<pre><code>#define ABCD 2\n#include &lt;stdio.h&gt;\n\nint main(void)\n{\n\n#ifdef ABCD\n    printf(\"1: yes\\n\");\n#else\n    printf(\"1: no\\n\");\n#endif\n\n#ifndef ABCD\n    printf(\"2: no1\\n\");\n#elif ABCD == 2\n    printf(\"2: yes\\n\");\n#else\n    printf(\"2: no2\\n\");\n#endif\n\n#if !defined(DCBA) &amp;&amp; (ABCD &lt; 2 * 4 - 3)\n    printf(\"3: yes\\n\");\n#endif\n\n// C23 directives #elifdef/#elifndef\n#ifdef CPU\n    printf(\"4: no1\\n\");\n#elifdef GPU\n    printf(\"4: no2\\n\");\n#elifndef RAM\n    printf(\"4: yes\\n\"); // selected in C23 mode, may be selected in pre-C23 mode\n#else\n    printf(\"4: no3\\n\"); // may be selected in pre-C23 mode\n#endif\n}\n</code></pre>"},{"location":"Languages/C/03%20-%20Pre-processor/02%20-%20Conditional%20inclusion/#nice-to-know","title":"Nice to know","text":""},{"location":"Languages/C/03%20-%20Pre-processor/02%20-%20Conditional%20inclusion/#include-guards","title":"Include guards","text":"<p><code>#ifndef</code> is used to create include guards, these prevent multiple inclusions of a header file, which can happen if the file is included more than once in a program.</p> <pre><code>#ifndef COLLATZ_CONJECTURE_H\n#define COLLATZ_CONJECTURE_H\n\n#define ERROR_VALUE -1\n\nint steps(int start);\n\n#endif\n</code></pre>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Binary%20Search/","title":"Binary Search","text":"<p>You have stumbled upon a group of mathematicians who are also singer-songwriters. They have written a song for each of their favorite numbers, and, as you can imagine, they have a lot of favorite numbers (like [0][zero] or [73][seventy-three] or [6174][kaprekars-constant]).</p> <p>You are curious to hear the song for your favorite number, but with so many songs to wade through, finding the right song could take a while. Fortunately, they have organized their songs in a playlist sorted by the title \u2014 which is simply the number that the song is about.</p> <p>You realize that you can use a binary search algorithm to quickly find a song given the title.</p>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Binary%20Search/#instructions","title":"Instructions","text":"<p>Your task is to implement a binary search algorithm.</p> <p>A binary search algorithm finds an item in a list by repeatedly splitting it in half, only keeping the half which contains the item we're looking for. It allows us to quickly narrow down the possible locations of our item until we find it, or until we've eliminated all possible locations.</p> <p>Important</p> <p>Binary search only works when a list has been sorted.</p> <p>The algorithm looks like this:</p> <ul> <li>Find the middle element of a sorted list and compare it with the item we're looking for.</li> <li>If the middle element is our item, then we're done!</li> <li>If the middle element is greater than our item, we can eliminate that element and all the elements after it.</li> <li>If the middle element is less than our item, we can eliminate that element and all the elements before it.</li> <li>If every element of the list has been eliminated then the item is not in the list.</li> <li>Otherwise, repeat the process on the part of the list that has not been eliminated.</li> </ul> <p>Here's an example:</p> <p>Let's say we're looking for the number 23 in the following sorted list: <code>[4, 8, 12, 16, 23, 28, 32]</code>.</p> <ul> <li>We start by comparing 23 with the middle element, 16.</li> <li>Since 23 is greater than 16, we can eliminate the left half of the list, leaving us with <code>[23, 28, 32]</code>.</li> <li>We then compare 23 with the new middle element, 28.</li> <li>Since 23 is less than 28, we can eliminate the right half of the list: <code>[23]</code>.</li> <li>We've found our item.</li> </ul>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Binary%20Search/#solution","title":"Solution","text":"<pre><code>// binary_search.h\n#ifndef BINARY_SEARCH_H\n#define BINARY_SEARCH_H\n\n#include &lt;math.h&gt;\n#include &lt;stddef.h&gt;\n\nconst int *binary_search(int value, const int *arr, size_t length);\n\n#endif\n</code></pre> <pre><code>// binary_search.c\n#include \"binary_search.h\"\n\nconst int *binary_search(int value, const int *arr, size_t length)\n{\n    // Handle edge cases: NULL array or zero-length array\n    if (!arr || length == 0)\n    {\n        return NULL;\n    }\n\n    int start = 0;\n    int end = length - 1;\n\n    while (start &lt;= end)\n    {\n        int mid = start + (end - start) / 2;\n        if (arr[mid] == value)\n        {\n            return &amp;arr[mid];\n        }\n        else if (arr[mid] &lt; value)\n        {\n            start = mid + 1;\n        }\n        else\n        {\n            end = mid - 1;\n        }\n    }\n\n    return NULL; // Value not found\n}\n</code></pre>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Gigasecond/","title":"Gigasecond","text":""},{"location":"Languages/C/98%20-%20Exercises/Easy/Gigasecond/#introduction","title":"Introduction","text":"<p>The way we measure time is kind of messy. We have 60 seconds in a minute, and 60 minutes in an hour. This comes from ancient Babylon, where they used 60 as the basis for their number system. We have 24 hours in a day, 7 days in a week, and how many days in a month? Well, for days in a month it depends not only on which month it is, but also on what type of calendar is used in the country you live in.</p> <p>What if, instead, we only use seconds to express time intervals? Then we can use metric system prefixes for writing large numbers of seconds in more easily comprehensible quantities.</p> <ul> <li>A food recipe might explain that you need to let the brownies cook in the oven for two kiloseconds (that's two thousand seconds).</li> <li>Perhaps you and your family would travel to somewhere exotic for two megaseconds (that's two million seconds).</li> <li>And if you and your spouse were married for a thousand million seconds, you would celebrate your one gigasecond anniversary.</li> </ul> <pre><code>If we ever colonize Mars or some other planet, measuring time is going to get even messier.\nIf someone says \"year\" do they mean a year on Earth or a year on Mars?\n\nThe idea for this exercise came from the science fiction novel [\"A Deepness in the Sky\"][vinge-novel] by author Vernor Vinge.\nIn it the author uses the metric system as the basis for time measurements.\n\n[vinge-novel]: https://www.tor.com/2017/08/03/science-fiction-with-something-for-everyone-a-deepness-in-the-sky-by-vernor-vinge/\n</code></pre>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Gigasecond/#instructions","title":"Instructions","text":"<p>Your task is to determine the date and time one gigasecond after a certain date.</p> <p>A gigasecond is one thousand million seconds. That is a one with nine zeros after it.</p> <p>If you were born on January 24th, 2015 at 22:00 (10:00:00pm), then you would be a gigasecond old on October 2nd, 2046 at 23:46:40 (11:46:40pm).</p>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Gigasecond/#use-utc","title":"Use UTC","text":"<p>To solve this problem, you'll need to convert a <code>time_t</code> value into a <code>struct tm</code> value. The [<code>time.h</code>][https://en.cppreference.com/w/c/chrono] library gives you two functions to do this. Make sure you use the one that results in a calendar time expressed in UTC.</p>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Gigasecond/#solution","title":"Solution","text":"<pre><code>// giga_second.h\n#ifndef GIGASECOND_H\n#define GIGASECOND_H\n#define ONE_GIGA_SECOND 1000000000;\n\n#include &lt;time.h&gt;\n\nvoid gigasecond(time_t input, char *output, size_t size);\n\n#endif\n</code></pre> <pre><code>#include \"gigasecond.h\"\n\nvoid gigasecond(time_t input, char *output, size_t size)\n{\n    time_t giga_second_after_input = input + ONE_GIGA_SECOND;\n    struct tm *new_time = gmtime(&amp;giga_second_after_input);\n    strftime(output, size, \"%Y-%m-%d %H:%M:%S\", new_time);\n}\n</code></pre>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Isograms/","title":"Isograms","text":"<p>Determine if a word or phrase is an isogram.</p> <p>An isogram (also known as a \"non-pattern word\") is a word or phrase without a repeating letter, however spaces and hyphens are allowed to appear multiple times.</p> <p>Examples of isograms:</p> <ul> <li>lumberjacks</li> <li>background</li> <li>downstream</li> <li>six-year-old</li> </ul> <p>The word isograms, however, is not an isogram, because the s repeats.</p>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Isograms/#solution","title":"Solution","text":"<pre><code>// isogram.h\n#ifndef ISOGRAM_H\n#define ISOGRAM_H\n\n#include &lt;stdbool.h&gt;\n\nbool is_isogram(const char phrase[]);\n\n#endif\n</code></pre> <pre><code>// isogram.c\n#include \"isogram.h\"\n\nbool is_isogram(const char phrase[])\n{\n    // If phrase is null, it is not an isogram.\n    if (!phrase)\n    {\n        return false;\n    }\n\n    // Result array confirming how much of each letter was found.\n    int results[26] = {0};\n\n    // Loop through each character in the phrase.\n    for (int index = 0; phrase[index] != '\\0'; index++)\n    {\n        // Get ascii value of character.\n        int asciiValue = phrase[index];\n\n        // If Uppercase, convert to lowercase.\n        if (asciiValue &gt;= 'A' &amp;&amp; asciiValue &lt;= 'Z')\n        {\n            asciiValue += 32;\n        }\n\n        // If within acceptable characters, add one to the index in the results matching the alphabet order.\n        if (asciiValue &gt;= 'a' &amp;&amp; asciiValue &lt;= 'z')\n        {\n            int result_position = asciiValue - 'a';\n            results[result_position] += 1;\n        }\n    }\n\n    // Check results\n    for (int index = 0; index &lt; 26; index++)\n    {\n        // If any value occurs more than once, it is not an isogram.\n        if (results[index] != 0 &amp;&amp; results[index] != 1)\n        {\n            return false;\n        }\n    }\n\n    // If made this far, it is an isogram.\n    return true;\n}\n</code></pre>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Robot%20Simulator/","title":"Robot Simulator","text":"<p>Write a robot simulator.</p> <p>A robot factory's test facility needs a program to verify robot movements.</p> <p>The robots have three possible movements:</p> <ul> <li>turn right</li> <li>turn left</li> <li>advance</li> </ul> <p>Robots are placed on a hypothetical infinite grid, facing a particular direction (north, east, south, or west) at a set of {x,y} coordinates, e.g., {3,8}, with coordinates increasing to the north and east.</p> <p>The robot then receives a number of instructions, at which point the testing facility verifies the robot's new position, and in which direction it is pointing.</p> <ul> <li>The letter-string \"RAALAL\" means:</li> <li>Turn right</li> <li>Advance twice</li> <li>Turn left</li> <li>Advance once</li> <li>Turn left yet again</li> <li>Say a robot starts at {7, 3} facing north.   Then running this stream of instructions should leave it at {9, 4} facing west.</li> </ul>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Robot%20Simulator/#solution","title":"Solution","text":"<pre><code>// robot_simulator.h\n#ifndef ROBOT_SIMULATOR_H\n#define ROBOT_SIMULATOR_H\n\ntypedef enum {\n   DIRECTION_NORTH = 0,\n   DIRECTION_DEFAULT = DIRECTION_NORTH,\n   DIRECTION_EAST,\n   DIRECTION_SOUTH,\n   DIRECTION_WEST,\n   DIRECTION_MAX\n} robot_direction_t;\n\ntypedef struct {\n   int x;\n   int y;\n} robot_position_t;\n\ntypedef struct {\n   robot_direction_t direction;\n   robot_position_t position;\n} robot_status_t;\n\nrobot_status_t robot_create(robot_direction_t direction, int x, int y);\nvoid robot_move(robot_status_t *robot, const char *commands);\n\n#endif\n</code></pre> <pre><code>// robot_simulator.c\n#include \"robot_simulator.h\"\n\nrobot_status_t robot_create(robot_direction_t direction, int x, int y)\n{\n    robot_position_t position;\n    position.x = x;\n    position.y = y;\n\n    robot_status_t robot;\n    robot.direction = direction;\n    robot.position = position;\n\n    return robot;\n}\n\nvoid robot_move(robot_status_t *robot, const char *commands)\n{\n    for (int i = 0; commands[i] != '\\0'; i++)\n    {\n        if (commands[i] == 'R')\n        {\n            robot-&gt;direction++;\n        }\n\n        if (commands[i] == 'L')\n        {\n            if (robot-&gt;direction == DIRECTION_NORTH)\n            {\n                robot-&gt;direction = DIRECTION_WEST;\n            }\n            else\n            {\n                robot-&gt;direction--;\n            }\n        }\n\n        if (robot-&gt;direction == DIRECTION_MAX)\n        {\n            robot-&gt;direction = DIRECTION_NORTH;\n        }\n\n        if (commands[i] == 'A')\n        {\n            int operation = robot-&gt;direction == DIRECTION_SOUTH || robot-&gt;direction == DIRECTION_WEST ? -1 : 1;\n            if (robot-&gt;direction == DIRECTION_NORTH || robot-&gt;direction == DIRECTION_SOUTH)\n            {\n                robot-&gt;position.y += operation;\n            }\n            else\n            {\n                robot-&gt;position.x += operation;\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Triangle/","title":"Triangle","text":""},{"location":"Languages/C/98%20-%20Exercises/Easy/Triangle/#instructions","title":"Instructions","text":"<p>Determine if a triangle is equilateral, isosceles, or scalene.</p> <p>An equilateral triangle has all three sides the same length.</p> <p>An isosceles triangle has at least two sides the same length. (It is sometimes specified as having exactly two sides the same length, but for the purposes of this exercise we'll say at least two.)</p> <p>A scalene triangle has all sides of different lengths.</p>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Triangle/#note","title":"Note","text":"<p>For a shape to be a triangle at all, all sides have to be of length &gt; 0, and the sum of the lengths of any two sides must be greater than or equal to the length of the third side.</p> <p>In equations:</p> <p>Let <code>a</code>, <code>b</code>, and <code>c</code> be sides of the triangle. Then all three of the following expressions must be true:</p> <pre><code>a + b \u2265 c\nb + c \u2265 a\na + c \u2265 b\n</code></pre> <p>See Triangle Inequality</p>"},{"location":"Languages/C/98%20-%20Exercises/Easy/Triangle/#solution","title":"Solution","text":"<pre><code>// triangle.h\n#ifndef TRIANGLE_H\n#define TRIANGLE_H\n\n#include &lt;stdbool.h&gt;\n\ntypedef struct\n{\n   double a;\n   double b;\n   double c;\n} triangle_t;\n\nbool is_triangle(triangle_t triangle);\n\nbool is_equilateral(triangle_t triangle);\n\nbool is_isosceles(triangle_t triangle);\n\nbool is_scalene(triangle_t triangle);\n\n#endif\n</code></pre> <pre><code>// triangle.c\n#include \"triangle.h\"\n\nbool is_triangle(triangle_t triangle)\n{\n    bool valid_a_side = triangle.a &gt; 0 &amp;&amp; triangle.a &lt;= (triangle.b + triangle.c);\n    bool valid_b_side = triangle.b &gt; 0 &amp;&amp; triangle.b &lt;= (triangle.a + triangle.c);\n    bool valid_c_side = triangle.c &gt; 0 &amp;&amp; triangle.c &lt;= (triangle.a + triangle.b);\n\n    return valid_a_side &amp;&amp; valid_b_side &amp;&amp; valid_c_side;\n}\n\nbool is_equilateral(triangle_t triangle)\n{\n    if (!is_triangle(triangle))\n    {\n        return false;\n    }\n\n    return triangle.a == triangle.b &amp;&amp; triangle.a == triangle.c;\n}\n\nbool is_isosceles(triangle_t triangle)\n{\n    if (!is_triangle(triangle))\n    {\n        return false;\n    }\n\n    return triangle.a == triangle.b || triangle.a == triangle.c || triangle.b == triangle.c;\n}\n\nbool is_scalene(triangle_t triangle)\n{\n    if (!is_triangle(triangle))\n    {\n        return false;\n    }\n\n    return triangle.a != triangle.b &amp;&amp; triangle.a != triangle.c &amp;&amp; triangle.b != triangle.c;\n}\n</code></pre>"},{"location":"Languages/C/98%20-%20Exercises/Medium/Luhn/","title":"Luhn","text":""},{"location":"Languages/C/98%20-%20Exercises/Medium/Luhn/#instructions","title":"Instructions","text":"<p>Given a number determine whether or not it is valid per the Luhn formula.</p> <p>The [Luhn algorithm][luhn] is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers and Canadian Social Insurance Numbers.</p> <p>The task is to check if a given string is valid.</p>"},{"location":"Languages/C/98%20-%20Exercises/Medium/Luhn/#validating-a-number","title":"Validating a Number","text":"<p>Strings of length 1 or less are not valid. Spaces are allowed in the input, but they should be stripped before checking. All other non-digit characters are disallowed.</p>"},{"location":"Languages/C/98%20-%20Exercises/Medium/Luhn/#example-1-valid-credit-card-number","title":"Example 1: valid credit card number","text":"<pre><code>4539 3195 0343 6467\n</code></pre> <p>The first step of the Luhn algorithm is to double every second digit, starting from the right. We will be doubling</p> <pre><code>4539 3195 0343 6467\n\u2191 \u2191  \u2191 \u2191  \u2191 \u2191  \u2191 \u2191  (double these)\n</code></pre> <p>If doubling the number results in a number greater than 9 then subtract 9 from the product. The results of our doubling:</p> <pre><code>8569 6195 0383 3437\n</code></pre> <p>Then sum all of the digits:</p> <pre><code>8+5+6+9+6+1+9+5+0+3+8+3+3+4+3+7 = 80\n</code></pre> <p>If the sum is evenly divisible by 10, then the number is valid. This number is valid!</p>"},{"location":"Languages/C/98%20-%20Exercises/Medium/Luhn/#example-2-invalid-credit-card-number","title":"Example 2: invalid credit card number","text":"<pre><code>8273 1232 7352 0569\n</code></pre> <p>Double the second digits, starting from the right</p> <pre><code>7253 2262 5312 0539\n</code></pre> <p>Sum the digits</p> <pre><code>7+2+5+3+2+2+6+2+5+3+1+2+0+5+3+9 = 57\n</code></pre> <p>57 is not evenly divisible by 10, so this number is not valid.</p>"},{"location":"Languages/C/98%20-%20Exercises/Medium/Luhn/#based-on","title":"Based on","text":"<p>The Luhn Algorithm on Wikipedia - https://en.wikipedia.org/wiki/Luhn_algorithm</p>"},{"location":"Languages/C/98%20-%20Exercises/Medium/Luhn/#solution","title":"Solution","text":"<pre><code>// luhn.h\n#ifndef LUHN_H\n#define LUHN_H\n\n#include &lt;stdbool.h&gt;\n#include &lt;ctype.h&gt;\n#include &lt;string.h&gt;\n\nint char_to_int(char num);\n\nbool is_valid_luhn_number(char num);\n\nbool luhn(const char *num);\n\n#endif\n</code></pre> <pre><code>// luhn.c\n#include \"luhn.h\"\n\nint char_to_int(char num)\n{\n    return num - '0';\n}\n\nbool is_valid_luhn_number(char num)\n{\n    return num &gt;= '0' &amp;&amp; num &lt;= '9';\n}\n\nbool luhn(const char *num)\n{\n    // If length of value provided equal or less than 1, it is invalid.\n    int len_of_id = strlen(num);\n    if (len_of_id &lt;= 1)\n    {\n        return false;\n    }\n\n    // Total of the luhn algorithm, value should be divisible by 10 to be valid.\n    int total = 0;\n\n    // Which character we are on from the end of the string.\n    int counter = 0;\n\n    // Number of spaces\n    int num_spaces = 0;\n\n    // Loop from end of string to start of string.\n    for (int index = len_of_id - 1; index &gt;= 0; index--)\n    {\n        char value = num[index];\n\n        // Ignore spaces\n        if (value == ' ')\n        {\n            num_spaces += 1;\n            continue;\n        }\n\n        // Validate value is valid number\n        if (!is_valid_luhn_number(value))\n        {\n            return false;\n        }\n\n        // Determine how much we need to plus to total based on location in string.\n        int increment = char_to_int(value);\n        if (counter % 2 == 1)\n        {\n            increment = increment * 2;\n            increment = increment &gt; 9 ? increment - 9 : increment;\n        }\n\n        // Increment counter and adjust total\n        counter += 1;\n        total += increment;\n    }\n\n    // Invalid ID with spaces.\n    if ((len_of_id - num_spaces) &lt;= 1)\n    {\n        return false;\n    }\n\n    // Is valid luhn?\n    return total % 10 == 0;\n}\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/","title":"Common Imports","text":"<ul> <li>Booleans:  <code>#include &lt;stdbool.h&gt;</code></li> <li>Integers:  <code>#include &lt;stdint.h&gt;</code></li> <li>Input/Output: <code>#include &lt;stdio.h&gt;</code></li> <li>Strings: <code>#include &lt;string.h&gt;</code></li> <li>Standard Lib: <code>#include &lt;stdlib.h&gt;</code></li> </ul>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#integers","title":"Integers","text":"<ul> <li><code>unsigned int</code>: Number that does not allow negatives.</li> <li><code>int</code>: Number that does allow negatives.</li> <li><code>uint64_t</code>: Signed integers with different size variations (8, 16, 32, 64) (Requires: <code>&lt;stdint.h&gt;</code>)</li> </ul>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#header-files","title":"Header Files","text":"<p>You will usually have two files when creating a C project, your C source files  (<code>hello_world.c</code>) and its corresponding header file (<code>hello_world.h</code>).  You will need to import your header file in your C file like so: <code>#include \"hello_world.h\"</code>.</p>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#difference-between-file-imports-and-lib-imports","title":"Difference between file imports and lib imports","text":"<p>Lib: <code>#include &lt;stdbool.h&gt;</code> File: <code>#include \"hello_world.h\"</code></p>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#include-guards","title":"Include Guards","text":"<p>Include guards ensures that the header is only included once.</p> <p>Example:</p> <pre><code>#ifndef HELLO_WORLD_H\n#define HELLO_WORLD_H\n\nconst char *hello(void);\n\n#endif\n</code></pre> <p>If you file is named <code>hello_world.h</code> it seems it is required/best practice to define the include guard as <code>#ifndef HELLO_WORLD_H</code>.</p>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#defining-values","title":"Defining values","text":"<p>You can also define values within header files like so:</p> <pre><code>#define ERROR_VALUE -1\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#enums","title":"Enums","text":"<p>Usually defined within header files:</p> <pre><code>typedef enum {\n    BLACK = 0,\n    BROWN = 1,\n    RED = 2,\n    ORANGE = 3,\n    YELLOW = 4,\n    GREEN = 5,\n    BLUE = 6,\n    VIOLET = 7,\n    GREY = 8,\n    WHITE = 9\n} resistor_band_t;\n</code></pre> <p>Could also be defined like:</p> <pre><code>#define COLORS BLACK, BROWN, RED, ORANGE, YELLOW, GREEN, BLUE, VIOLET, GREY, WHITE\n\ntypedef enum RESISTOR_BANDS { COLORS } resistor_band_t;\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#pointers","title":"Pointers","text":""},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#string-pointers","title":"String pointers:","text":"<pre><code>char * name = \"John\";\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#dereferencing","title":"Dereferencing:","text":"<pre><code>int a = 1;\nint * pointer_to_a = &amp;a;\n\n/* let's change the variable a */\na += 1;\n\n/* we just changed the variable again! */\n*pointer_to_a += 1;\n\n/* will print out 3 */\nprintf(\"The value of a is now %d\\n\", a);\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#function-arguments-by-reference","title":"Function arguments by reference","text":"<pre><code>void addone(int *n) {\n    // n is a pointer here which point to a memory-adress outside the function scope\n    (*n)++; // this will effectively increment the value of n\n}\n\nint n;\nprintf(\"Before: %d\\n\", n);\naddone(&amp;n);\nprintf(\"After: %d\\n\", n);\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#structures","title":"Structures","text":"<pre><code>struct point {\n    int x;\n    int y;\n};\n\n/* draws a point at 10, 5 */\nstruct point p;\np.x = 10;\np.y = 5;\ndraw(p);\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#typedefs","title":"Typedefs","text":"<pre><code>typedef struct {\n    int x;\n    int y;\n} point;\n\npoint p;\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#pointers-in-structures","title":"Pointers in structures","text":"<pre><code>typedef struct {\n    char * brand;\n    int model;\n} vehicle;\n\nvehicle mycar;\nmycar.brand = \"Ford\";\nmycar.model = 2007;\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#dynamic-allocation","title":"Dynamic Allocation","text":"<pre><code>typedef struct {\n    char * name;\n    int age;\n} person;\n\nperson * myperson = (person *) malloc(sizeof(person)); // Memory allocation\n\nmyperson-&gt;name = \"John\";\nmyperson-&gt;age = 27;\n\nfree(myperson); // Free the memory\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/01%20-%20General/#dynamic-allocation-for-arrays","title":"Dynamic Allocation for arrays","text":"<pre><code>int nrows = 2;\nint ncols = 5;\nint i, j;\n\n// Allocate memory for nrows pointers\nchar **pvowels = (char **) malloc(nrows * sizeof(char *));\n\n// For each row, allocate memory for ncols elements\npvowels[0] = (char *) malloc(ncols * sizeof(char));\npvowels[1] = (char *) malloc(ncols * sizeof(char));\n\npvowels[0][0] = 'A';\npvowels[0][1] = 'E';\npvowels[0][2] = 'I';\npvowels[0][3] = 'O';\npvowels[0][4] = 'U';\n\npvowels[1][0] = 'a';\npvowels[1][1] = 'e';\npvowels[1][2] = 'i';\npvowels[1][3] = 'o';\npvowels[1][4] = 'u';\n\nfor (i = 0; i &lt; nrows; i++) {\n    for(j = 0; j &lt; ncols; j++) {\n        printf(\"%c \", pvowels[i][j]);\n    }\n\n    printf(\"\\n\");\n}\n\n// Free individual rows\nfree(pvowels[0]);\nfree(pvowels[1]);\n\n// Free the top-level pointer\nfree(pvowels);\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/02%20-%20Strings/","title":"Strings","text":""},{"location":"Languages/C/99%20-%20Notes/02%20-%20Strings/#combining-two-strings-concatenating-two-strings","title":"Combining two strings (Concatenating two strings)","text":"<pre><code>#include &lt;string.h&gt;\nchar actual[16] = {0};\nstrcat(actual, \"Ping\");\nstrcat(actual, \"Pong\");\n\n// Actual = \"PingPong\"\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/02%20-%20Strings/#adding-integers-into-strings-and-other-data-types","title":"Adding integers into strings (and other data types)","text":"<pre><code>#include &lt;stdio.h&gt;\nchar actual[16] = {0};\nsnprintf(actual, 16, \"%d\", 202);\n\n// Actual = \"202\"\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/03%20-%20Random/","title":"Using random values","text":""},{"location":"Languages/C/99%20-%20Notes/03%20-%20Random/#random-integer-in-range","title":"Random integer in range","text":"<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nsrand(time(NULL));\nint min = 1;\nint max = 6;\nint random_number = (rand() % (max - min + 1)) + min;\n</code></pre>"},{"location":"Languages/C/99%20-%20Notes/04%20-%20Math/","title":"Math","text":""},{"location":"Languages/C/99%20-%20Notes/04%20-%20Math/#floor","title":"Floor","text":"<pre><code>#include &lt;math.h&gt;\ndouble floorTo3 = floor(3.1);\n</code></pre>"},{"location":"Languages/CSharp/01%20-%20Getting%20Started/","title":"Getting Started with C#","text":"<p>To get started with C#, I would highly recommend using docker as it is much simpler to setup then C# itself on your local machine unless you are using visual studio on windows.</p>"},{"location":"Languages/CSharp/01%20-%20Getting%20Started/#install-dependencies-for-the-net-runtime","title":"Install dependencies for the .NET runtime","text":"<pre><code>sudo apt update &amp;&amp; sudo apt install ca-certificates libc6 libgcc-s1 libicu74 liblttng-ust1 libssl3 libstdc++6 libunwind8 zlib1g\n</code></pre>"},{"location":"Languages/CSharp/01%20-%20Getting%20Started/#installing-the-net-sdk","title":"Installing the .NET SDK","text":"<pre><code>sudo apt-get update &amp;&amp; sudo apt-get install dotnet-sdk-8.0\n</code></pre>"},{"location":"Languages/CSharp/01%20-%20Getting%20Started/#starting-a-simple-c-project","title":"Starting a simple C# project","text":"<p>Start a simple console application with C#.</p> <pre><code>dotnet new console -o Project\n</code></pre> <pre><code>dotnet restore # For dependency installation\ndotnet run # To run the application (without build)\ndotnet build # For building the application\n</code></pre>"},{"location":"Languages/CSharp/01%20-%20Getting%20Started/#starting-an-aspnet-project","title":"Starting an ASP.NET project","text":"<p>Start API project with C# using the following command:</p> <pre><code>dotnet new webapi -o Play\n</code></pre>"},{"location":"Languages/CSharp/01%20-%20Getting%20Started/#using-docker-with-an-aspnet-project","title":"Using docker with an ASP.NET project","text":"<pre><code>FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build\n\nWORKDIR /src\n\nCOPY [\"Program.cs\", \"Play.csproj\", \"appsettings.json\", \"/src/\"]\nRUN dotnet restore \"./Play.csproj\"\n\nARG BUILD_CONFIGURATION=Release\nRUN dotnet build \"./Play.csproj\" -c $BUILD_CONFIGURATION -o /app/build/\nRUN dotnet publish \"./Play.csproj\" -c $BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false\n\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final\n\nWORKDIR /app\nEXPOSE 8080\n\nCOPY --from=build /app/publish .\nENTRYPOINT [ \"dotnet\", \"Play.dll\" ]\n</code></pre> <pre><code>services:\n  server:\n    build: .\n    ports:\n      - 8000:8080\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/01%20-%20Hello%20World/","title":"Hello, World!","text":"<p>C# is a static programming language which lets us write code that runs atop Microsoft's .NET framework. C# has more features and capabilities than Java, but since it is owned by Microsoft, it means that you may find yourself paying money for your IDE, server operating system, etc.</p> <p>C# is object oriented and its syntax is very similar to Java.</p> <p>In our tutorial, we will be using the <code>System.Console.WriteLine</code> function to write lines to the output console. When writing a C# command line application, the same function can also be used to print to the console.</p> <p>Note: Writing web applications using C# is possible using the ASP.NET framework.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/01%20-%20Hello%20World/#exercise","title":"Exercise","text":"<p>Print \"Hello, World!\" to the console.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/01%20-%20Hello%20World/#given","title":"Given","text":"<pre><code>using System;\n\npublic class Hello\n{\n    public static void Main()\n    {\n        System.Console.WriteLine(\"Goodbye, World!\");\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/01%20-%20Hello%20World/#solution","title":"Solution","text":"<pre><code>using System;\n\npublic class Hello\n{\n    public static void Main()\n    {\n        System.Console.WriteLine(\"Hello, World!\");\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/","title":"Variables and Types","text":"<p>C# is a statically-typed language. Therefore, we must define the types of variables before using them.</p> <p>To define a variable in C#, we use the following syntax, which is similar to C / Java:</p> <pre><code>int myInt = 1;\nfloat myFloat = 1f;\nbool myBoolean = true;\nstring myName = \"John\";\nchar myChar = 'a';\ndouble myDouble = 1.75;\n</code></pre> <p>Notice that defining a floating point number requires an explicit <code>f</code> letter after the number.</p> <p>C# supports type inference - which means that you don't always have to explicitly specify a type - you can let the compiler try and understand the type of variable automatically. However, once the type of variable has been determined, it cannot be assigned a different type.</p> <pre><code>var x = 1;\nvar y = 2;\nvar sum = x + y;    // sum will also be defined as an integer\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#enums","title":"Enums","text":"<p>Enums are integers that should be used when an integer is used to specify an option from a fixed amount of options.</p> <pre><code>public enum CarType\n{\n    Toyota = 1,\n    Honda = 2,\n    Ford = 3,\n}\n\npublic class Tutorial\n{\n    public static void Main()\n    {\n        CarType myCarType = CarType.Toyota;\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#exercise","title":"Exercise","text":"<p>Define three variables:</p> <ul> <li>A string named <code>productName</code> equal to <code>TV</code>.</li> <li>An integer named <code>productYear</code> equal to 2012.</li> <li>A float named <code>productPrice</code> equal to 279.99f.</li> </ul>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#given","title":"Given","text":"<pre><code>using System;\n\npublic class Tutorial\n{\n   public static void Main()\n   {\n      // write your code here\n\n      // test code\n      Console.WriteLine(\"productName: \" + productName);\n      Console.WriteLine(\"productYear: \" + productYear);\n      Console.WriteLine(\"productPrice: \" + productPrice);\n\n   }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#solution","title":"Solution","text":"<pre><code>using System;\n\npublic class Tutorial\n{\n    public static void Main()\n    {\n        // write your code here\n        string productName = \"TV\";\n        int productYear = 2012;\n        float productPrice = 279.99f;\n\n        // test code\n        Console.WriteLine(\"productName: \" + productName);\n        Console.WriteLine(\"productYear: \" + productYear);\n        Console.WriteLine(\"productPrice: \" + productPrice);\n\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/03%20-%20Type%20Conversion/","title":"Type Conversion","text":"<p>C# types are not the same! In some cases you have to convert a value's type.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/03%20-%20Type%20Conversion/#automatically","title":"Automatically","text":"<p>Usually C# functions convert argument's value type automatically. There you don't have to do anything.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/03%20-%20Type%20Conversion/#manually","title":"Manually","text":"<p>Sometimes C# function don't convert value types. Here, you have to do it manually. There are two methods:</p> <ul> <li>By reassigning the value to a new variable: <code>int myVar = (int) 1.0;</code></li> <li>By using methods: <code>string myVar = Convert.ToString(10);</code> Note: <code>Convert</code> uses the <code>ToInt32</code> for int conversion.</li> </ul>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/03%20-%20Type%20Conversion/#exercise","title":"Exercise","text":"<p>Convert <code>myDBL</code> to int and print it.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/03%20-%20Type%20Conversion/#given","title":"Given","text":"<pre><code>using System;\n\npublic class Conversion\n{\n    public static void Main()\n    {\n        double myDBL = 99.0;\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/03%20-%20Type%20Conversion/#solution","title":"Solution","text":"<pre><code>using System;\npublic class Conversion\n{\n    public static void Main()\n    {\n        double myDBL = 99.0;\n        Console.WriteLine((int) myDBL);  \n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/04%20-%20Conditionals/","title":"Conditionals","text":"<p>C# uses Boolean variables to evaluate conditions. The Boolean values <code>true</code> and <code>false</code> are returned when an expression is compared or evaluated. For example:</p> <pre><code>int a = 4;\nbool b = a == 4;\n\nif (b) {\n    Console.WriteLine(\"It's true!\");\n}\n</code></pre> <p>Of course we don't normally assign a conditional expression to a bool, we just use the short version:</p> <pre><code>int a = 4;\n\nif (a == 4) {\n    Console.WriteLine(\"Ohhh! So a is 4!\");\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/04%20-%20Conditionals/#boolean-operators","title":"Boolean operators","text":"<p>There aren't that many operators to use in conditional statements and most of them are pretty straight forward:</p> <pre><code>int a = 4;\nint b = 5;\nbool result;\nresult = a &lt; b; // true\nresult = a &gt; b; // false\nresult = a &lt;= 4; // a smaller or equal to 4 - true\nresult = b &gt;= 6; // b bigger or equal to 6 - false\nresult = a == b; // a equal to b - false\nresult = a != b; // a is not equal to b - true\nresult = a &gt; b || a &lt; b; // Logical or - true\nresult = 3 &lt; a &amp;&amp; a &lt; 6; // Logical and - true\nresult = !result; // Logical not - false\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/04%20-%20Conditionals/#if-else-and-between","title":"if - else and between","text":"<p>The if, else statement in C# is pretty simple.</p> <pre><code>if (a == b) {\n    // a and b are equal, let's do something cool\n}\n</code></pre> <p>And we can also add an else statement after an if, to do something if the condition is not true</p> <pre><code>if (a == b) {\n    // We already know this part\n} else {\n    // a and b are not equal... :/\n}\n</code></pre> <p>The if - else statements doesn't have to be in several lines with {}, if can be used in one line, or without the {}, for a single line statment.</p> <pre><code>if (a == b)\n    Console.WriteLine(\"Another line Wow!\");\nelse\n    Console.WriteLine(\"Double rainbow!\");\n</code></pre> <p>Although this method might be useful for making your code shorter by using fewer lines, we strongly recommend for beginners not to use this short version of statements and always use the full version with {}. This goes to every statement that can be shorted to a single line (for, while, etc).</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/04%20-%20Conditionals/#exercise","title":"Exercise","text":"<p>In this exercise, you must construct an <code>if</code> statement that checks if the number <code>guess</code> is equal to 500. If that is the case, use <code>Console.WriteLine</code> to display \"Success!\".</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/04%20-%20Conditionals/#given","title":"Given","text":"<pre><code>using System;\n\npublic class Conditionals\n{\n    public static void Main()\n    {\n        int guess = 500;\n\n        // Write conditional here\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/04%20-%20Conditionals/#solution","title":"Solution","text":"<pre><code>using System;\n\npublic class Conditionals\n{\n    public static void Main()\n    {\n        int guess = 500;\n\n        if (guess == 500)\n        {\n            Console.WriteLine(\"Success!\");\n        }\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/05%20-%20Arrays/","title":"Arrays","text":"<p>Arrays in C# are very similar to arrays in C. They are defined using the brackets operator <code>[]</code>, and they are initialized using a list defined with curly braces. For example:</p> <pre><code>int[] nums = { 1, 2, 3, 4, 5 };\n</code></pre> <p>We can also define an empty array like this:</p> <pre><code>int[] nums = new int[10];\n</code></pre> <p>To get the size of the array, use the <code>Length</code> method.</p> <pre><code>int[] nums = new int[10];\nConsole.WriteLine(nums.Length);\n</code></pre> <p>To access a specific item in the array, we use the brackets operator:</p> <pre><code>int[] nums = new int[10];\nint firstNumber = nums[0];\nint secondNumber = nums[1];\nnums[2] = 10;\n</code></pre> <p>Notice that C# uses zero based indices.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/05%20-%20Arrays/#multidimensional-arrays","title":"Multidimensional arrays","text":"<p>C# supports multidimensional arrays, defined in the following manner:</p> <pre><code>int[,] matrix = new int[2,2];\n\nmatrix[0,0] = 1;\nmatrix[0,1] = 2;\nmatrix[1,0] = 3;\nmatrix[1,1] = 4;\n\nint[,] predefinedMatrix = new int[2,2] { { 1, 2 }, { 3, 4 } };\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/05%20-%20Arrays/#jagged-arrays","title":"Jagged arrays","text":"<p>These are multidimensional arrays where each subarray is an independent array - Can have subarrays of different lengths. Use a separate set of square brackets for each dimension of the array.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/05%20-%20Arrays/#jagarray1274","title":"jagArray1<code>[2][7][4]</code>","text":"<p>Unlike other types of arrays, you cannot fully instantiate a jagged array in a single step. Since a jagged array is an array of independent arrays, each array must be created separately.</p> <p>For example, the following code shows the declaration, instantiation, and initialization of a twodimensional jagged array. </p> <p>Notice in the code that the reference to each subarray is assigned to an element in the top-level array. </p> <pre><code>int[][] Arr = new int[3][]; \nArr[0] = new int[] {10, 20, 30}; \nArr[1] = new int[] {40, 50, 60, 70}; \nArr[2] = new int[] {80, 90, 100, 110, 120};\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/05%20-%20Arrays/#exercise","title":"Exercise","text":"<p>Define an array called <code>fruits</code> that holds the following strings: \"apple\", \"banana\", and \"orange\".</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/05%20-%20Arrays/#given","title":"Given","text":"<pre><code>using System;\n\npublic class Tutorial\n{\n   public static void Main()\n   {\n      // write your code here\n\n      // test code\n      Console.WriteLine(fruits[0]);\n      Console.WriteLine(fruits[1]);\n      Console.WriteLine(fruits[2]);\n   }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/05%20-%20Arrays/#solution","title":"Solution","text":"<pre><code>using System;\n\npublic class Tutorial\n{\n   public static void Main()\n   {\n      string[] fruits = { \"apple\", \"banana\", \"orange\" };\n\n      // test code\n      Console.WriteLine(fruits[0]);\n      Console.WriteLine(fruits[1]);\n      Console.WriteLine(fruits[2]);\n   }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/06%20-%20Lists/","title":"Lists","text":"<p>Lists in C# are very similar to lists in Java. A list is an object which holds variables in a specific order. The type of variable that the list can store is defined using the generic syntax. Here is an example of defining a list called <code>numbers</code> which holds integers.</p> <pre><code>List&lt;int&gt; numbers = new List&lt;int&gt;();\n</code></pre> <p>The difference between a list and an array is that lists are dynamic sized, while arrays have a fixed size. When you do not know the amount of variables your array should hold, use a list instead.</p> <p>Once the list is initialized, we can begin inserting numbers into the list.</p> <pre><code>List&lt;int&gt; numbers = new List&lt;int&gt;();\nnumbers.Add(1);\nnumbers.Add(2);\nnumbers.Add(3);\n</code></pre> <p>We can also add a whole array to a list using the <code>AddRange</code> function:</p> <pre><code>List&lt;int&gt; numbers = new List&lt;int&gt;();\nint[] array = new int[] { 1, 2, 3 };\nnumbers.AddRange(array);\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/06%20-%20Lists/#removing-from-a-list","title":"Removing from a list","text":"<p>We can use <code>Remove</code> to remove an item from a list by specifying the item we want to remove.</p> <pre><code>List&lt;string&gt; fruits = new List&lt;string&gt;();\n// add fruits\nfruits.Add(\"apple\");\nfruits.Add(\"banana\");\nfruits.Add(\"orange\");\n\n// now remove the banana\nfruits.Remove(\"banana\");\nConsole.WriteLine(fruits.Count);\n</code></pre> <p>We can also use <code>RemoveAt</code> to specify an index of an item to remove. In our case, to remove the banana, we will use the index 1.</p> <pre><code>List&lt;string&gt; fruits = new List&lt;string&gt;();\n// add fruits\nfruits.Add(\"apple\");\nfruits.Add(\"banana\");\nfruits.Add(\"orange\");\n\n// now remove the banana\nfruits.RemoveAt(1);\nConsole.WriteLine(fruits.Count);\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/06%20-%20Lists/#concatenating-lists","title":"Concatenating lists","text":"<p>We can use <code>AddRange</code> to join between lists.</p> <pre><code>List&lt;string&gt; food = new List&lt;string&gt;();\nfood.Add(\"apple\");\nfood.Add(\"banana\");\n\nList&lt;string&gt; vegetables = new List&lt;string&gt;();\nvegetables.Add(\"tomato\");\nvegetables.Add(\"carrot\");\n\nfood.AddRange(vegetables);\nConsole.WriteLine(food.Count);\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/06%20-%20Lists/#exercise","title":"Exercise","text":"<p>Construct a list of the first 5 prime numbers (2, 3, 5, 7, 11) called <code>primeNumbers</code>.</p>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/06%20-%20Lists/#given","title":"Given","text":"<pre><code>using System;\nusing System.Collections.Generic;\n\npublic class Hello\n{\n    public static void Main()\n    {\n        // TODO: add your code here\n\n        // test code\n        Console.WriteLine(primeNumbers.Count);\n        Console.WriteLine(primeNumbers[0]);\n        Console.WriteLine(primeNumbers[1]);\n        Console.WriteLine(primeNumbers[2]);\n        Console.WriteLine(primeNumbers[3]);\n        Console.WriteLine(primeNumbers[4]);\n\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/06%20-%20Lists/#solution","title":"Solution","text":"<pre><code>using System;\nusing System.Collections.Generic;\n\npublic class Hello\n{\n    public static void Main()\n    {\n        List&lt;int&gt; primeNumbers = new List&lt;int&gt;();\n        int[] array = new int[] { 2, 3, 5, 7, 11 };\n        primeNumbers.AddRange(array);\n\n        // test code\n        Console.WriteLine(primeNumbers.Count);\n        Console.WriteLine(primeNumbers[0]);\n        Console.WriteLine(primeNumbers[1]);\n        Console.WriteLine(primeNumbers[2]);\n        Console.WriteLine(primeNumbers[3]);\n        Console.WriteLine(primeNumbers[4]);\n\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/07%20-%20Dictionaries/","title":"Dictionaries","text":"<p>Dictionaries are special lists, whereas every value in the list has a key which is also a variable. A good example for a dictionary is a phone book.</p> <pre><code>Dictionary&lt;string, long&gt; phonebook = new Dictionary&lt;string, long&gt;();\nphonebook.Add(\"Alex\", 4154346543);\nphonebook[\"Jessica\"] = 4159484588;\n</code></pre> <p>Notice that when defining a dictionary, we need to provide a generic definition with two types - the type of the key and the type of the value. In this case, the key is a string whereas the value is an integer.</p> <p>There are also two ways of adding a single value to the dictionary, either using the brackets operator or using the <code>Add</code> method.</p> <p>To check whether a dictionary has a certain key in it, we can use the <code>ContainsKey</code> method:</p> <pre><code>Dictionary&lt;string, long&gt; phonebook = new Dictionary&lt;string, long&gt;();\nphonebook.Add(\"Alex\", 415434543);\nphonebook[\"Jessica\"] = 415984588;\n\nif (phonebook.ContainsKey(\"Alex\"))\n{\n    Console.WriteLine(\"Alex's number is \" + phonebook[\"Alex\"]);\n}\n</code></pre> <p>To remove an item from a dictionary, we can use the <code>Remove</code> method. Removing an item from a dictionary by its key is fast and very efficient. When removing an item from a <code>List</code> using its value, the process is slow and inefficient, unlike the dictionary <code>Remove</code> function.</p> <pre><code>Dictionary&lt;string, long&gt; phonebook = new Dictionary&lt;string, long&gt;();\nphonebook.Add(\"Alex\", 415434543);\nphonebook[\"Jessica\"] = 415984588;\n\nphonebook.Remove(\"Jessica\");\nConsole.WriteLine(phonebook.Count);\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/07%20-%20Dictionaries/#exercise","title":"Exercise","text":"<p>Create a new dictionary called <code>inventory</code> that holds 3 names of fruits, and the amount they are in stock.</p> <p>Here is the inventory specification:</p> <ul> <li>3 of type <code>apple</code></li> <li>5 of type <code>orange</code></li> <li>2 of type <code>banana</code></li> </ul>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/07%20-%20Dictionaries/#given","title":"Given","text":"<pre><code>using System;\nusing System.Collections.Generic;\n\npublic class Hello\n{\n    public static void Main()\n    {\n        // TODO: add the inventory dictionary here\n\n        Console.WriteLine(inventory[\"apple\"]);\n        Console.WriteLine(inventory[\"orange\"]);\n        Console.WriteLine(inventory[\"banana\"]);\n\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/02%20-%20Fundamentals/07%20-%20Dictionaries/#solution","title":"Solution","text":"<pre><code>using System;\nusing System.Collections.Generic;\n\npublic class Hello\n{\n    public static void Main()\n    {\n        Dictionary&lt;string, int&gt; inventory = new Dictionary&lt;string, int&gt;();\n        inventory[\"apple\"] = 3;\n        inventory[\"orange\"] = 5;\n        inventory[\"banana\"] = 2;\n\n        Console.WriteLine(inventory[\"apple\"]);\n        Console.WriteLine(inventory[\"orange\"]);\n        Console.WriteLine(inventory[\"banana\"]);\n\n    }\n}\n</code></pre>"},{"location":"Languages/CSharp/03%20-%20Redis/01%20-%20Getting%20Started/","title":"Getting started with Redis using CSharp","text":"<p>Before we can use Redis, we need to spin up a instance of it. </p> <p>We can do this using docker and docker compose.</p>"},{"location":"Languages/CSharp/03%20-%20Redis/01%20-%20Getting%20Started/#setting-up-project","title":"Setting up project","text":"<p>To start a basic project, you can do the following:</p> <pre><code>dotnet new console -o play-csharp-redis\n</code></pre> <p>Once your project is created, install the Redis dependency/packages:</p> <pre><code>cd play-csharp-redis\ndotnet add package StackExchange.Redis\n</code></pre>"},{"location":"Languages/CSharp/03%20-%20Redis/01%20-%20Getting%20Started/#compose-stack","title":"Compose Stack","text":"<p>You can create a <code>compose.yaml</code> file that looks like the below to setup Redis:</p> <pre><code>services:\n  redis:\n    image: redis:7.4.1-alpine3.20\n    container_name: redis-container\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n    networks:\n      - redis-network\n\nvolumes:\n  redis-data:\n    external: false\n\nnetworks:\n  redis-network:\n    driver: bridge\n</code></pre> <p>You can start the compose stack with the following command: <code>docker compose up -d</code>.</p>"},{"location":"Languages/CSharp/03%20-%20Redis/01%20-%20Getting%20Started/#code","title":"Code","text":"<pre><code>// Program.cs\nusing StackExchange.Redis;\n\nConnectionMultiplexer cache = ConnectionMultiplexer.Connect(\"localhost\");\nIDatabase cacheDB = cache.GetDatabase();\ncacheDB.StringSet(\"Test\", \"Value\", TimeSpan.FromSeconds(3));\n\nbool exists = cacheDB.KeyExists(\"Test\");\nConsole.WriteLine(exists);\n\nThread.Sleep(5000);\n\nexists = cacheDB.KeyExists(\"Test\");\nConsole.WriteLine(exists);\n</code></pre> <p>This little script will create a key value pair with an expiry time of 3 seconds and check if it exists right after creation, and then check again after 5 seconds to see if it still exists, which it shouldn't.</p> <p>The output will look like the following:</p> <pre><code>True\nFalse\n</code></pre> <p>You can run the code with <code>dotnet run</code>.</p>"},{"location":"Languages/CSharp/03%20-%20Redis/02%20-%20PubSub%20with%20redis/","title":"PubSub with redis","text":"<p>Another common use of redis is as a pub/sub message distribution tool; this is also simple, and in the event of connection failure, the <code>ConnectionMultiplexer</code> will handle all the details of re-subscribing to the requested channels.</p> <pre><code>ISubscriber sub = redis.GetSubscriber();\n</code></pre>"},{"location":"Languages/CSharp/03%20-%20Redis/02%20-%20PubSub%20with%20redis/#subscribing","title":"Subscribing","text":"<p>Again, the object returned from <code>GetSubscriber</code> is a cheap pass-thru object that does not need to be stored. The pub/sub API has no concept of databases, but as before we can optionally provide an async-state. Note that all subscriptions are global: they are not scoped to the lifetime of the <code>ISubscriber</code> instance. The pub/sub features in redis use named \u201cchannels\u201d; channels do not need to be defined in advance on the server (an interesting use here is things like per-user notification channels, which is what drives parts of the realtime updates on Stack Overflow). As is common in .NET, subscriptions take the form of callback delegates which accept the channel-name and the message:</p> <pre><code>sub.Subscribe(\"messages\", (channel, message) =&gt; {\n    Console.WriteLine((string)message);\n});\n</code></pre> <p>Note: exceptions are caught and discarded by StackExchange.Redis he, to prevent cascadingre failures. To handle failures, use a <code>try</code>/<code>catch</code> inside your handler to do as you wish with any exceptions.</p> <p>In v2, you can subscribe without providing a callback directly to the <code>Subscribe()</code> method, and instead using the returned <code>ChannelMessageQueue</code>, which represents a message queue of ordered pub/sub notifications. This allows the usage of the <code>ChannelMessageQueue.OnMessage()</code> method, which provides overloads for both synchronous (<code>Action&lt;ChannelMessage&gt;</code>) and asynchronous (<code>Func&lt;ChannelMessage, Task&gt;</code>) handlers to execute when receiving a message.</p> <pre><code>// Synchronous handler\nsub.Subscribe(\"messages\").OnMessage(channelMessage =&gt; {\n    Console.WriteLine((string) channelMessage.Message);\n});\n\n// Asynchronous handler\nsub.Subscribe(\"messages\").OnMessage(async channelMessage =&gt; {\n    await Task.Delay(1000);\n    Console.WriteLine((string) channelMessage.Message);\n});\n</code></pre>"},{"location":"Languages/CSharp/03%20-%20Redis/02%20-%20PubSub%20with%20redis/#publishing","title":"Publishing","text":"<p>Separately (and often in a separate process on a separate machine) you can publish to this channel:</p> <pre><code>sub.Publish(\"messages\", \"hello\");\n</code></pre> <p>This will (virtually instantaneously) write <code>\"hello\"</code> to the console of the subscribed process. As before, both channel-names and messages can be binary.</p> <p>Please also see Pub / Sub Message Order for guidance on sequential versus concurrent message processing.</p>"},{"location":"Languages/PHP/01%20-%20Getting%20Started/","title":"Getting Started with PHP","text":"<p>To get started with PHP, I would highly recommend using docker as it is much simpler to setup then PHP itself on your local machine.</p> <p>It will also be good to use composer for dependency management.</p>"},{"location":"Languages/PHP/01%20-%20Getting%20Started/#setting-up-php-and-composer","title":"Setting up PHP and composer","text":"<pre><code>sudo apt install php-common php-cli\nwget https://getcomposer.org/download/2.8.2/composer.phar\nsudo mv composer.phar /usr/local/bin/composer # Or any PATH directory\nsudo chmod +x /usr/local/bin/composer\n</code></pre>"},{"location":"Languages/PHP/01%20-%20Getting%20Started/#initialize-your-project-with-composer","title":"Initialize your project with composer","text":"<pre><code>composer init\n</code></pre> <pre><code>  Welcome to the Composer config generator\n\n\n\nThis command will guide you through creating your composer.json config.\n\nPackage name (&lt;vendor&gt;/&lt;name&gt;) [animalalpaca/learning-php]: evanlab02/getting-started\nDescription []:\nAuthor [Evanlab02 &lt;evanlabuschagne70@gmail.com&gt;, n to skip]:\nMinimum Stability []:\nPackage Type (e.g. library, project, metapackage, composer-plugin) []: project\nLicense []:\n\nDefine your dependencies.\n\nWould you like to define your dependencies (require) interactively [yes]? no\nWould you like to define your dev dependencies (require-dev) interactively [yes]? no\nAdd PSR-4 autoload mapping? Maps namespace \"Evanlab02\\GettingStarted\" to the entered relative path. [src/, n to skip]:\n\n{\n    \"name\": \"evanlab02/getting-started\",\n    \"type\": \"project\",\n    \"autoload\": {\n        \"psr-4\": {\n            \"Evanlab02\\\\GettingStarted\\\\\": \"src/\"\n        }\n    },\n    \"authors\": [\n        {\n            \"name\": \"Evanlab02\",\n            \"email\": \"evanlabuschagne70@gmail.com\"\n        }\n    ],\n    \"require\": {}\n}\n\nDo you confirm generation [yes]?\nGenerating autoload files\nGenerated autoload files\n</code></pre>"},{"location":"Languages/PHP/01%20-%20Getting%20Started/#creating-your-first-php-file","title":"Creating your first .php file","text":"<p>Create a folder <code>public</code> and put this file into as <code>index.php</code>.</p> <pre><code>// public/index.php\n&lt;?php $user = \"World\"; ?&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;/head&gt;\n&lt;body&gt;\nHello &lt;?php echo $user; ?&gt;!\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Languages/PHP/01%20-%20Getting%20Started/#setting-up-dockerfile","title":"Setting up Dockerfile","text":"<pre><code># Dockerfile\nFROM php:8.3.13-apache AS final\n\n# Setup project\nRUN mv \"$PHP_INI_DIR/php.ini-production\" \"$PHP_INI_DIR/php.ini\"\nCOPY ./public/index.php /var/www/html/\n\n# Set appropriate permissions\nRUN chown -R www-data:www-data /var/www/html\n\n# Expose port 80\nEXPOSE 80\n\n# Set user to www-data\nUSER www-data\n</code></pre>"},{"location":"Languages/PHP/01%20-%20Getting%20Started/#setting-up-compose-file","title":"Setting up compose file","text":"<pre><code># compose.yaml\nservices:\n  server:\n    build: .\n    ports:\n      - 8000:80\n</code></pre>"},{"location":"Languages/PHP/01%20-%20Getting%20Started/#running-it","title":"Running it","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/01%20-%20Hello%20World/","title":"Hello, World!","text":"<p>PHP is the most commonly used programming language for the web today. PHP is very common because it has a relatively simple architecture compared to other MVC based web frameworks (Python, Ruby, node.js, etc).</p> <p>Unlike the standard web frameworks, a PHP file is actually an \"enhanced\" HTML file, which is also capable of executing code inside a document. So for example, you may start with a simple HTML page which looks like this:</p> <pre><code>&lt;html&gt;\n&lt;head&gt;&lt;/head&gt;\n&lt;body&gt;\nHello!\n&lt;?php ?&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>And later on add a PHP section which executes PHP code, and writes the output as HTML. Notice that the PHP line disappeared when executing, since the PHP code is replaced by the output.</p> <p>Let's try adding the name of the user's name.</p> <pre><code>&lt;?php $user = \"John\"; ?&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;/head&gt;\n&lt;body&gt;\nHello &lt;?php echo $user; ?&gt;!\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>On this tutorial however, we will focus on learning PHP as a programming language and not web development. Therefore, we will not use HTML at all, and focus on writing code as opposed to rendering web pages.</p> <p>In our tutorials, we will always open and close a PHP tag (starting with <code>&lt;?php</code> and ending with <code>?&gt;</code>) in the beginning and the end of our code.</p> <p>For testing our code, we are able to print messages to our console using the <code>echo</code> command.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/01%20-%20Hello%20World/#exercise","title":"Exercise","text":"<p>Print \"Hello, World!\" to the console.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/01%20-%20Hello%20World/#given","title":"Given","text":"<pre><code>&lt;?php\necho \"Goodbye, World!\";\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/01%20-%20Hello%20World/#solution","title":"Solution","text":"<pre><code>&lt;?php\necho \"Hello, World!\";\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/","title":"Variables and Types","text":"<p>To define a variable, simply use the following syntax:</p> <pre><code>$x = 1;\n$y = \"foo\";\n$z = True;\n</code></pre> <p>We have just defined a variable named <code>x</code> with the number 1, a variable named <code>y</code> with the string \"foo\" and a variable name <code>z</code> with the boolean value True. Once they are defined, we can use them in the code.</p> <p>PHP has many types of variables, but the most basic variable types are integer (whole numbers), float (real numbers), strings, and booleans.</p> <p>PHP also has arrays and objects which we will explain in other tutorials.</p> <p>Variables can also be set to NULL, which means that the variables exist, but do not contain any value.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#operators","title":"Operators","text":"<p>We can use simple arithmetic operators to add, subtract or concatenate between variables.</p> <p>We can also print out PHP variables using the <code>echo</code> command (you can try it out now).</p> <p>For example, let's sum up two numbers, put the result in a new variable, and print out the result.</p> <pre><code>$x = 1;\n$y = 2;\n$sum = $x + $y;\necho $sum;       // prints out 3\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#string-formatting","title":"String formatting","text":"<p>Like Perl, PHP double quoted strings can format strings using defined variables. For example:</p> <pre><code>$name = \"Jake\";\necho \"Your name is $name\";    // prints out Your name is Jake\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#exercise","title":"Exercise","text":""},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#part-1","title":"Part 1","text":"<p>Define the variables <code>name</code> and <code>age</code> so that a line would be printed out saying the following sentence:</p> <p><code>Hello Jake. You are 20 years old.</code></p> <p>Notice that the code contains a special character sequence at the end called a newline - <code>\\n</code>. This sequence will cause the next line printed out to be printed out on the next line. For web development, this is not important, since we use HTML tags for this purpose.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#part-2","title":"Part 2","text":"<p>Sum up the variables x and y and put the result in the sum variable.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#given","title":"Given","text":"<pre><code>&lt;?php\n// Part 1: add the name and age variables.\necho \"Hello $name. You are $age years old.\\n\";\n\n// Part 2: sum up the variables x and y and\n// put the result in the sum variable.\n$x = 195793;\n$y = 256836;\n$sum = NULL;\n\necho \"The sum of $x and $y is $sum.\"\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/02%20-%20Variables%20and%20Types/#solution","title":"Solution","text":"<pre><code>&lt;?php\n// Part 1: add the name and age variables.\n$name = \"Jake\";\n$age = 20;\necho \"Hello $name. You are $age years old.\\n\";\n\n// Part 2: sum up the variables x and y and\n// put the result in the sum variable.\n$x = 195793;\n$y = 256836;\n$sum = $x + $y;\n\necho \"The sum of $x and $y is $sum.\"\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/","title":"Simple arrays","text":"<p>Arrays are a special type of variable that can contain many variables, and hold them in a list.</p> <p>For example, let's say we want to create a list of all the odd numbers between 1 and 10. Once we create the list, we can assign new variables that will refer to a variable in the array, using the index of the variable.</p> <p>To use the first variable in the list (in this case the number 1), we will need to give the first index, which is 0, since PHP uses zero based indices, like almost all programming languages today.</p> <pre><code>$odd_numbers = [1,3,5,7,9];\n$first_odd_number = $odd_numbers[0];\n$second_odd_number = $odd_numbers[1];\n\necho \"The first odd number is $first_odd_number\\n\";\necho \"The second odd number is $second_odd_number\\n\";\n</code></pre> <p>We can now add new variables using an index. To add an item to the end of the list, we can assign the array with index 5 (the 6th variable):</p> <pre><code>$odd_numbers = [1,3,5,7,9];\n$odd_numbers[5] = 11;\nprint_r($odd_numbers);\n</code></pre> <p>Arrays can contain different types of variables according to your needs, and can even contain other arrays or objects as members.</p> <p>To delete an item from an array, use the <code>unset</code> function on the member itself. For example:</p> <pre><code>$odd_numbers = [1,3,5,7,9];\nunset($odd_numbers[2]); // will remove the 3rd item (5) from the list\nprint_r($odd_numbers);\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#useful-functions","title":"Useful functions","text":"<p>The <code>count</code> function returns the number of members an array has.</p> <pre><code>$odd_numbers = [1,3,5,7,9];\necho count($odd_numbers);\n</code></pre> <p>The <code>reset</code> function gets the first member of the array. (It also resets the internal iteration pointer).</p> <pre><code>$odd_numbers = [1,3,5,7,9];\n$first_item = reset($odd_numbers);\necho $first_item;\n</code></pre> <p>We can also use the index syntax to get the first member of the array, as follows:</p> <pre><code>$odd_numbers = [1,3,5,7,9];\n$first_item = $odd_numbers[0];\necho $first_item;\n</code></pre> <p>The <code>end</code> function gets the last member of the array.</p> <pre><code>$odd_numbers = [1,3,5,7,9];\n$last_item = end($odd_numbers);\necho $last_item;\n</code></pre> <p>We can also use the <code>count</code> function to get the number of elements in the list, and then use it to refer to the last variable in the array. Note that we subtract one from the last index because indices are zero based in PHP, so we need to fix the fact that we don't count variable number zero.</p> <pre><code>$odd_numbers = [1,3,5,7,9];\n$last_index = count($odd_numbers) - 1;\n$last_item = $odd_numbers[$last_index];\necho $last_item;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#stack-and-queue-functions","title":"Stack and queue functions","text":"<p>Arrays can be used as stacks and queues as well.</p> <p>To push a member to the end of an array, use the <code>array_push</code> function:</p> <pre><code>$numbers = [1,2,3];\narray_push($numbers, 4); // now array is [1,2,3,4];\n\n// print the new array\nprint_r($numbers);\n</code></pre> <p>To pop a member from the end of an array, use the <code>array_pop</code> function:</p> <pre><code>$numbers = [1,2,3,4];\narray_pop($numbers); // now array is [1,2,3];\n\n// print the new array\nprint_r($numbers);\n</code></pre> <p>To push a member to the beginning of an array, use the <code>array_unshift</code> function:</p> <pre><code>$numbers = [1,2,3];\narray_unshift($numbers, 0); // now array is [0,1,2,3];\n\n// print the new array\nprint_r($numbers);\n</code></pre> <p>To pop a member from the beginning of an array, use the <code>array_shift</code> function:</p> <pre><code>$numbers = [0,1,2,3];\narray_shift($numbers); // now array is [1,2,3];\n\n// print the new array\nprint_r($numbers);\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#concatenating-arrays","title":"Concatenating arrays","text":"<p>We can use the <code>array_merge</code> to concatenate between two arrays:</p> <pre><code>$odd_numbers = [1,3,5,7,9];\n$even_numbers = [2,4,6,8,10];\n$all_numbers = array_merge($odd_numbers, $even_numbers);\nprint_r($all_numbers);\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#sorting-arrays","title":"Sorting arrays","text":"<p>We can use the <code>sort</code> function to sort arrays. The <code>rsort</code> function sorts arrays in reverse. Notice that sorting is done on the input array and does not return a new array.</p> <pre><code>$numbers = [4,2,3,1,5];\nsort($numbers);\nprint_r($numbers);\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#advanced-array-functions","title":"Advanced array functions","text":"<p>The <code>array_slice</code> function returns a new array that contains a certain part of a specific array from an offset. For example, if we want to discard the first 3 elements of an array, we can do the following:</p> <pre><code>$numbers = [1,2,3,4,5,6];\nprint_r(array_slice($numbers, 3));\n</code></pre> <p>We can also decide to take a slice of a specific length. For example, if we want to take only two items, we can add another argument to the function:</p> <pre><code>$numbers = [1,2,3,4,5,6];\nprint_r(array_slice($numbers, 3, 2));\n</code></pre> <p>The <code>array_splice</code> function does exactly the same, however it will also remove the slice returned from the original array (in this case, the <code>numbers</code> variable).</p> <pre><code>$numbers = [1,2,3,4,5,6];\nprint_r(array_splice($numbers, 3, 2));\nprint_r($numbers);\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#exercise","title":"Exercise","text":"<ol> <li>Create a new array which contains the even numbers 2,4,6,8 and 10. The name of the new array should be <code>$even_numbers</code>.</li> <li>Concatenate the <code>male_names</code> and <code>female_names</code> arrays to create the <code>names</code> array.</li> </ol>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#given","title":"Given","text":"<pre><code>&lt;?php\n\n// TODO: add the even_numbers array here\n\n$male_names = [\"Jake\", \"Eric\", \"John\"];\n$female_names = [\"Jessica\", \"Beth\", \"Sandra\"];\n\n// TODO: join the male and female names to one array\n$names = NULL;\n\nprint_r($even_numbers);\nprint_r($names);\n\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/03%20-%20Simple%20arrays/#solution","title":"Solution","text":"<pre><code>&lt;?php\n\n$even_numbers = [2, 4, 6, 8, 10];\n$male_names = [\"Jake\", \"Eric\", \"John\"];\n$female_names = [\"Jessica\", \"Beth\", \"Sandra\"];\n\n$names = array_merge($male_names, $female_names);\n\nprint_r($even_numbers);\nprint_r($names);\n\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/04%20-%20Arrays%20with%20Keys/","title":"Arrays with keys","text":"<p>PHP arrays are actually ordered maps, meaning that all values of arrays have keys, and the items inside the array preserve order. When using arrays as simple lists as we have seen last chapter, a zero based counter is used to set the keys. Each item which is added to the array increments the next index by 1.</p> <p>A good example for using arrays with keys is a phone book. Let's say we want to save the phone numbers of people in a class.</p> <pre><code>$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n];\n\nprint_r($phone_numbers);\necho \"Alex's phone number is \" . $phone_numbers[\"Alex\"] . \"\\n\";\necho \"Jessica's phone number is \" . $phone_numbers[\"Jessica\"] . \"\\n\";\n</code></pre> <p>To add an item to an array using a key, we use the brackets operator, as you would expect.</p> <pre><code>$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n];\n\n$phone_numbers[\"Michael\"] = \"415-955-3857\";\n\nprint_r($phone_numbers);\n</code></pre> <p>To check if a key exists within an array, we can use the <code>array_key_exists</code> function:</p> <pre><code>$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n];\n\nif (array_key_exists(\"Alex\", $phone_numbers)) {\n    echo \"Alex's phone number is \" . $phone_numbers[\"Alex\"] . \"\\n\";\n} else {\n    echo \"Alex's phone number is not in the phone book!\";\n}\n\nif (array_key_exists(\"Michael\", $phone_numbers)) {\n    echo \"Michael's phone number is \" . $phone_numbers[\"Michael\"] . \"\\n\";\n} else {\n    echo \"Michael's phone number is not in the phone book!\";\n}\n</code></pre> <p>If we want to extract only the keys of the array (the names), we can use the <code>array_keys</code> function.</p> <pre><code>$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n];\n\nprint_r(array_keys($phone_numbers));\n</code></pre> <p>Alternatively, to get only the values of an array (the phone numbers), we can use the <code>array_values</code> function.</p> <pre><code>$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n];\n\nprint_r(array_values($phone_numbers));\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/04%20-%20Arrays%20with%20Keys/#exercise","title":"Exercise","text":"<p>Add a number to the phone book for Eric, with the number 415-874-7659, either by adding it to the array definition, or as a separate code line.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/04%20-%20Arrays%20with%20Keys/#given","title":"Given","text":"<pre><code>&lt;?php\n$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n];\n\nprint_r($phone_numbers);\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/04%20-%20Arrays%20with%20Keys/#solution","title":"Solution","text":"<pre><code>&lt;?php\n$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n  \"Eric\" =&gt; \"415-874-7659\",\n];\n\nprint_r($phone_numbers);\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/05%20-%20Multidimensional%20Arrays/","title":"Multidimensional arrays","text":"<p>Let's continue with the topic of arrays. There is another type of array - multidimensional array. This type of array may contain another array as a value for a specific index:</p> <pre><code>$multiArray = [ \n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n];\n</code></pre> <p>Now, if we print the value with the index <code>0</code>, it will be an array:</p> <pre><code>print_r($multiArray[0])\n\n// Array\n// (\n//     [0] =&gt; 1\n//     [1] =&gt; 2\n//     [2] =&gt; 3\n// )\n</code></pre> <p>so that we can get the value from any index from the inner array, and that will be the number:</p> <pre><code>print_r($multiArray[0][0]) // 1\nprint_r($multiArray[0][1]) // 2\nprint_r($multiArray[0][2]) // 3\n</code></pre> <p>We can also create an associative multidimensional array:</p> <pre><code>$people = [\n    \"john_doe\" =&gt; [\n        \"name\" =&gt; \"John\",\n        \"surname\" =&gt; \"Doe\",\n        \"age\" =&gt; 25,\n    ],\n    \"jane_doe\" =&gt; [\n        \"name\" =&gt; \"Jane\",\n        \"surname\" =&gt; \"Doe\",\n        \"age\" =&gt; 25,\n    ]\n];\n\nprint_r($people);\n\n// Array\n// (\n//     [john_doe] =&gt; Array\n//         (\n//             [name] =&gt; John\n//             [surname] =&gt; Doe\n//             [age] =&gt; 25\n//         )\n//     [jane_doe] =&gt; Array\n//         (\n//             [name] =&gt; Jane\n//             [surname] =&gt; Doe\n//             [age] =&gt; 25\n//         )\n// )\n\nprint_r($people['john_doe']['name']); // John\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/05%20-%20Multidimensional%20Arrays/#exercise","title":"Exercise","text":"<p>Complete the code below to sum all the numbers in all inner arrays and print the result in a new line.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/05%20-%20Multidimensional%20Arrays/#given","title":"Given","text":"<pre><code>&lt;?php\n\n$matrix = [ \n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n];\n\nforeach ($matrix as $numbers) {\n    $sum = 0;\n\n    // TODO: Write you'r code here.\n\n    echo \"{$sum}\\n\";\n}\n\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/05%20-%20Multidimensional%20Arrays/#solution","title":"Solution","text":"<pre><code>&lt;?php\n\n$matrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n];\n\nforeach ($matrix as $numbers) {\n    $sum = 0;\n\n    foreach ($numbers as $number) {\n        $sum += $number;\n    }\n\n    echo \"{$sum}\\n\";\n}\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/06%20-%20Strings/","title":"Strings","text":"<p>Strings are variables that hold text. For example, a string which contains a name is defined as follows:</p> <pre><code>$name = \"John\";\necho $name;\n</code></pre> <p>We can easily format strings using variables. For example:</p> <pre><code>$name = \"John\";\n$introduction = \"Hello $name\";\necho $introduction;\n</code></pre> <p>We can also concatenate strings using the dot <code>.</code> operator. For example:</p> <pre><code>$first_name = \"John\";\n$last_name = \"Doe\";\n$name = $first_name . \" \" . $last_name;\necho $name;\n</code></pre> <p>To measure the length of a string, we use the <code>strlen</code> function:</p> <pre><code>$string = \"The length of this string is 43 characters.\";\necho strlen($string);\n</code></pre> <p>To cut a part of a string and return it as a new string, we can use the <code>substr</code> function:</p> <pre><code>$filename = \"image.png\";\n$extension = substr($filename, strlen($filename) - 3);\necho \"The extension of the file is $extension\";\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/06%20-%20Strings/#joining-and-splitting","title":"Joining and splitting","text":"<p>We can join arrays to form strings, or split strings to arrays of strings.</p> <p>For example, to split a string with a list of fruits separated by a comma, we use the <code>explode</code> function:</p> <pre><code>$fruits = \"apple,banana,orange\";\n$fruit_list = explode(\",\", $fruits);\necho \"The second fruit in the list is $fruit_list[1]\";\n</code></pre> <p>To join back an array to a single string separated with commas, we use the <code>implode</code> function:</p> <pre><code>$fruit_list = [\"apple\",\"banana\",\"orange\"];\n$fruits = implode(\",\", $fruit_list);\necho \"The fruits are $fruits\";\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/06%20-%20Strings/#exercise","title":"Exercise","text":"<p>Split string that contains the list of numbers into a new array called number_list.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/06%20-%20Strings/#given","title":"Given","text":"<pre><code>&lt;?php\n$numbers = \"38,42,58,48,33,59,87,17,20,8,98,14,62,66,14,62,97,66,74,78,66,2,79,29,72,6,3,71,46,68,48,4,12,52,66,48,14,39,63,69,81,61,21,77,10,44,39,82,19,77,100,98,53,95,30,17,30,96,68,47,81,52,82,11,13,83,10,14,49,96,27,73,42,76,71,15,81,36,77,38,17,2,29,100,26,86,22,18,38,64,82,51,39,7,88,53,82,30,98,86\";\n\n// TODO: split the $numbers variable to an array\n// called $number_list\n\nprint_r($number_list);\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/06%20-%20Strings/#solution","title":"Solution","text":"<pre><code>&lt;?php\n$numbers = \"38,42,58,48,33,59,87,17,20,8,98,14,62,66,14,62,97,66,74,78,66,2,79,29,72,6,3,71,46,68,48,4,12,52,66,48,14,39,63,69,81,61,21,77,10,44,39,82,19,77,100,98,53,95,30,17,30,96,68,47,81,52,82,11,13,83,10,14,49,96,27,73,42,76,71,15,81,36,77,38,17,2,29,100,26,86,22,18,38,64,82,51,39,7,88,53,82,30,98,86\";\n\n$number_list = explode(\",\", $numbers);\n\nprint_r($number_list);\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/07%20-%20For%20Loops/","title":"For loops","text":"<p>For loops are simple loops which helps us iterate over an iterable variable by using an index. There are two types of for loops - a simple (C style) for loop, and a foreach loop.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/07%20-%20For%20Loops/#for-loop","title":"For loop","text":"<p>For loops are very useful when we need to iterate over an array and refer to member of the array using a changing index. For example, let's say we have a list of odd numbers. To print them out, we need to refer to each item individually. The code we write in the for loop can use the index <code>i</code>, which changes in every iteration of the for loop.</p> <pre><code>$odd_numbers = [1,3,5,7,9];\nfor ($i = 0; $i &lt; count($odd_numbers); $i=$i+1) {\n    $odd_number = $odd_numbers[$i];\n    echo $odd_number . \"\\n\";\n}\n</code></pre> <p>The first line of the for loop defines 3 parts:</p> <ul> <li>The initialization statement - in our case, we initialize the iterator variable <code>$i</code> to 0.</li> <li>The condition statement - this statement gets evaluated in every loop. The loop stops when this condition is unmet. This will happen when the iterator variable <code>$i</code> will be larger than the length of the array.</li> <li>The increment statement - this statement is executed every iteration to increase the index variable by the needed amount. Usually, we will increase <code>$i</code> by 1. There are two shorter ways of increasing a variable by 1 as well. We can use <code>$i+=1</code> or <code>$i++</code> as well.</li> </ul>"},{"location":"Languages/PHP/02%20-%20Fundamentals/07%20-%20For%20Loops/#foreach-loop","title":"Foreach loop","text":"<p>The foreach loop iterates over an iterable element such as an array or an object, providing the members in a specific variable one at a time.</p> <p>For example, let's say we want to create a list of all the odd numbers between 1 and 10, and print them out one by one, like in the previous example. This time, we will be using the <code>foreach</code> statement instead of a regular <code>for</code> loop with an iterator variable. Instead of using the iterator variable as an index to the array, we get the item from the array directly into the <code>$odd_number</code> variable.</p> <pre><code>$odd_numbers = [1,3,5,7,9];\nforeach ($odd_numbers as $odd_number) {\n  echo $odd_number . \"\\n\";\n}\n</code></pre> <p>When iterating over arrays with keys, we can use the following syntax:</p> <pre><code>$phone_numbers = [\n  \"Alex\" =&gt; \"415-235-8573\",\n  \"Jessica\" =&gt; \"415-492-4856\",\n];\n\nforeach ($phone_numbers as $name =&gt; $number) {\n  echo \"$name's number is $number.\\n\";\n}\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/07%20-%20For%20Loops/#exercise","title":"Exercise","text":"<p>Print out all numbers inside the array, one by one, using the <code>\\n</code> newline character sequence to separate between results.</p>"},{"location":"Languages/PHP/02%20-%20Fundamentals/07%20-%20For%20Loops/#given","title":"Given","text":"<pre><code>&lt;?php\n$even_numbers = [2,4,6,8,10,12,14,16,18,20];\n\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/02%20-%20Fundamentals/07%20-%20For%20Loops/#solution","title":"Solution","text":"<pre><code>&lt;?php\n$even_numbers = [2,4,6,8,10,12,14,16,18,20];\n\nforeach ($even_numbers as $number) {\n    echo \"$number\\n\";\n}\n</code></pre>"},{"location":"Languages/PHP/03%20-%20Postgres/001%20-%20Connecting%20to%20postgres/","title":"Connecting to postgres with PHP","text":"<pre><code>pg_connect(string $connection_string): PgSql\\Connection | False\n</code></pre> <p><code>pg_connect()</code> opens a connection to a PostgreSQL database specified by the <code>connection_string</code>.</p> <p>If a second call is made to <code>pg_connect()</code> with the same <code>connection_string</code> as an existing connection, the existing connection will be returned unless you pass <code>PGSQL_CONNECT_FORCE_NEW</code> as <code>flags</code>.</p> <p>The old syntax with multiple parameters <code>$conn = pg_connect(\"host\", \"port\", \"options\", \"tty\", \"dbname\")</code> has been deprecated.</p>"},{"location":"Languages/PHP/03%20-%20Postgres/001%20-%20Connecting%20to%20postgres/#parameters","title":"Parameters","text":""},{"location":"Languages/PHP/03%20-%20Postgres/001%20-%20Connecting%20to%20postgres/#connection_string","title":"<code>connection_string</code>","text":"<p>The <code>connection_string</code> can be empty to use all default parameters, or it can contain one or more parameter settings separated by whitespace. Each parameter setting is in the form <code>keyword = value</code>. Spaces around the equal sign are optional. To write an empty value or a value containing spaces, surround it with single quotes, e.g., <code>keyword = 'a value'</code>. Single quotes and backslashes within the value must be escaped with a backslash.</p> <p>The currently recognized parameter keywords are: <code>host</code>, <code>hostaddr</code>, <code>port</code>, <code>dbname</code> (defaults to value of <code>user</code>), <code>user</code>, <code>password</code>, <code>connect_timeout</code>, <code>options</code>, <code>tty</code> (ignored), <code>sslmode</code>, <code>requiressl</code> (deprecated in favor of <code>sslmode</code>), and <code>service</code>. Which of these arguments exist depends on your PostgreSQL version.</p> <p>The <code>options</code> parameter can be used to set command line parameters to be invoked by the server.</p>"},{"location":"Languages/PHP/03%20-%20Postgres/001%20-%20Connecting%20to%20postgres/#examples","title":"Examples","text":"<pre><code>&lt;?php\n$dbconn = pg_connect(\"dbname=mary\");\n//connect to a database named \"mary\"\n\n$dbconn2 = pg_connect(\"host=localhost port=5432 dbname=mary\");\n// connect to a database named \"mary\" on \"localhost\" at port \"5432\"\n\n$dbconn3 = pg_connect(\"host=sheep port=5432 dbname=mary user=lamb password=foo\");\n//connect to a database named \"mary\" on the host \"sheep\" with a username and password\n\n$conn_string = \"host=sheep port=5432 dbname=test user=lamb password=bar\";\n$dbconn4 = pg_connect($conn_string);\n//connect to a database named \"test\" on the host \"sheep\" with a username and password\n\n$dbconn5 = pg_connect(\"host=localhost options='--client_encoding=UTF8'\");\n//connect to a database on \"localhost\" and set the command line parameter which tells the encoding is in UTF-8\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/03%20-%20Postgres/001%20-%20Connecting%20to%20postgres/#see-also","title":"See Also","text":"<ul> <li>pg_pconnect() - Open a persistent PostgreSQL connection</li> <li>pg_close() - Closes a PostgreSQL connection</li> <li>pg_host() - Returns the host name associated with the connection</li> <li>pg_port() - Return the port number associated with the connection</li> <li>pg_tty() - Return the TTY name associated with the connection</li> <li>pg_options() - Get the options associated with the connection</li> <li>pg_dbname() - Get the database name</li> </ul>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/","title":"pg_prepare","text":"<p>(PHP 5 &gt;= 5.1.0, PHP 7, PHP 8)</p> <p>pg_prepare \u2014 Submits a request to the server to create a prepared statement with the given parameters, and waits for completion</p>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#description","title":"Description","text":"<p>pg_prepare() creates a prepared statement for later execution with pg_execute() or pg_send_execute(). This feature allows commands that will be used repeatedly to be parsed and planned just once, rather than each time they are executed. pg_prepare() is supported only against PostgreSQL 7.4 or higher connections; it will fail when using earlier versions.</p> <p>The function creates a prepared statement named <code>stmtname</code> from the <code>query</code> string, which must contain a single SQL command. <code>stmtname</code> may be <code>\"\"</code> to create an unnamed statement, in which case any pre-existing unnamed statement is automatically replaced; otherwise it is an error if the statement name is already defined in the current session. If any parameters are used, they are referred to in the <code>query</code> as <code>$1</code>, <code>$2</code>, etc.</p> <p>Prepared statements for use with pg_prepare() can also be created by executing SQL <code>PREPARE</code> statements. (But pg_prepare() is more flexible since it does not require parameter types to be pre-specified.) Also, although there is no PHP function for deleting a prepared statement, the SQL <code>DEALLOCATE</code> statement can be used for that purpose.</p>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#parameters","title":"Parameters","text":""},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#connection","title":"<code>connection</code>","text":"<p>An PgSql\\Connection instance. When <code>connection</code> is unspecified, the default connection is used. The default connection is the last connection made by pg_connect() or pg_pconnect().</p> <p>Warning</p> <p>As of PHP 8.1.0, using the default connection is deprecated.</p>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#stmtname","title":"<code>stmtname</code>","text":"<p>The name to give the prepared statement. Must be unique per-connection. If <code>\"\"</code> is specified, then an unnamed statement is created, overwriting any previously defined unnamed statement.</p>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#query","title":"<code>query</code>","text":"<p>The parameterized SQL statement. Must contain only a single statement (multiple statements separated by semi-colons are not allowed). If any parameters are used, they are referred to as <code>$1</code>, <code>$2</code>, etc.</p>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#return-values","title":"Return Values","text":"<p>An PgSql\\Result instance on success, or <code>[false](https://www.php.net/manual/en/reserved.constants.php#constant.false)</code> on failure.</p>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#changelog","title":"Changelog","text":"Version Description 8.1.0 Returns an PgSql\\Result instance now; previously, a resource was returned. 8.1.0 The <code>connection</code> parameter expects an PgSql\\Connection instance now; previously, a resource was expected."},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#examples","title":"Examples","text":"<pre><code>&lt;?php\n\n// Connect to a database named \"mary\"\n$dbconn = pg_connect(\"dbname=mary\");\n\n// Prepare a query for execution\n$result = pg_prepare($dbconn, \"my_query\", 'SELECT * FROM shops WHERE name = $1');\n\n// Execute the prepared query.  Note that it is not necessary to escape\n// the string \"Joe's Widgets\" in any way\n$result = pg_execute($dbconn, \"my_query\", array(\"Joe's Widgets\"));\n\n// Execute the same prepared query, this time with a different parameter\n$result = pg_execute($dbconn, \"my_query\", array(\"Clothes Clothes Clothes\"));\n\n?&gt;\n</code></pre>"},{"location":"Languages/PHP/03%20-%20Postgres/002%20-%20Create%20a%20prepared%20statement/#see-also","title":"See Also","text":"<ul> <li>pg_execute() - Sends a request to execute a prepared statement with given parameters, and waits for the result</li> <li>pg_send_execute() - Sends a request to execute a prepared statement with given parameters, without waiting for the result(s)</li> </ul>"},{"location":"Languages/Python/001%20-%20Getting%20started/","title":"Getting started with Python","text":"<p>I would recommend using pyenv to install python. I would highly advise against using any built-in python versions that come with your OS, if there are any.</p>"},{"location":"Languages/Python/001%20-%20Getting%20started/#pyenv-installation-and-setup","title":"pyenv installation and setup","text":"<p>NOTE: The below steps are assuming you are using Ubuntu/Debian with bash:</p> <pre><code>curl https://pyenv.run | bash\n\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bashrc\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc\n\nexec \"$SHELL\"\n\nsudo apt update; sudo apt install build-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev curl git \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n</code></pre>"},{"location":"Languages/Python/001%20-%20Getting%20started/#install-python-using-pyenv","title":"Install python using pyenv","text":"<pre><code>pyenv install 3.13\n</code></pre>"},{"location":"Languages/Python/001%20-%20Getting%20started/#use-python-version-for-current-directoryproject","title":"Use python version for current directory/project","text":"<pre><code>pyenv local 3.13\n</code></pre>"},{"location":"Languages/Python/001%20-%20Getting%20started/#use-python-version-globally","title":"Use python version globally","text":"<pre><code>pyenv global 3.13\n</code></pre>"},{"location":"Languages/Python/001%20-%20Getting%20started/#simple-python-program","title":"Simple python program","text":"<pre><code>\"\"\"\nMain module for the program.\n\"\"\"\n\nimport sys\n\n\ndef main():\n    \"\"\"Entry point for the program.\"\"\"\n    print(\"Hello World!\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    EXIT = main()\n    sys.exit(EXIT)\n</code></pre>"},{"location":"Languages/Python/002%20-%20Package%20Management/","title":"002 - Package Management","text":"<p>List of package managers for python.</p> <ul> <li>uv<ul> <li>An extremely fast Python package and project manager, written in Rust.</li> </ul> </li> </ul>"},{"location":"Package%20Management/uv/001%20-%20Getting%20Started/","title":"Getting started","text":"<p>Last updated as part of: v0.4.24</p> <p>To install uv, run the standalone installer using the following command:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"Package%20Management/uv/001%20-%20Getting%20Started/#python-versions","title":"Python versions","text":"<p>Installing and managing Python itself.</p> <ul> <li><code>uv python install</code>: Install Python versions. (Or install latest version)<ul> <li><code>uv python install 3.11</code></li> </ul> </li> <li><code>uv python list</code>: View available Python versions.</li> <li><code>uv python find</code>: Find an installed Python version.</li> <li><code>uv python pin</code>: Pin the current project to use a specific Python version.</li> <li><code>uv python uninstall</code>: Uninstall a Python version.</li> </ul> <p>See the guide on installing Python to get started.</p>"},{"location":"Package%20Management/uv/001%20-%20Getting%20Started/#scripts","title":"Scripts","text":"<p>Executing standalone Python scripts, e.g., <code>example.py</code>.</p> <ul> <li><code>uv run</code>: Run a script.</li> <li><code>uv add --script</code>: Add a dependency to a script</li> <li><code>uv remove --script</code>: Remove a dependency from a script</li> </ul> <p>See the guide on running scripts to get started.</p>"},{"location":"Package%20Management/uv/001%20-%20Getting%20Started/#projects","title":"Projects","text":"<p>Creating and working on Python projects, i.e., with a <code>pyproject.toml</code>.</p> <ul> <li><code>uv init</code>: Create a new Python project.</li> <li><code>uv add</code>: Add a dependency to the project.<ul> <li><code>uv add &lt;package&gt;</code></li> <li><code>uv add '&lt;package&gt;==&lt;version&gt;'</code></li> </ul> </li> <li><code>uv remove</code>: Remove a dependency from the project.<ul> <li><code>uv remove &lt;package&gt;</code></li> </ul> </li> <li><code>uv sync</code>: Sync the project's dependencies with the environment.</li> <li><code>uv lock</code>: Create a lockfile for the project's dependencies.<ul> <li><code>uv lock --upgrade</code></li> <li><code>uv lock --upgrade &lt;package&gt;</code></li> <li><code>uv lock --upgrade-package &lt;package&gt;==&lt;version&gt;</code></li> </ul> </li> <li><code>uv run</code>: Run a command in the project environment.</li> <li><code>uv tree</code>: View the dependency tree for the project.</li> <li><code>uv build</code>: Build the project into distribution archives.</li> <li><code>uv publish</code>: Publish the project to a package index.</li> </ul> <p>See the guide on projects to get started.</p>"},{"location":"Package%20Management/uv/001%20-%20Getting%20Started/#tools","title":"Tools","text":"<p>Running and installing tools published to Python package indexes, e.g., <code>ruff</code> or <code>black</code>.</p> <ul> <li><code>uvx</code> / <code>uv tool run</code>: Run a tool in a temporary environment.</li> <li><code>uv tool install</code>: Install a tool user-wide.</li> <li><code>uv tool uninstall</code>: Uninstall a tool.</li> <li><code>uv tool list</code>: List installed tools.</li> <li><code>uv tool update-shell</code>: Update the shell to include tool executables.</li> </ul> <p>See the guide on tools to get started.</p>"},{"location":"Package%20Management/uv/001%20-%20Getting%20Started/#the-pip-interface","title":"The pip interface","text":"<p>Manually managing environments and packages \u2014 intended to be used in legacy workflows or cases where the high-level commands do not provide enough control.</p> <p>Creating virtual environments (replacing <code>venv</code> and <code>virtualenv</code>):</p> <ul> <li><code>uv venv</code>: Create a new virtual environment.</li> </ul> <p>See the documentation on using environments for details.</p> <p>Managing packages in an environment (replacing <code>pip</code> and <code>pipdeptree</code>):</p> <ul> <li><code>uv pip install</code>: Install packages into the current environment.</li> <li><code>uv pip show</code>: Show details about an installed package.</li> <li><code>uv pip freeze</code>: List installed packages and their versions.</li> <li><code>uv pip check</code>: Check that the current environment has compatible packages.</li> <li><code>uv pip list</code>: List installed packages.</li> <li><code>uv pip uninstall</code>: Uninstall packages.</li> <li><code>uv pip tree</code>: View the dependency tree for the environment.</li> </ul> <p>See the documentation on managing packages for details.</p> <p>Locking packages in an environment (replacing <code>pip-tools</code>):</p> <ul> <li><code>uv pip compile</code>: Compile requirements into a lockfile.</li> <li><code>uv pip sync</code>: Sync an environment with a lockfile.</li> </ul> <p>See the documentation on locking environments for details.</p> <p>Important</p> <p>These commands do not exactly implement the interfaces and behaviour of the tools they are based on. The further you stray from common workflows, the more likely you are to encounter differences. Consult the pip-compatibility guide for details.</p>"},{"location":"Package%20Management/uv/001%20-%20Getting%20Started/#utility","title":"Utility","text":"<p>Managing and inspecting uv's state, such as the cache, storage directories, or performing a self-update:</p> <ul> <li><code>uv cache clean</code>: Remove cache entries.</li> <li><code>uv cache prune</code>: Remove outdated cache entries.</li> <li><code>uv cache dir</code>: Show the uv cache directory path.</li> <li><code>uv tool dir</code>: Show the uv tool directory path.</li> <li><code>uv python dir</code>: Show the uv installed Python versions path.</li> <li><code>uv self update</code>: Update uv to the latest version.</li> </ul>"},{"location":"Package%20Management/uv/002%20-%20Managing%20python%20via%20uv/","title":"Managing python via uv","text":"<p>Last updated as part of: v0.4.24</p> <p>If Python is already installed on your system, uv will detect and use it without configuration. However, uv can also install and manage Python versions for you.</p> <p>To install the latest Python version:</p> <pre><code>uv python install\n</code></pre> <p>This will install a uv-managed Python version even if there is already a Python installation on your system. If you've previously installed Python with uv, a new version will not be installed.</p>"},{"location":"Package%20Management/uv/002%20-%20Managing%20python%20via%20uv/#installing-a-specific-version","title":"Installing a specific version","text":"<p>To install a specific Python version:</p> <pre><code>uv python install 3.12\n</code></pre> <p>To install multiple Python versions:</p> <pre><code>uv python install 3.11 3.12\n</code></pre> <p>To install an alternative Python implementation, e.g. PyPy:</p> <pre><code>uv python install pypy@3.12\n</code></pre>"},{"location":"Package%20Management/uv/002%20-%20Managing%20python%20via%20uv/#viewing-python-installations","title":"Viewing python installations","text":"<p>To view available and installed Python versions:</p> <pre><code>uv python list\n</code></pre>"},{"location":"Package%20Management/uv/003%20-%20Scripts/","title":"Running scripts","text":"<p>Last updated as part of: v0.4.24</p> <p>With uv you can run scripts with their dependencies.</p> <p>Given you have a file with <code>rich</code> as a dependency:</p> <pre><code>import time\nfrom rich.progress import track\n\nfor i in track(range(20), description=\"For example:\"):\n    time.sleep(0.05)\n</code></pre> <p>You can run this script with the following command: </p> <pre><code>uv run --with rich example.py\n</code></pre>"},{"location":"Package%20Management/uv/003%20-%20Scripts/#creating-python-scripts","title":"Creating python scripts","text":"<p>You can create scripts with uv with the following command:</p> <pre><code>uv init --script example.py --python 3.12\n</code></pre> <p>This will attach inline script metadata inline with PEP.  You can also dependencies to the script, for example:</p> <pre><code>uv add --script example.py 'requests&lt;3' 'rich'\n</code></pre>"},{"location":"Package%20Management/uv/005%20-%20pip%20compile/","title":"uv - pip-tools","text":"<p><code>uv</code>  allows us to lock dependencies and create requirements.txt from those pinned versions files.</p> <p>Similarly to how we would with <code>pip-tools</code> (or <code>pip-compile</code>).</p>"},{"location":"Package%20Management/uv/005%20-%20pip%20compile/#start-a-uv-project","title":"Start a uv project","text":"<p>To have an environment to be able demonstrate this behaviour, start a uv project with the following commands:</p> <pre><code>uv init Play\ncd Play\nuv add fastapi[all]\n</code></pre>"},{"location":"Package%20Management/uv/005%20-%20pip%20compile/#create-a-requirementstxt-file-with-pinned-versions","title":"Create a requirements.txt file with pinned versions","text":"<p>To create our requirements.txt file, we can use the following command:</p> <pre><code>uv pip compile -o requirements.txt pyproject.toml\n</code></pre> <p>We should get something similar to the following:</p> <pre><code># This file was autogenerated by uv via the following command:\n#    uv pip compile -o requirements.txt pyproject.toml\nannotated-types==0.7.0\n    # via pydantic\nanyio==4.7.0\n    # via\n    #   httpx\n    #   starlette\n    #   watchfiles\ncertifi==2024.8.30\n    # via\n    #   httpcore\n    #   httpx\nclick==8.1.7\n    # via\n    #   rich-toolkit\n    #   typer\n    #   uvicorn\ndnspython==2.7.0\n    # via email-validator\nemail-validator==2.2.0\n    # via fastapi\nfastapi==0.115.6\n    # via play (pyproject.toml)\nfastapi-cli==0.0.6\n    # via fastapi\nh11==0.14.0\n    # via\n    #   httpcore\n    #   uvicorn\nhttpcore==1.0.7\n    # via httpx\nhttptools==0.6.4\n    # via uvicorn\nhttpx==0.28.1\n    # via fastapi\nidna==3.10\n    # via\n    #   anyio\n    #   email-validator\n    #   httpx\nitsdangerous==2.2.0\n    # via fastapi\njinja2==3.1.4\n    # via fastapi\nmarkdown-it-py==3.0.0\n    # via rich\nmarkupsafe==3.0.2\n    # via jinja2\nmdurl==0.1.2\n    # via markdown-it-py\norjson==3.10.12\n    # via fastapi\npydantic==2.10.3\n    # via\n    #   fastapi\n    #   pydantic-extra-types\n    #   pydantic-settings\npydantic-core==2.27.1\n    # via pydantic\npydantic-extra-types==2.10.1\n    # via fastapi\npydantic-settings==2.6.1\n    # via fastapi\npygments==2.18.0\n    # via rich\npython-dotenv==1.0.1\n    # via\n    #   pydantic-settings\n    #   uvicorn\npython-multipart==0.0.19\n    # via fastapi\npyyaml==6.0.2\n    # via\n    #   fastapi\n    #   uvicorn\nrich==13.9.4\n    # via\n    #   rich-toolkit\n    #   typer\nrich-toolkit==0.12.0\n    # via fastapi-cli\nshellingham==1.5.4\n    # via typer\nsniffio==1.3.1\n    # via anyio\nstarlette==0.41.3\n    # via fastapi\ntyper==0.15.1\n    # via fastapi-cli\ntyping-extensions==4.12.2\n    # via\n    #   fastapi\n    #   pydantic\n    #   pydantic-core\n    #   pydantic-extra-types\n    #   rich-toolkit\n    #   typer\nujson==5.10.0\n    # via fastapi\nuvicorn==0.32.1\n    # via\n    #   fastapi\n    #   fastapi-cli\nuvloop==0.21.0\n    # via uvicorn\nwatchfiles==1.0.3\n    # via uvicorn\nwebsockets==14.1\n    # via uvicorn\n</code></pre>"},{"location":"Package%20Management/uv/005%20-%20pip%20compile/#removing-annotations","title":"Removing annotations","text":"<p>To remove the annotations (example: <code># via uvicorn</code>), we can use another command line argument, as below:</p> <pre><code>uv pip compile --no-annotate -o requirements.txt pyproject.toml\n</code></pre> <p>Output:</p> <pre><code># This file was autogenerated by uv via the following command:\n#    uv pip compile --no-annotate -o requirements.txt pyproject.toml\nannotated-types==0.7.0\nanyio==4.7.0\ncertifi==2024.8.30\nclick==8.1.7\ndnspython==2.7.0\nemail-validator==2.2.0\nfastapi==0.115.6\nfastapi-cli==0.0.6\nh11==0.14.0\nhttpcore==1.0.7\nhttptools==0.6.4\nhttpx==0.28.1\nidna==3.10\nitsdangerous==2.2.0\njinja2==3.1.4\nmarkdown-it-py==3.0.0\nmarkupsafe==3.0.2\nmdurl==0.1.2\norjson==3.10.12\npydantic==2.10.3\npydantic-core==2.27.1\npydantic-extra-types==2.10.1\npydantic-settings==2.6.1\npygments==2.18.0\npython-dotenv==1.0.1\npython-multipart==0.0.19\npyyaml==6.0.2\nrich==13.9.4\nrich-toolkit==0.12.0\nshellingham==1.5.4\nsniffio==1.3.1\nstarlette==0.41.3\ntyper==0.15.1\ntyping-extensions==4.12.2\nujson==5.10.0\nuvicorn==0.32.1\nuvloop==0.21.0\nwatchfiles==1.0.3\nwebsockets==14.1\n</code></pre>"},{"location":"Package%20Management/uv/005%20-%20pip%20compile/#disclaimer","title":"Disclaimer","text":"<p>You can also use <code>uv pip compile</code> with a requirements.in file instead of a pyproject.toml, allowing you to use uv as a replacement for pip-tools without changing too much about your environment.</p>"},{"location":"Package%20Management/uv/005%20-%20pip%20compile/#shortcomings","title":"Shortcomings","text":""},{"location":"Package%20Management/uv/005%20-%20pip%20compile/#dev-dependencies-with-pyprojecttoml","title":"Dev dependencies with pyproject.toml","text":"<p>You can not create a file for dev dependencies using the pyproject.toml file so you will need to use a requirements.in file. So lets do that for some dev dependencies:</p> <pre><code># requirements.dev.in\nblack\nflake8\npytest\nmypy\n</code></pre> <p>To create a requirements.dev.txt file for these, we can do the following:</p> <pre><code>uv pip compile --no-annotate -o requirements.dev.txt requirements.dev.in\n</code></pre> <p>Which will look like the following:</p> <pre><code># This file was autogenerated by uv via the following command:\n#    uv pip compile --no-annotate -o requirements.dev.txt requirements.dev.in\nblack==24.10.0\nclick==8.1.7\nflake8==7.1.1\niniconfig==2.0.0\nmccabe==0.7.0\nmypy==1.13.0\nmypy-extensions==1.0.0\npackaging==24.2\npathspec==0.12.1\nplatformdirs==4.3.6\npluggy==1.5.0\npycodestyle==2.12.1\npyflakes==3.2.0\npytest==8.3.4\ntyping-extensions==4.12.2\n</code></pre> <p>And then finally we can add the dependencies to the pyproject.toml with the following command while still having a dev requirements.txt</p> <pre><code>uv add --dev -r requirements.dev.txt\n</code></pre>"},{"location":"Package%20Management/uv/999%20-%20Additional%20resources/","title":"Additional resources","text":"<ul> <li>Dependencies with uv</li> <li>Workspaces with uv</li> <li>Tools with uv</li> <li>Caching with uv</li> <li>Environment variables with uv</li> <li>Authentication to registries with uv</li> <li>Configuring indices with uv</li> </ul>"},{"location":"Package%20Management/uv/999%20-%20Additional%20resources/#integration","title":"Integration","text":"<ul> <li>Docker<ul> <li>Example</li> </ul> </li> <li>Jupyter</li> <li>GitHub Actions</li> <li>GitLab</li> <li>FastAPI<ul> <li>Example</li> </ul> </li> <li>Alternative indices</li> </ul>"},{"location":"Package%20Management/uv/999%20-%20Additional%20resources/#uv-reference","title":"uv reference","text":"<ul> <li>uv reference</li> </ul>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.1%20-%20Creating%20a%20new%20project/","title":"Creating a new project","text":"<p>Last updated as part of: v0.4.24</p> <p>You can create a new Python project using the uv init command:</p> <pre><code>uv init hello-world\ncd hello-world\n</code></pre> <p>Alternatively, you can initialize a project in the working directory:</p> <pre><code>mkdir hello-world\ncd hello-world\nuv init\n</code></pre>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.1%20-%20Creating%20a%20new%20project/#managing-dependencies","title":"Managing dependencies","text":"<p>You can add dependencies to your <code>pyproject.toml</code> with the <code>uv add</code> command. This will also update the lockfile and project environment:</p> <pre><code>uv add requests\n</code></pre> <p>You can also specify version constraints or alternative sources:</p> <pre><code># Specify a version constraint\nuv add 'requests==2.31.0'\n</code></pre> <pre><code># Add a git dependency\nuv add git+https://github.com/psf/requests\n</code></pre> <p>To remove a package, you can use <code>uv remove</code>:</p> <pre><code>uv remove requests\n</code></pre> <p>To upgrade a package, run <code>uv lock</code> with the <code>--upgrade-package</code> flag:</p> <pre><code>uv lock --upgrade-package requests\n</code></pre> <p>The <code>--upgrade-package</code> flag will attempt to update the specified package to the latest compatible version, while keeping the rest of the lockfile intact.</p>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.1%20-%20Creating%20a%20new%20project/#running-the-project","title":"Running the project","text":"<p>For example using a project that is running FastAPI.</p> <pre><code>uv add \"fastapi==0.115.2\" \"uvicorn==0.32.0\"\n</code></pre> <pre><code>\"\"\"Creates and loads the FastAPI.\"\"\"\n\nimport asyncio\nfrom fastapi import FastAPI\nfrom uvicorn import Config as UConfig\nfrom uvicorn import Server as UServer\n\napi = FastAPI(title=\"Python Requests Releases API\")\n\n\nasync def serve() -&gt; None:\n    \"\"\"Serve the API.\"\"\"\n    config = UConfig(\"main:api\", port=8000, log_level=\"info\", workers=16, reload=False)\n    server = UServer(config)\n    await server.serve()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(serve())\n</code></pre> <pre><code>uv run example.py\n</code></pre> <p>Or you could use the .venv created by uv:</p> <pre><code>uv sync\nsource .venv/bin/activate\npython example.py\n</code></pre>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.1%20-%20Creating%20a%20new%20project/#dev-and-optional-dependencies","title":"Dev and optional dependencies","text":"<p>You can also add dev and optional dependencies with the following commands:</p> <pre><code>uv add \"pytest==8.3.3\" --dev # Dev dependency\nuv add \"pytest==8.3.3\" --optional devtools-testing # Optional dependency in a group\n</code></pre>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.2%20-%20Creating%20a%20new%20library%20or%20package/","title":"Creating a new library or package","text":"<p>Last updated as part of: v0.4.24</p> <p>UV distinguishes between libraries and packaged applications and we will do the same here, so starting off with:</p>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.2%20-%20Creating%20a%20new%20library%20or%20package/#libraries","title":"Libraries","text":"<p>Libraries can be created by using the <code>--lib</code> flag:</p> <pre><code>uv init --lib example-lib\n</code></pre> <p>You can select a different build backend template by using <code>--build-backend</code> with <code>hatchling</code>, <code>flit-core</code>, <code>pdm-backend</code>, <code>setuptools</code>, <code>maturin</code>, or <code>scikit-build-core</code>.</p> <pre><code>uv init --lib --build-backend maturin example-lib\n</code></pre>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.2%20-%20Creating%20a%20new%20library%20or%20package/#packages-packaged-applications","title":"Packages (Packaged Applications)","text":"<p>You can create one with the following command:</p> <pre><code>uv init --app --package example-packaged-app\n</code></pre> <p>You can also use a different build backend template by using the following command:</p> <pre><code>uv init --app --package --build-backend maturin example-packaged-app\n</code></pre>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.3%20-%20Platform%20restrictions/","title":"Platform restrictions","text":""},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.3%20-%20Platform%20restrictions/#limited-resolution-environments","title":"Limited resolution environments","text":"<p>If your project supports a more limited set of platforms or Python versions, you can constrain the set of solved platforms via the <code>environments</code> setting, which accepts a list of PEP 508 environment markers. For example, to constrain the lockfile to macOS and Linux, and exclude Windows:</p> <pre><code>[tool.uv]\nenvironments = [\n    \"sys_platform == 'darwin'\",\n    \"sys_platform == 'linux'\",\n]\n</code></pre> <p>Entries in the <code>environments</code> setting must be disjoint (i.e., they must not overlap). For example, <code>sys_platform == 'darwin'</code> and <code>sys_platform == 'linux'</code> are disjoint, but <code>sys_platform == 'darwin'</code> and <code>python_version &gt;= '3.9'</code> are not, since both could be true at the same time.</p>"},{"location":"Package%20Management/uv/004%20-%20Managing%20Projects/4.3%20-%20Platform%20restrictions/#platform-specific-dependencies","title":"Platform-specific dependencies","text":"<p>To ensure that a dependency is only installed on a specific platform or on specific Python versions, use Python's standardized environment markers syntax.</p> <p>For example, to install jax on Linux, but not on Windows or macOS:</p> <pre><code>uv add 'jax; sys_platform == \"linux\"'\n</code></pre> <p>or for python versions:</p> <pre><code>uv add 'numpy; python_version &gt;= \"3.11\"'\n</code></pre>"},{"location":"Projects/DockerLens/00%20-%20DockerLens%20outline/","title":"DockerLens","text":"<p>The project will should allow users to do the following things in the V1 release:</p>"},{"location":"Projects/DockerLens/00%20-%20DockerLens%20outline/#container-list-page","title":"Container list page","text":"<ul> <li>View all containers on device<ul> <li>Cards indicating number of containers with a certain state.</li> <li>Table with all containers information.</li> </ul> </li> </ul>"},{"location":"Projects/DockerLens/00%20-%20DockerLens%20outline/#container-detail-page","title":"Container detail page","text":"<ul> <li>Container Detail Page<ul> <li>State card.</li> <li>Restart count card.</li> <li>Number of mounts.</li> <li>Container resources CPU card.</li> <li>Container resources Memory card.</li> <li>Container resources CPU chart.</li> <li>Container resources Memory chart.</li> </ul> </li> </ul>"},{"location":"Projects/DockerLens/00%20-%20DockerLens%20outline/#system-overview-page","title":"System overview page","text":"<p>TBC</p>"},{"location":"Projects/DockerLens/01%20-%20Repository%20Clean-up/","title":"Background","text":"<ul> <li>Branch</li> <li>Issue</li> <li>PR</li> </ul>"},{"location":"Projects/DockerLens/01%20-%20Repository%20Clean-up/#description","title":"Description","text":"<p>We should update the repository to be a bit more self-explanatory in structure and also improve developer experience.</p>"},{"location":"Projects/DockerLens/01%20-%20Repository%20Clean-up/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Update the <code>/server</code> directory to be the <code>/app</code> directory. AC2 - Update the <code>/site</code> directory to be the <code>/web</code> directory. AC3 - Update .gitignore to use <code>**</code> instead of <code>/server</code> AC4 - Update compose file to be more conventional. AC5 - Update compose file to have watch/dev support.</p>"},{"location":"Projects/DockerLens/01%20-%20Repository%20Clean-up/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/01%20-%20Repository%20Clean-up/#update-directories","title":"Update directories","text":"<p>This is a small updates just to be more verbose about what is actually in which folder. </p> <p><code>Server</code> was ambiguous at best, so renamed it to <code>app</code> and also moved the <code>.gitignore</code> to the app directory, to keep the exclusions specified by it local to that directory.</p> <p>Also updated the <code>site</code> directory to <code>web</code> .</p>"},{"location":"Projects/DockerLens/01%20-%20Repository%20Clean-up/#update-compose-file","title":"Update compose file","text":"<p>Updated the compose file to match the new directory structure with some small tweaks.</p> <pre><code>services:\n  app:\n    container_name: lens-app\n    build:\n      context: app\n\n  web:\n    container_name: lens-web\n    build:\n      context: .\n    ports:\n      - 80:80\n</code></pre> <p>This is also meant I had to update the Caddyfile:</p> <pre><code>:80 {\n    handle_path /* {\n        reverse_proxy app:80\n    }\n\n    handle_path /docs/* {\n        root * /var/www/html/docs/\n        file_server\n    }\n}\n</code></pre>"},{"location":"Projects/DockerLens/01%20-%20Repository%20Clean-up/#developwatch-mode-with-compose-file","title":"Develop/Watch mode with compose file","text":"<p>Updated the compose file to use watch to enable on the fly changes when changing code that reflects in the running containers.</p> <pre><code>services:\n  app:\n    container_name: lens-app\n    restart: always\n    build:\n      context: app\n    develop:\n      watch:\n        - action: sync+restart\n          path: ./app/config/custom.conf\n          target: /etc/apache2/conf-available/custom.conf\n        - action: sync+restart\n          path: ./app/public/\n          target: /var/www/html/public\n        - action: sync+restart\n          path: ./app/src/\n          target: /var/www/html/src\n        - action: rebuild\n          path: ./app/composer.json\n        - action: rebuild\n          path: ./app/Dockerfile\n\n  web:\n    container_name: lens-web\n    restart: always\n    build:\n      context: .\n    ports:\n      - 80:80\n    develop:\n      watch:\n        - action: sync+restart\n          path: ./web/Caddyfile\n          target: /etc/caddy/Caddyfile\n        - action: rebuild\n          path: ./docs\n</code></pre>"},{"location":"Projects/DockerLens/02%20-%20Overview%20page%20wireframes/","title":"Background","text":"<ul> <li>Branch</li> <li>Issue</li> <li>PR</li> </ul>"},{"location":"Projects/DockerLens/02%20-%20Overview%20page%20wireframes/#description","title":"Description","text":"<p>We should update the documentation with wire frames/mock-ups of the overview page. This page is what will greet users when they first open DockerLens.</p>"},{"location":"Projects/DockerLens/02%20-%20Overview%20page%20wireframes/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Create wire frames for the overview page.</p>"},{"location":"Projects/DockerLens/02%20-%20Overview%20page%20wireframes/#outcome","title":"Outcome","text":"<p>Pretty basic ticket, added wireframes for system overview page to the documentation.</p> <p></p>"},{"location":"Projects/DockerLens/03%20-%20Documentation%20on%20initial%20implementation/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/03%20-%20Documentation%20on%20initial%20implementation/#description","title":"Description","text":"<p>We should add documentation on the initial code pattern and architecture that is being used to create DockerLens.</p>"},{"location":"Projects/DockerLens/03%20-%20Documentation%20on%20initial%20implementation/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Documentation on the initial code pattern and architecture that is being used</p>"},{"location":"Projects/DockerLens/03%20-%20Documentation%20on%20initial%20implementation/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/04%20-%20Initial%20test%20suite/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/04%20-%20Initial%20test%20suite/#description","title":"Description","text":"<p>We should add tests to the code-base to cover the work that has been done so far.</p>"},{"location":"Projects/DockerLens/04%20-%20Initial%20test%20suite/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Add tests to cover initial implementation AC2 - Create test pipelines/workflows.</p>"},{"location":"Projects/DockerLens/04%20-%20Initial%20test%20suite/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/05%20-%20General%20Pipelines/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/05%20-%20General%20Pipelines/#description","title":"Description","text":"<p>Add pipelines for the following checks</p> <ul> <li>Lint</li> <li>Build</li> <li>Sonarcloud</li> </ul>"},{"location":"Projects/DockerLens/05%20-%20General%20Pipelines/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Add linting pipeline/workflow. AC2 - Add build pipeline/workflow. AC3 - Add sonarcloud pipeline/workflow.</p>"},{"location":"Projects/DockerLens/05%20-%20General%20Pipelines/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/06%20-%20Release%20pipelines/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/06%20-%20Release%20pipelines/#description","title":"Description","text":"<p>Create release pipelines using release-please.</p>"},{"location":"Projects/DockerLens/06%20-%20Release%20pipelines/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Add release pipeline/workflow.</p>"},{"location":"Projects/DockerLens/06%20-%20Release%20pipelines/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/07%20-%20Create%20more%20modular%20twig%20templates/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/07%20-%20Create%20more%20modular%20twig%20templates/#description","title":"Description","text":"<p>Create more modular twig templates that will allow only loading the necessary CSS and JS files instead of loading all. This will also remove the need to have HTML within the index.php file.</p>"},{"location":"Projects/DockerLens/07%20-%20Create%20more%20modular%20twig%20templates/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Create base template that loads all base resources. AC1.1 - Allow for injecting styles that will be required for each page. AC1.2 - Allow for injecting JS that will be required for each page. AC2 - Any other improvements to the twig templates.</p>"},{"location":"Projects/DockerLens/07%20-%20Create%20more%20modular%20twig%20templates/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/08%20-%20Update%20container%20query%20endpoint%20to%20include%20all%20containers/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/08%20-%20Update%20container%20query%20endpoint%20to%20include%20all%20containers/#description","title":"Description","text":"<p>Currently we only retrieve running containers from the docker API, we should update the query to include all containers regardless of state.</p>"},{"location":"Projects/DockerLens/08%20-%20Update%20container%20query%20endpoint%20to%20include%20all%20containers/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>Scenario 1: We would like to be able to view all containers running on the containers list page. Given: User is on the containers page When: User is viewing the containers in the table Then: User should be able to see all running containers.</p>"},{"location":"Projects/DockerLens/08%20-%20Update%20container%20query%20endpoint%20to%20include%20all%20containers/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/09%20-%20Update%20table%20to%20display%20the%20state%20in%20different%20colours/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/09%20-%20Update%20table%20to%20display%20the%20state%20in%20different%20colours/#description","title":"Description","text":"<p>To be more clear to the user, we should add colours to the state of the containers to illustrate what the container state is.</p>"},{"location":"Projects/DockerLens/09%20-%20Update%20table%20to%20display%20the%20state%20in%20different%20colours/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>Scenario 1: We would like to be able to see all containers that are running as a green colour. Given: User is on the containers page When: User is viewing the containers in the table Then: User should be able to see all containers with the running state having a green colour.</p> <p>Scenario 2: We would like to be able to see all containers that are paused as an amber colour. Given: User is on the containers page When: User is viewing the containers in the table Then: User should be able to see all containers with the paused state having an amber colour.</p> <p>Scenario 1: We would like to be able to see all containers that are paused as a red colour. Given: User is on the containers page When: User is viewing the containers in the table Then: User should be able to see all containers with the exited/restarting state having a red colour.</p>"},{"location":"Projects/DockerLens/09%20-%20Update%20table%20to%20display%20the%20state%20in%20different%20colours/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/10%20-%20Cards%20for%20different%20states/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/10%20-%20Cards%20for%20different%20states/#description","title":"Description","text":"<p>To give an overview of the containers, we need to add cards that indicate how much containers are in different states.</p>"},{"location":"Projects/DockerLens/10%20-%20Cards%20for%20different%20states/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Card that indicates number of containers that are running. AC2 - Card that indicates number of containers that are paused. AC3 - Card that indicates number of containers that are exited/restarting.</p>"},{"location":"Projects/DockerLens/10%20-%20Cards%20for%20different%20states/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/11%20-%20Container%20Detail%20Page%20-%20React%20FE%20Setup/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/11%20-%20Container%20Detail%20Page%20-%20React%20FE%20Setup/#description","title":"Description","text":"<p>Due to the nature and complexity of the components that will be on this page, we need to setup a react FE to ease development for this page.</p>"},{"location":"Projects/DockerLens/11%20-%20Container%20Detail%20Page%20-%20React%20FE%20Setup/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Setup a react FE/page that will integrate into the PHP app, should only be used for the container detail page for now.</p>"},{"location":"Projects/DockerLens/11%20-%20Container%20Detail%20Page%20-%20React%20FE%20Setup/#outcome","title":"Outcome","text":""},{"location":"Projects/DockerLens/12%20-%20Container%20detail%20page%20-%20State%20card/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/DockerLens/12%20-%20Container%20detail%20page%20-%20State%20card/#description","title":"Description","text":"<p>Create a card for viewing the containers state, that should be styled according to its state.</p>"},{"location":"Projects/DockerLens/12%20-%20Container%20detail%20page%20-%20State%20card/#acceptance-criteria","title":"Acceptance criteria","text":"<p>Scenario 1: As a user I would like to be able to view the status of a container on the container detail page. Given: User is on the containers detail page, When: User is at the top of the page, Then: User should be able to see the state of the container in a card.</p>"},{"location":"Projects/DockerLens/12%20-%20Container%20detail%20page%20-%20State%20card/#outcome","title":"Outcome","text":""},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/","title":"GitTogether","text":"<p>The project will should allow users to do the following things in the V1 release:</p> <ul> <li>List all pull requests for the repos they specify.<ul> <li>They will have the following fields shown:<ul> <li>PR ID and link</li> <li>Organization</li> <li>Repository</li> <li>State</li> <li>Title</li> <li>PR opener</li> </ul> </li> <li>Two charts<ul> <li>PRs by state</li> <li>PRs by organization</li> </ul> </li> <li>The user will be allowed to filter out repositories based on different fields, they should be able to filter by:<ul> <li>Organization</li> <li>Repository</li> <li>State (Open by default)</li> <li>User type of pull request creator (User by default)</li> <li>If you have been assigned a reviewer. (True by default)</li> </ul> </li> <li>The should also be allowed to sort the results.</li> </ul> </li> <li>View the details of a specific pull request.<ul> <li>There should be the following elements on this page<ul> <li>Heading with the title of the PR</li> <li>Card with the state</li> <li>Card with the PR opener</li> <li>Card with number of commits</li> <li>Card with number of additions</li> <li>Card with number of deletions</li> <li>Card with changed files</li> <li>Tab Component<ul> <li>PR reviewers</li> <li>Line chart with commit overview</li> </ul> </li> <li>Tab Component<ul> <li>Conversation/Comments panel</li> <li>Conversation/Reviews panel</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#setting-up-users","title":"Setting up users","text":"<p>Users will be able to create \"GitHub Profiles\" associated to their account/user. These GitHub Profiles will align to a existing GitHub profile.</p> <p>This is to allow users to create different profiles for work and personal accounts as an example.</p> <p>These will require the following fields:</p> <ul> <li>ID (primary key)</li> <li>Name (Ex. Evanlab02)</li> <li>Project user (foreign key)</li> </ul>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#setting-up-tokens","title":"Setting up tokens","text":"<p>The API will force using GH tokens to request data from the API.</p> <p>Therefore you will need to create records with the GitHub API tokens. You can create your own names for these.</p> <ul> <li>ID (Primary key)</li> <li>Name (Ex. My personal token/My work token)</li> <li>Token (The GH token)</li> <li>Project user (foreign key)</li> </ul>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#setting-up-synchronized-repositories","title":"Setting up synchronized repositories","text":"<p>You will also need to tell the API which repositories it should use. This will link to your profile and tokens. The profile part will enable us to differentiate things like comments indicating your comments different than other users comments.</p> <ul> <li>ID (Primary key)</li> <li>Organization (Ex. Evanlab02)</li> <li>Repository (Ex. GitTogether)</li> <li>GitHub Profile (Foreign Key)</li> <li>GitHub Token (Foreign Key)</li> <li>Project user (foreign key)</li> </ul>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#pull-requests-list-view","title":"Pull requests - List View","text":""},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#getting-the-data","title":"Getting the data","text":"<p>To retrieve this data, we will be using the GitHub pull requests API.</p> <p>API Documentation</p> <pre><code>curl -L \\\n  -H \"Accept: application/vnd.github+json\" \\\n  -H \"Authorization: Bearer &lt;YOUR-TOKEN&gt;\" \\\n  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n  https://api.github.com/repos/OWNER/REPO/pulls\n</code></pre>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#filters-and-charts","title":"Filters and Charts","text":"<p>To calculate the filters, we should only require the values that have been manually configured by the user.</p> <p>For the charts, we will be able to calculate this from the values we have received from the GitHub API.</p>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#table","title":"Table","text":"<p>The tables are pretty standard except for 2 things</p> <p>In the repository table, we will have 2 buttons in the actions column that does two things:</p> <ul> <li>Resync for organization</li> <li>Resync for repository</li> </ul> <p>What this will do, is just destroy the old cached value(s) for a repository or organization, and then re-query and cache them.</p> <p>We will also have a button on the PR table that will take you directly to the GitHub pull request.</p>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#caching","title":"Caching","text":"<p>The GitHub API only allows requests for a single repositories pull requests and there is rate limiting on the GitHub API. So we will need to ensure that these requests are cached.</p> <p>All results will be cached for 1 hour. This helps the application only pull what it needs at the time.</p>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#how-we-will-cache","title":"How we will cache","text":"<p>If we use something like Redis, the key will be something like</p> <p><code>&lt;org&gt;-&lt;repo&gt;-prs</code> with a value of the API response payload.</p>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#pull-requests-detail-view","title":"Pull requests - Detail View","text":"<p>This page will also use caching. Which will be covered below.</p> <p></p> <p></p>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#cards","title":"Cards","text":"<p>Here we will outline some key details about the PR.</p> <p>There will be buttons on the state card that will allow you to view the PR or resync it.</p>"},{"location":"Projects/GitTogether/000%20-%20GitTogether%20outline/#caching_1","title":"Caching","text":"<p>If we use something like Redis, the key will be something like</p> <p><code>&lt;org&gt;-&lt;repo&gt;-pr-&lt;number&gt;</code> with a value of the API response payload.</p>"},{"location":"Projects/GitTogether/001%20-%20Repository%20clean-up/","title":"Background","text":"<ul> <li>Branch</li> <li>Issue</li> <li>PR</li> </ul>"},{"location":"Projects/GitTogether/001%20-%20Repository%20clean-up/#description","title":"Description","text":"<p>Need to do some clean-up to improve the overall developer experience.</p>"},{"location":"Projects/GitTogether/001%20-%20Repository%20clean-up/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Update ROADMAP with issues. AC2 - Cleanup compose files.  (DONE) AC2.1 - Add PgAdmin to the compose files.  (DONE) AC2.2 - Improve docker files  (DONE) AC2.3 - Clearly document development and production run methods. (DONE) AC3 - Restructure and rename folders.  (DONE)</p>"},{"location":"Projects/GitTogether/002%20-%20Profile%20Creation%20Page/","title":"Background","text":"<ul> <li>Branch</li> <li>Issue</li> <li>PR</li> </ul>"},{"location":"Projects/GitTogether/002%20-%20Profile%20Creation%20Page/#description","title":"Description","text":"<p>As a user, I should be able to create profiles for my different GitHub accounts where applicable.  These will be used to identify which github tokens should be used for each repository that should be synchronized.</p>"},{"location":"Projects/GitTogether/002%20-%20Profile%20Creation%20Page/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Create a navigation bar/menu entry for <code>Profiles</code><ul> <li>AC1.1 - For larger screens we want a dropdown menu, that has an <code>Add</code> menu option in it on the bar.</li> <li>AC1.2 - For smaller screens we want a section in the navigation menu for GitHub profiles with <code>Add</code> option under it.</li> </ul> </li> <li>AC2 - Create a page for the GitHub profile add.<ul> <li>AC2.1 - Create form for the adding.</li> <li>AC2.2 - Should have name field.</li> </ul> </li> <li>AC3 - Create a model for GitHub profiles</li> <li>AC4 - Create controller flow for page render.</li> <li>AC5 - Create controller flow for profile creation.</li> </ul>"},{"location":"Projects/GitTogether/003%20-%20Add%20PgAdmin%20to%20compose%20file/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/GitTogether/003%20-%20Add%20PgAdmin%20to%20compose%20file/#description","title":"Description","text":"<p>As a developer, I should be able to view more information about the postgres database using PgAdmin.</p>"},{"location":"Projects/GitTogether/003%20-%20Add%20PgAdmin%20to%20compose%20file/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Add PgAdmin to the compose file with all necessary .env config.</p>"},{"location":"Projects/GitTogether/003%20-%20Add%20PgAdmin%20to%20compose%20file/#outcome","title":"Outcome","text":"<p>Was already done at some stage.</p>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/","title":"Background","text":"<ul> <li>Issue</li> <li>Branch</li> <li>PR</li> </ul>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#description","title":"Description","text":"<p>Currently we do not document two important things:</p> <ul> <li>The admin app will only be accessible on the primary host.</li> <li>The home portal is not intended to be run in production environments.</li> </ul> <p>We should update the documentation to reflect this.</p>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Document that the admin app is only available on the primary host.</li> <li>AC2 - Document that the home portal is not intended for production environments.</li> <li>AC3 - Use the new styling for the documentation.</li> <li>BONUS - Remove all the different release types we are creating as it is redundant.</li> </ul>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#outcome","title":"Outcome","text":""},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#admin-app-is-only-available-on-the-primary-host","title":"Admin app is only available on the primary host","text":"<p>A recent new addition to functionality allows for multiple hosts (up to 3 hosts). An unintentional side effect was that the admin page was only available on the primary host, but after reviewing this. This seems like a good thing. I will be leaving it as is, but added some documentation to make users aware of this.</p> <p>You will be able to see the updates on the documentation here.</p>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#documenting-that-home-portal-is-not-intended-for-production-environments","title":"Documenting that home portal is not intended for production environments","text":"<p>Home portal is a hobby project, and is only maintained by myself. Therefore it is highly recommended that users do not use this within a production environment. I have created a disclaimer on the home page of the documentation that indicates that this project is not meant for production environments.</p> <p>You will be able to see the updates on the documentation here.</p>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#updating-the-theme-to-mkdocs-material","title":"Updating the theme to mkdocs-material","text":"<p>To align with the other documentation sites I have available, I have decided to update the theme for the documentation to <code>mkdocs-material</code>. This has also become quite standard within the python community for documentation.</p>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#added-mkdocs-material-to-the-pipfile","title":"Added mkdocs-material to the Pipfile","text":"<pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\nmkdocs = \"==1.6.1\"\nmkdocs-material = \"==9.5.41\"\n\n[dev-packages]\n\n[requires]\npython_version = \"3.12\"\n</code></pre>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#mkdocs-material-configuration","title":"mkdocs-material configuration","text":"<pre><code>plugins:\n  - search\nmarkdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - \"pymdownx.inlinehilite\"\n  - \"pymdownx.snippets\"\n  - \"pymdownx.superfences\"\n  - \"admonition\"\n  - \"pymdownx.details\"\n  - \"pymdownx.superfences\"\ntheme:\n  name: material\n  features:\n    - content.code.copy\n  palette: \n    - scheme: default\n      toggle:\n        icon: material/brightness-7 \n        name: Switch to dark mode\n    - scheme: slate\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n</code></pre>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#remove-redundant-release-jobs","title":"Remove redundant release jobs","text":"<p>Currently we are creating a release image for all the different parts of the app changes and then one final release that combines all of the changes together. This is not necessary as it would all be the same images in the end. Now we are just releasing one image for all these changes.</p>"},{"location":"Projects/HomePortal/001%20-%20Update%20the%20documentation/#other-fixes","title":"Other fixes","text":"<ul> <li>Updated the host of the URLs for the documentation on the index/home page. This is due to the fact that I moved all my documentation to one host for all my projects and this was an artifact from before where I had a dedicated host/site for the Home Portal documentation. You will be able to see the updates on the documentation here.</li> </ul>"},{"location":"Projects/HomePortal/002%20-%20Create%20story%20for%20card%20component/","title":"Background","text":"<ul> <li>Issue</li> <li>Branch</li> <li>PR</li> </ul>"},{"location":"Projects/HomePortal/002%20-%20Create%20story%20for%20card%20component/#description","title":"Description","text":"<p>Currently we are missing some stories for some components from the home portal component library, we will be focusing on the the card component.</p> <p>We should add a story for the card component.</p>"},{"location":"Projects/HomePortal/002%20-%20Create%20story%20for%20card%20component/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Create story (stories) for the card component in the Home Portal component library.</li> <li>BONUS - Update compose file with latest versions and update documentation with latest versions</li> </ul>"},{"location":"Projects/HomePortal/002%20-%20Create%20story%20for%20card%20component/#outcome","title":"Outcome","text":""},{"location":"Projects/HomePortal/002%20-%20Create%20story%20for%20card%20component/#added-a-story-for-the-card-component","title":"Added a story for the card component","text":"<p>Within the home portal component library, we have a card component that does not have any existing storybook stories to view it. So we have added that to the storybook now.</p>"},{"location":"Projects/HomePortal/002%20-%20Create%20story%20for%20card%20component/#support-0170-of-the-shopping-list-app","title":"Support 0.17.0 of the Shopping List App","text":"<p>Home Portal now officially supports 0.17.0 of the Shopping List App. This has now become the recommended version for running instances of Home Portal and the documentation has been updated to reflect that.</p>"},{"location":"Projects/HomePortal/002%20-%20Create%20story%20for%20card%20component/#upgraded-versions-to-032-in-compose-file","title":"Upgraded versions to 0.3.2 in compose file","text":"<p>For the upcoming release of 0.3.2, I have decided to update compose files in anticipation for that as this will also ensure the installer is using the latest versions, instead of 0.2.0.</p>"},{"location":"Projects/HomePortal/003%20-%20Investigate%20running%20shopping%20app%20on%20the%20same%20host/","title":"Background","text":"<ul> <li>Issue</li> </ul>"},{"location":"Projects/HomePortal/003%20-%20Investigate%20running%20shopping%20app%20on%20the%20same%20host/#outcome","title":"Outcome","text":"<p>We need to add the following things on the ShoppingListApp repository.</p> <ul> <li>A dedicated  admin container that houses the admin interface.</li> <li>A flag that allows the adjustment of the static path for static assets.</li> <li>A flag that allows the adjustment of the admin path.</li> </ul> <p>Here is the issue for it (should be released as part of <code>V0.18.0</code>):</p> <ul> <li>Issue</li> </ul> <p>Here is the issue for updating it on the home portal repository:</p> <ul> <li>Issue</li> </ul>"},{"location":"Projects/HomePortal/004%20-%20Remove%20development%20applications/","title":"Background","text":"<ul> <li>Issue</li> <li>PR (PENDING)</li> </ul>"},{"location":"Projects/HomePortal/004%20-%20Remove%20development%20applications/#description","title":"Description","text":"<p>The development apps are not useful or beneficial and just make the build processes more cumbersome, so remove them.</p>"},{"location":"Projects/HomePortal/004%20-%20Remove%20development%20applications/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Remove them from dockerfile AC2 - Remove the documentation on them AC3 - Add a migration guide on this topic AC4 - Remove them from the caddyfile AC5 - Create a developer getting started guide</p>"},{"location":"Projects/HomePortal/005%20-%20Add%20your_spotify%20to%20home%20portal/","title":"Background","text":"<ul> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/HomePortal/005%20-%20Add%20your_spotify%20to%20home%20portal/#description","title":"Description","text":"<p>Add your_spotify to home portal</p>"},{"location":"Projects/HomePortal/005%20-%20Add%20your_spotify%20to%20home%20portal/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Add your_spotify. AC2 - Create application setup guide for it. AC3 - Create some docs on the architecture for home portal.</p>"},{"location":"Projects/HomePortal/000%20-%20Releases/Home%20Portal%20V0.3.2/","title":"Home Portal V0.3.2","text":""},{"location":"Projects/HomePortal/000%20-%20Releases/Home%20Portal%20V0.3.2/#032-2024-11-13","title":"0.3.2 (2024-11-13)","text":""},{"location":"Projects/HomePortal/000%20-%20Releases/Home%20Portal%20V0.3.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Compose: Update home portal images in compose file to '0.3.2' (e0b9e87)</li> <li>Documentation: Added a note that the primary host for home portal will be the only one the admin page can be accessed on. (f0468d0)</li> <li>Documentation: Changed theme to mkdocs-material (8526bd4)</li> <li>Documentation: Update documentation URLs on home/index page, hosts were still the old values (582b93b)</li> <li>Documentation: Updated the documentation to reflect that this project is not intended for production environments and is only a hobby project (2739e1c)</li> <li>Releases: Removed all the duplicate release jobs and pipelines (c90eba1)</li> <li>ShoppingListApp: Now officially support '0.17.0' of the ShoppingListApp. Updated documentatation to reflect this (2906e8d)</li> </ul> <p>The new documentation has also been deployed to https://evanlab-gme8r.ondigitalocean.app/hp/</p>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/","title":"Background","text":"<ul> <li>Issue</li> <li>Branch</li> <li>PR</li> </ul>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#description","title":"Description","text":"<p>We would like to use Argon2 for password hashing instead of the default provided by django. This is to improve security but also to learn new things about hashing.</p> <p>We will need to update the PASSWORD_HASHERS setting in the settings file, to support the new algorithm (and the old one, so that users can upgrade to Argon2).</p> <p>We should not force users to have the old password hasher in the settings, and instead should allow a flag that will add this value if the user would like for this to be included, this will allow old users of ShoppingListApp to upgrade and new users to just use it as is.</p>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Add Argon2 password hashing as the default.</li> <li>AC2 - Add optional env value config that allows the upgrade to Argon2 by adding old hashing algorithms. This will be deprecated in future.</li> <li>AC3 - Add optional value that forces the legacy hasher instead of Argon2 to allow users to stay behind. This will be deprecated in future.</li> <li>AC4 - Add documentation on the above to V0.18.0 upgrade guide.</li> <li>BONUS - Update documentation with regards to the pattern we use for our code in the application.</li> </ul> <p>Created the following issue to keep track of the deprecation coming up: https://github.com/Evanlab02/ShoppingListApp/issues/214</p>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#outcome","title":"Outcome","text":""},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#add-argon2-password-hashing-as-the-default","title":"Add Argon2 password hashing as the default","text":"<p>First I needed to add the Argon2 dependencies to the project to allow django to use it for hashing, this is the updated Pipfile:</p> <pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\ndjango = \"==5.1.2\"\ndjango-ninja = \"==1.3.0\"\nuvicorn = \"==0.31.1\"\ngunicorn = \"==23.0.0\"\nuvicorn-worker = \"==0.2.0\"\npsycopg = {extras = [\"binary\", \"pool\"], version = \"==3.2.3\"}\nargon2-cffi = \"==23.1.0\"\n\n[dev-packages]\nblack = \"==24.10.0\"\nmypy = \"==1.11.2\"\nflake8 = \"==7.1.1\"\npydocstyle = \"==6.3.0\"\npytest = \"==8.3.3\"\nisort = \"==5.13.2\"\npytest-django = \"==4.9.0\"\npytest-cov = \"==5.0.0\"\ncoverage = \"==7.6.2\"\nrequests = \"==2.32.3\"\ntypes-requests = \"==2.32.0.20240914\"\nselenium = \"==4.25.0\"\nwebdriver-manager = \"==4.0.2\"\npytest-xdist = \"==3.6.1\"\nflake8-docstrings = \"==1.7.0\"\npytest-sugar = \"==1.0.0\"\ndjango-stubs = {extras = [\"compatible-mypy\"], version = \"==5.1.0\"}\n\n[requires]\npython_version = \"3.12\"\n</code></pre> <p>And then to add the Argon2 hashing as the default, I changed the settings.py file to contain the following:</p> <pre><code>PASSWORD_HASHERS = [\"django.contrib.auth.hashers.Argon2PasswordHasher\"]\n</code></pre>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#allow-for-upgrade-from-old-hashing-algorithms-to-argon2","title":"Allow for upgrade from old hashing algorithms to Argon2","text":"<p>To allow for the upgrade to Argon2 hashing, I have allowed for env var to adjust the <code>PASSWORD_HASHERS</code> value. This will be allowed if you set the <code>SHOPPING_ALLOW_LEGACY_HASHING</code> to a value of <code>1</code>. This will make the preference Argon2, but still allow old hashing algorithms for the passwords that have not changed to this hashing algorithm yet.</p> <p>NOTE: This env var will default to 1 if not provided, allowing legacy hashing. This is to allow smooth operating for now. As I get closer to V1, I will be changing this to be defaulted to 0, and then eventually removed</p> <p>This was implemented like so:</p> <pre><code>PASSWORD_HASHERS = [\"django.contrib.auth.hashers.Argon2PasswordHasher\"]\n\nOPTIONAL_LEGACY_HASHERS = [\n    \"django.contrib.auth.hashers.Argon2PasswordHasher\",\n    \"django.contrib.auth.hashers.PBKDF2PasswordHasher\",\n    \"django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher\",\n]\n\nENABLE_LEGACY_HASHING = getenv(\"SHOPPING_ALLOW_LEGACY_HASHING\", \"1\")\nif ENABLE_LEGACY_HASHING == \"1\":\n    PASSWORD_HASHERS = OPTIONAL_LEGACY_HASHERS\n</code></pre>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#allow-for-the-forced-usage-of-old-hashing-algorithms","title":"Allow for the forced usage of old hashing algorithms","text":"<p>This is not recommended to use but allows for a fallback if this update fails on your existing passwords for some reason. This is meant to be an escape hatch out of the possibility that something is not working as part of the new release. </p> <p>You can force using the legacy password hashers by passing the env var (<code>SHOPPING_FORCE_LEGACY_HASHING</code>) as a <code>1</code>. This defaults to <code>0</code>.</p> <p>This was implemented like so:</p> <pre><code>FORCED_LEGACY_HASHERS = [\n    \"django.contrib.auth.hashers.PBKDF2PasswordHasher\",\n    \"django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher\",\n]\n\nFORCE_LEGACY_HASHING = getenv(\"SHOPPING_FORCE_LEGACY_HASHING\", \"0\")\nif FORCE_LEGACY_HASHING == \"1\":\n    PASSWORD_HASHERS = FORCED_LEGACY_HASHERS\n</code></pre>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#update-documentation-on-these-changes-for-the-0180-upgrade","title":"Update documentation on these changes for the 0.18.0 upgrade","text":"<p>After these changes are applied, you should be able to view this documentation here.</p>"},{"location":"Projects/ShoppingListApp/001%20-%20Argon2%20Password%20Hashing/#update-documentation-on-some-code-patterns","title":"Update documentation on some code patterns","text":"<p>After these changes are applied, you should be able to view this documentation here.</p>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/","title":"Background","text":"<ul> <li>Issue</li> <li>Branch</li> <li>PR</li> </ul>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#description","title":"Description","text":"<p>We currently do not support sorting the data we receive from the API when it comes to the stores listing.</p> <p>We would like to add this functionality as it can be useful to see values alphabetically or by date created etc.</p>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Add store sorting to <code>/api/v1/stores</code> endpoint.</li> <li>AC2 - Add store sorting to <code>/api/v1/stores/me</code> endpoint.</li> <li>AC3 - Add store sorting to <code>/api/v1/stores/search</code> endpoint.</li> <li>AC4 - Should allow sorting for all of the following fields:<ul> <li><code>name</code></li> <li><code>created_on</code></li> <li><code>updated_on</code></li> </ul> </li> <li>BONUS - Add documentation on the code pattern for views.</li> </ul>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#outcome","title":"Outcome","text":""},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#updated-store-repo-to-allow-for-sorting","title":"Updated store repo to allow for sorting","text":"<p>Updated the <code>get_stores</code> and <code>filter_stores</code> functions to allow for sorting parameters that will be passed to the<code>_filter</code> function.</p> <pre><code>async def get_stores(\n    page_number: int = 1,\n    stores_per_page: int = 10,\n    user: User | None = None,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Get all stores.\n\n    Args:\n        page (int): The page number, default to 1.\n        stores_per_page (int): The number of stores per page, default to 10.\n        user (User | None): User who owns the stores.\n        sort (str | None): The field to sort by.\n        dir (str | None): The direction to sort in.\n\n    Returns:\n        StorePaginationSchema: Store pagination object.\n    \"\"\"\n    return await _filter(page_number, stores_per_page, user=user, sort=sort, sort_dir=sort_dir)\n</code></pre> <pre><code>async def filter_stores(\n    page_number: int = 1,\n    stores_per_page: int = 10,\n    name: str | None = None,\n    store_types: list[int] | None = None,\n    created_on: date | None = None,\n    created_before: date | None = None,\n    created_after: date | None = None,\n    updated_on: date | None = None,\n    updated_before: date | None = None,\n    updated_after: date | None = None,\n    user: User | None = None,\n    ids: list[int] | None = None,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Filter stores.\n\n    Args:\n        page_number (int): The page number.\n        stores_per_page (int): The number of stores per page.\n        name (str | None): The name of the store.\n        store_types (list[int] | None): The store types.\n        created_on (date | None): The date the store was created.\n        created_before (date | None): The date the store was created before.\n        created_after (date | None): The date the store was created after.\n        updated_on (date | None): The date the store was updated.\n        updated_before (date | None): The date the store was updated before.\n        updated_after (date | None): The date the store was updated after.\n        user (User | None): The user who created the store.\n        ids (list[int] | None): The store ids to filter from.\n        sort (str | None): The field to sort by.\n        dir (str | None): The direction to sort in.\n\n    Returns:\n        StorePaginationSchema: Store pagination object.\n    \"\"\"\n    return await _filter(\n        page_number,\n        stores_per_page,\n        name,\n        store_types,\n        created_on,\n        created_before,\n        created_after,\n        updated_on,\n        updated_before,\n        updated_after,\n        user,\n        ids=ids,\n        sort=sort,\n        sort_dir=sort_dir\n    )\n</code></pre> <p>Updated the private <code>_filter</code> function to allow for sorting. First I updated the function declaration:</p> <pre><code>def _filter(\n    page_number: int = 1,\n    stores_per_page: int = 10,\n    name: str | None = None,\n    store_types: list[int] | None = None,\n    created_on: date | None = None,\n    created_before: date | None = None,\n    created_after: date | None = None,\n    updated_on: date | None = None,\n    updated_before: date | None = None,\n    updated_after: date | None = None,\n    user: User | None = None,\n    ids: list[int] | None = None,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n</code></pre> <p>Second, I added code for sorting logic:</p> <pre><code>    sort_field = sort or \"updated_at\"\n    prefix = \"-\" if sort_dir == \"desc\" else \"\"\n    stores = stores.order_by(f\"{prefix}{sort_field}\")\n</code></pre> <p>And I added three tests for this functionality:</p> <ul> <li>stores/tests/database/test_store_repo_get.py<ul> <li>async def test_get_stores_with_sort_dir(self) -&gt; None:</li> <li>async def test_get_stores_with_sort(self) -&gt; None:</li> <li>async def test_get_stores_with_sort_and_dir(self) -&gt; None:</li> </ul> </li> </ul>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#updated-store-service-to-allow-for-sorting","title":"Updated store service to allow for sorting","text":"<p>Updated the two functions below to allow for this:</p> <pre><code>async def get_stores(\n    limit: int = 10,\n    page_number: int = 1,\n    user: Any | None = None,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Get the stores.\n\n    Args:\n        limit (int): The limit of stores per page, defaults 10.\n        page_number (int): The page number, defaults to 1.\n        user (User): User who created the stores.\n        sort (str | None): The field to sort by.\n        sort_dir (str | None): The direction to sort in.\n\n    Returns:\n        StorePaginationSchema: The stores in a paginated format.\n    \"\"\"\n    log.info(f\"Retrieving stores for page {page_number} with limit {limit}...\")\n    paginated_stores = await store_repo.get_stores(page_number, limit, user, sort, sort_dir)\n    return paginated_stores\n</code></pre> <pre><code>async def search_stores(\n    page: int = 1,\n    limit: int = 10,\n    name: str | None = None,\n    user: Any | None = None,\n    ids: list[int] | None = None,\n    store_types: list[int] | None = None,\n    created_on: date | None = None,\n    created_before: date | None = None,\n    created_after: date | None = None,\n    updated_on: date | None = None,\n    updated_before: date | None = None,\n    updated_after: date | None = None,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Search for stores based on criteria.\n\n    Args:\n        page (int): The page number.\n        limit (int): The number of stores per page.\n        name (str): Partial or full name of store.\n        user (User): The user that owns the store.\n        ids (list[int]): List of ids to filter from.\n        store_types (list[int]): List of store types to filter from.\n        created_on (date): The date the store was created.\n        created_before (date): Date the store was created before.\n        created_after (date): Date the store was created after.\n        updated_on (date): Date the store was last updated.\n        updated_before (date): Date the store was last updated before.\n        updated_after (date): Date the store was last updated after.\n        sort (str | None): The field to sort by.\n        sort_dir (str | None): The direction to sort in.\n\n    Returns:\n        StorePaginationSchema: The schema result which contains the stores that were searched for.\n    \"\"\"\n    log.info(f\"PAGE NO - {page}\")\n    log.info(f\"LIMIT - {limit}\")\n    log.info(f\"STORE TYPES - {store_types}\")\n    log.info(f\"CREATED ON - {created_on}\")\n    log.info(f\"CREATED BEFORE - {created_before}\")\n    log.info(f\"CREATED AFTER - {created_after}\")\n    log.info(f\"UPDATED ON - {updated_on}\")\n    log.info(f\"UPDATED BEFORE - {updated_before}\")\n    log.info(f\"UPDATED AFTER - {updated_after}\")\n    log.info(f\"IDS - {ids}\")\n    return await store_repo.filter_stores(\n        page_number=page,\n        stores_per_page=limit,\n        name=name,\n        user=user,\n        store_types=store_types,\n        created_on=created_on,\n        created_before=created_before,\n        created_after=created_after,\n        updated_on=updated_on,\n        updated_before=updated_before,\n        updated_after=updated_after,\n        ids=ids,\n        sort=sort,\n        sort_dir=sort_dir,\n    )\n</code></pre>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#updated-routersendpoints-to-allow-for-sorting","title":"Updated routers/endpoints to allow for sorting","text":"<p>Updated the endpoints:</p> <ul> <li><code>/api/v1/stores</code></li> <li><code>/api/v1/stores/me</code> </li> <li><code>/api/v1/stores/search</code></li> </ul> <pre><code>@store_router.get(\"\", response={200: StorePaginationSchema})\nasync def get_stores(\n    request: HttpRequest,\n    limit: int = 10,\n    page: int = 1,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Get the stores.\n\n    Args:\n        request (HttpRequest): The HTTP request.\n        limit (int): The limit of stores to get per page.\n        page (int): The page number.\n        sort (str | None): The field to sort by.\n        sort_dir (str | None): The direction to sort in.\n\n    Returns:\n        StorePaginationSchema: The stores.\n    \"\"\"\n    log.info(f\"User requested stores with limit ({limit}) for page: {page}.\")\n    result = await store_service.get_stores(limit, page, sort=sort, sort_dir=sort_dir)\n    return result\n</code></pre> <pre><code>@store_router.get(\"/me\", response={200: StorePaginationSchema})\nasync def get_personal_stores(\n    request: HttpRequest,\n    limit: int = 10,\n    page: int = 1,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Get the stores you have created.\n\n    Args:\n        request (HttpRequest): The HTTP request.\n        limit (int): The limit of stores to get per page.\n        page (int): The page number.\n        sort (str | None): The field to sort by.\n        sort_dir (str | None): The direction to sort in.\n\n    Returns:\n        StorePaginationSchema: The stores.\n    \"\"\"\n    user = await request.auser()\n    log.info(f\"User requested personal stores with limit ({limit}) for page: {page}.\")\n    result = await store_service.get_stores(limit, page, user, sort, sort_dir)\n    return result\n</code></pre> <pre><code>@store_router.post(\"/search\", response={200: StorePaginationSchema})\nasync def search(\n    request: HttpRequest,\n    filters: StoreSearch,\n    page: int = 1,\n    limit: int = 10,\n    name: str | None = None,\n    own: bool = False,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Perform search for stores.\n\n    Args:\n        request (HttpRequest): The HTTP request to the API.\n        filters (StoreSearch): The body containing the filters.\n        page (int): The page number.\n        limit (int): The number of stores per page.\n        name (str): Full or partial name to search for.\n        own (bool): Flag indicating if you would like to see only your own stores.\n        sort (str | None): The field to sort by.\n        sort_dir (str | None): The direction to sort in.\n\n    Returns:\n        StorePaginationSchema: The stores in a paginated response.\n    \"\"\"\n    user = None\n    if own:\n        user = await request.auser()\n\n    log.info(\"User searching stores...\")\n\n    return await store_service.search_stores(\n        page=page,\n        limit=limit,\n        name=name,\n        user=user,\n        ids=filters.ids,\n        store_types=filters.store_types,\n        created_on=filters.created_on,\n        created_before=filters.created_before,\n        created_after=filters.created_after,\n        updated_on=filters.updated_on,\n        updated_before=filters.updated_before,\n        updated_after=filters.updated_after,\n        sort=sort,\n        sort_dir=sort_dir,\n    )\n</code></pre>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#refactoroptimize-_filter-function","title":"Refactor/Optimize <code>_filter</code> function","text":"<p>Updated the <code>_filter</code> function in an attempt to optimize it. Use entirely async code where possible now instead of using the pagination class.</p> <pre><code>async def _filter(\n    page_number: int = 1,\n    stores_per_page: int = 10,\n    name: str | None = None,\n    store_types: list[int] | None = None,\n    created_on: date | None = None,\n    created_before: date | None = None,\n    created_after: date | None = None,\n    updated_on: date | None = None,\n    updated_before: date | None = None,\n    updated_after: date | None = None,\n    user: User | None = None,\n    ids: list[int] | None = None,\n    sort: Literal[\"name\", \"created_on\", \"updated_on\"] | None = None,\n    sort_dir: Literal[\"asc\", \"desc\"] | None = None,\n) -&gt; StorePaginationSchema:\n    \"\"\"\n    Filter stores.\n\n    Args:\n        page_number (int): The page number.\n        stores_per_page (int): The number of stores per page.\n        name (str | None): The name of the store.\n        store_types (list[int] | None): The store types.\n        created_on (date | None): The date the store was created.\n        created_before (date | None): The date the store was created before.\n        created_after (date | None): The date the store was created after.\n        updated_on (date | None): The date the store was updated.\n        updated_before (date | None): The date the store was updated before.\n        updated_after (date | None): The date the store was updated after.\n        user (User | AnonymousUser | AbstractBaseUser | None): The user who created the store.\n        ids (list[int] | None): The ids to filter from.\n\n    Returns:\n        StorePaginationSchema: Store pagination object.\n    \"\"\"\n    stores = Store.objects.select_related(\"user\").all()\n\n    if ids:\n        stores = stores.filter(id__in=ids)\n    if name:\n        stores = stores.filter(name__icontains=name)\n    if store_types:\n        stores = stores.filter(store_type__in=store_types)\n    if created_on:\n        stores = stores.filter(created_at__date=created_on)\n    if created_before:\n        stores = stores.filter(created_at__date__lte=created_before)\n    if created_after:\n        stores = stores.filter(created_at__date__gte=created_after)\n    if updated_on:\n        stores = stores.filter(updated_at__date=updated_on)\n    if updated_before:\n        stores = stores.filter(updated_at__date__lte=updated_before)\n    if updated_after:\n        stores = stores.filter(updated_at__date__gte=updated_after)\n    if user:\n        stores = stores.filter(user=user)\n\n    sort_field = sort or \"updated_at\"\n    prefix = \"\" if sort_dir == \"asc\" else \"-\"\n    stores = stores.order_by(f\"{prefix}{sort_field}\")\n\n    start = (page_number - 1) * stores_per_page\n    end = start + stores_per_page\n\n    total = await stores.acount()\n    total_pages = ceil(total / stores_per_page)\n\n    paginated_stores = stores[start:end]\n    results = [StoreSchema.from_orm(store) async for store in paginated_stores]\n\n    has_previous = page_number &gt; 1\n    previous_page = page_number - 1 if page_number &gt; 1 else None\n    has_next = end &lt; total\n    next_page = page_number + 1 if end &lt; total else None\n\n    result = StorePaginationSchema(\n        stores=results,\n        total=total,\n        page_number=page_number,\n        total_pages=total_pages,\n        has_previous=has_previous,\n        previous_page=previous_page,\n        has_next=has_next,\n        next_page=next_page,\n    )\n\n    return result\n</code></pre>"},{"location":"Projects/ShoppingListApp/002%20-%20Store%20Sorting%20Parameter/#added-documentation-for-another-code-pattern","title":"Added documentation for another code pattern","text":"<p>You can find the updates here once they go live.</p>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/","title":"Background","text":"<ul> <li>Issue</li> <li>Branch</li> <li>PR</li> </ul>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#description","title":"Description","text":"<p>We would like to flesh out the admin page capabilities and interactions with store models.</p> <p>Resources:</p> <ul> <li>Django Documentation</li> </ul>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#acceptance-criteria","title":"Acceptance criteria","text":"<p>Scenario 1: We would like to be able to bulk update the store type within in the admin page. Given: User is logged into admin page and viewing stores When: User is selecting multiple stores Then: User should be able to bulk update stores to update the store type.</p> <p>Scenario 2: We Would like to be able to filter by store type within in the admin page. Given: User is logged into admin page and viewing stores When: User is filtering the stores Then: User should be able to filter by store type.</p> <p>Scenario 3: We would like to be able to see all fields on the admin page. Given: User is logged into admin page When: User is viewing stores Then: User should be able to view all the fields of each store in the row.</p> <p>We should also add documentation on the new admin page to explain how it works and can be utilized.</p>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#bonus","title":"Bonus","text":"<p>Some things that would be nice to have but there is some uncertainty:</p> <ul> <li>Date filtering.</li> <li>Admin documentation (using the django standard way).</li> <li>Client model admin configuration.</li> <li>Clean-up compose files.</li> <li>Add documentation on development getting started.</li> </ul>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#outcome","title":"Outcome","text":"<p>NOTE: I accidentally pushed some changes straight to trunk.</p>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#we-would-like-to-be-able-to-see-all-fields-on-the-admin-page","title":"We would like to be able to see all fields on the admin page.","text":"<p>Created a <code>StoreAdmin</code> class to allow for custom admin configuration and updated the list_display value with the fields we would like to see on the admin page.</p> <pre><code>\"\"\"Contains the admin configuration for the stores app.\"\"\"\n\nimport logging\n\nfrom django.contrib import admin\nfrom django.contrib.admin import ModelAdmin\n\nfrom stores.models import ShoppingStore as Store\n\nlog = logging.getLogger(__name__)\nlog.info(\"Loading stores admin config...\")\n\n\nclass StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    list_display = [\"name\", \"store_type\", \"description\", \"created_at\", \"updated_at\", \"user\"]\n\n\nadmin.site.register(Store, StoreAdmin)\n\nlog.info(\"Loaded stores admin config.\")\n</code></pre>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#we-would-like-to-be-able-to-filter-by-store-type-within-in-the-admin-page","title":"We Would like to be able to filter by store type within in the admin page.","text":"<p>Updated the <code>list_filter</code> field to include fields we would like to filter by, added some extras as they were easy to add.</p> <pre><code>class StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    list_display = [\"name\", \"store_type\", \"description\", \"created_at\", \"updated_at\", \"user\"]\n    list_filter = [\"store_type\", \"user\", \"created_at\", \"updated_at\"]\n</code></pre>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#we-would-like-to-be-able-to-bulk-update-the-store-type-within-in-the-admin-page","title":"We would like to be able to bulk update the store type within in the admin page.","text":"<p>Updated the actions on the model, and created the update functions for it.</p> <pre><code>class StoreAdmin(ModelAdmin):  # type: ignore\n    \"\"\"Admin configuration class for store models.\"\"\"\n\n    list_display = [\"name\", \"store_type\", \"description\", \"created_at\", \"updated_at\", \"user\"]\n    list_filter = [\"store_type\", \"user\", \"created_at\", \"updated_at\"]\n    actions = [\"make_online\", \"make_in_store\", \"make_online_and_in_store\"]\n\n    @admin.action(description=\"Mark selected stores as online.\")\n    def make_online(self, request, queryset) -&gt; None:  # type: ignore\n        \"\"\"Update all selected stores to be online.\"\"\"\n        queryset.update(store_type=1)\n\n    @admin.action(description=\"Mark selected stores as in-store.\")\n    def make_in_store(self, request, queryset) -&gt; None:  # type: ignore\n        \"\"\"Update all selected stores to be in-store.\"\"\"\n        queryset.update(store_type=2)\n\n    @admin.action(description=\"Mark selected stores as online and in-store.\")\n    def make_online_and_in_store(self, request, queryset) -&gt; None:  # type: ignore\n        \"\"\"Update all selected stores as online and in-store.\"\"\"\n        queryset.update(store_type=3)\n</code></pre>"},{"location":"Projects/ShoppingListApp/003%20-%20Stores%20Custom%20Admin%20Configuration/#admin-documentation","title":"Admin Documentation","text":"<p>Used the guide here to implement built-in django admin documentation: https://docs.djangoproject.com/en/5.1/ref/contrib/admin/admindocs/ I do not think this delivers any value as of right now, but thought it was a nice to have.</p>"},{"location":"Projects/ShoppingListApp/004%20-%20Item%20Sorting%20Parameter/","title":"Background","text":"<ul> <li>Issue</li> <li>PR</li> </ul>"},{"location":"Projects/ShoppingListApp/004%20-%20Item%20Sorting%20Parameter/#description","title":"Description","text":"<p>We currently do not support sorting the data we receive from the API when it comes to the items listing.</p> <p>We would like to add this functionality as it can be useful to see values alphabetically or by date created etc.</p>"},{"location":"Projects/ShoppingListApp/004%20-%20Item%20Sorting%20Parameter/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Add item sorting to <code>/api/v1/items</code> endpoint.</li> <li>AC2 - Add item sorting to <code>/api/v1/items/me</code> endpoint.</li> <li>AC3 - Add item sorting to <code>/api/v1/items/search</code> endpoint.</li> <li>AC4 - Should allow sorting for all of the following fields:<ul> <li><code>name</code></li> <li><code>price</code></li> <li><code>created_on</code></li> <li><code>updated_on</code></li> </ul> </li> <li>BONUS - Document our github workflows in a high level + low level for the build.yaml file<ul> <li>Improve where necessary.</li> </ul> </li> </ul>"},{"location":"Projects/ShoppingListApp/004%20-%20Item%20Sorting%20Parameter/#outcome","title":"Outcome","text":""},{"location":"Projects/ShoppingListApp/004%20-%20Item%20Sorting%20Parameter/#can-now-sort-items","title":"Can now sort items","text":"<p>On the following endpoints, we can now sort, using a combination of  the <code>sort</code> and <code>sort_dir</code> parameters:</p> <ul> <li><code>/api/v1/items/</code></li> <li><code>/api/v1/items/me</code></li> <li><code>/api/v1/items/search</code></li> </ul>"},{"location":"Projects/ShoppingListApp/004%20-%20Item%20Sorting%20Parameter/#improved-the-buildyml-action-file","title":"Improved the <code>build.yml</code> action file","text":"<ul> <li>Better whitespace around steps.</li> <li>Now use uv to download dependencies to speed it up and also prepare for uv migration.</li> <li>Added documentation for this action file.</li> </ul>"},{"location":"Projects/ShoppingListApp/004%20-%20Item%20Sorting%20Parameter/#identified-areas-for-improvement-in-upcoming-tasks","title":"Identified areas for improvement in upcoming tasks","text":"<ul> <li>Within <code>item_service</code> when using the <code>search_items</code> functionality. We are querying stores when it is not necessary doing unnecessary hits on the DB. We should rather use store IDs and pass them into the repository.</li> <li>We should migrate the backend away from <code>pipenv</code> and to <code>uv</code>.</li> <li>We need to update the sonarcloud action to the new updated one.</li> <li>Update dependencies before next release.</li> </ul>"},{"location":"Projects/ShoppingListApp/005%20-%20Items%20Custom%20Admin%20Configuration/","title":"Background","text":"<ul> <li>Issue</li> <li>PR(Pending)</li> </ul>"},{"location":"Projects/ShoppingListApp/005%20-%20Items%20Custom%20Admin%20Configuration/#description","title":"Description","text":"<p>We would like to flesh out the admin page capabilities and interactions with store models.</p> <p>Resources:</p> <ul> <li>Django Documentation</li> </ul>"},{"location":"Projects/ShoppingListApp/005%20-%20Items%20Custom%20Admin%20Configuration/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - We should be able to filter items within the admin page. AC2 - We would like to be able to see all fields on the admin page.</p> <p>BONUS:</p> <p>AC3 - Create documentation for <code>lint.yml</code> action file.</p>"},{"location":"Projects/ShoppingListApp/005%20-%20Items%20Custom%20Admin%20Configuration/#outcome","title":"Outcome","text":""},{"location":"Projects/ShoppingListApp/006%20-%20Add%20filtering%20to%20store%20aggregation%20API/","title":"Background","text":"<ul> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/ShoppingListApp/006%20-%20Add%20filtering%20to%20store%20aggregation%20API/#description","title":"Description","text":"<p>When using the store aggregation API endpoint, we should be able to filter the results we get back. This should be using a similar filter we use for the search or list endpoints.</p>"},{"location":"Projects/ShoppingListApp/006%20-%20Add%20filtering%20to%20store%20aggregation%20API/#acceptance-criteria","title":"Acceptance criteria","text":"<p>Scenario 1: We would like to be able to filter stores that are aggregated. Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>name</code>. Example: <code>?name=Test</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by a list of <code>ids</code>. Example: <code>{ids: [1, 2, 3]}</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>store_type</code>. Example: <code>{store_types: [1, 2]}</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>created_on</code>. Example: <code>{\"created_on\": \"\"}</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>created_before</code>. Example: <code>{\"created_before\": \"\"}</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>created_after</code>. Example: <code>{\"created_after\": \"\"}</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>updated_on</code>. Example: <code>{\"updated_on\": \"\"}</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>updated_before</code>. Example: <code>{\"updated_before\": \"\"}</code></p> <p>Given: User is logged in When: User hits <code>/api/v1/stores/aggregate</code> Then: User should be able to filter by <code>updated_after</code>. Example: <code>{\"updated_after\": \"\"}</code></p> <p>BONUS:</p> <ul> <li>Create documentation for <code>unit-tests.yml</code> action file.</li> </ul>"},{"location":"Projects/ShoppingListApp/006%20-%20Add%20filtering%20to%20store%20aggregation%20API/#outcome","title":"Outcome","text":""},{"location":"Projects/ShoppingListApp/000%20-%20V2%20FE/001%20-%20Linting/","title":"Background","text":"<ul> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/ShoppingListApp/000%20-%20V2%20FE/001%20-%20Linting/#description","title":"Description","text":"<p>We need to add linting to the project, <code>eslint</code> would be a great fit if possible.</p>"},{"location":"Projects/ShoppingListApp/000%20-%20V2%20FE/001%20-%20Linting/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Add eslint to project AC2 - Setup eslint config AC3 - Create linting pipelines/workflows</p>"},{"location":"Projects/ShoppingListApp/000%20-%20V2%20FE/002%20-%20Navigation%20Bar/","title":"Background","text":"<ul> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/ShoppingListApp/000%20-%20V2%20FE/002%20-%20Navigation%20Bar/#description","title":"Description","text":"<p>Create a navigation bar for the application.</p>"},{"location":"Projects/ShoppingListApp/000%20-%20V2%20FE/002%20-%20Navigation%20Bar/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Add navigation bar. AC2 - Write snapshot tests for the application.</p>"},{"location":"Projects/Smith/001%20-%20Create%20faker%20module/","title":"Background","text":"<ul> <li>Issue</li> <li>PR</li> </ul>"},{"location":"Projects/Smith/001%20-%20Create%20faker%20module/#description","title":"Description","text":"<p>Smith should have a module where we can generate some fake data for the bench marking commands.</p> <p>For now, we should just create a module/command where we can see the help command for faker by using either of the below commands:</p> <p><code>smith faker</code> or <code>smith faker --help</code></p>"},{"location":"Projects/Smith/001%20-%20Create%20faker%20module/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Faker module/command implemented. AC2 - Initial test suite created. AC3 - Initial README. AC4 - Initial documentation on faker module. AC5 - Deploy smith documentation. AC6 - URL to smith documentation.</p>"},{"location":"Projects/Smith/002%20-%20Testing%20Pipelines/","title":"Background","text":"<ul> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/Smith/002%20-%20Testing%20Pipelines/#description","title":"Description","text":"<p>We should create testing workflows for the Smith project.</p> <p>While we are doing this, we should also add a developer getting started guide and also create some documentation on the testing workflows.</p>"},{"location":"Projects/Smith/002%20-%20Testing%20Pipelines/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>AC1 - Testing Workflows for the smith project. AC2 - Developer getting started guide. AC3 - Testing guide. AC4 - Testing workflows guide.</p>"},{"location":"Projects/Smith/002%20-%20Testing%20Pipelines/#outcome","title":"Outcome","text":""},{"location":"Projects/Smith/003%20-%20Faker%20Basic%20Array/","title":"003 - Faker Basic Array","text":"<ul> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/Smith/003%20-%20Faker%20Basic%20Array/#description","title":"Description","text":"<p>Create a faker command that can create a very basic array.</p> <pre><code>smith faker array basic &lt;int:length&gt; --min &lt;int:min default:0&gt; --max &lt;int:max default:1000000&gt; --output &lt;string:path default:data.json&gt;\n</code></pre>"},{"location":"Projects/Smith/003%20-%20Faker%20Basic%20Array/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Create array module AC2 - Create basic array command AC3 - Allow for setting the length of array AC4 - Allow for setting min value in array AC5 - Allow for setting max value in array AC6 - Allow for setting output value</p>"},{"location":"Projects/Smith/004%20-%20ClangFormat/","title":"Background","text":"<ul> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/Smith/004%20-%20ClangFormat/#description","title":"Description","text":"<p>Add ClangFormat to the project to format the codebase.</p>"},{"location":"Projects/Smith/004%20-%20ClangFormat/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Add ClangFormat AC2 - Add actions/workflows for ClangFormat</p>"},{"location":"Projects/Smith/004%20-%20ClangFormat/#reference","title":"Reference","text":"<p>ClangFormat documentation</p>"},{"location":"Projects/TaskManager/001%20-%20BE%20Testing/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/TaskManager/001%20-%20BE%20Testing/#description","title":"Description","text":"<p>We should implement testing for the BE, there should already be some examples on the application from the template.</p>"},{"location":"Projects/TaskManager/001%20-%20BE%20Testing/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Implement testing E2E for the create cycle endpoint. AC2 - Implement testing workflows/actions for the backend AC3 - Document the testing process. AC4 - Document the actions.</p> <p>Bonus:</p> <p>AC5 - Update the README and home page of the documentation. AC6 - Update the developer getting started guide. AC7 - Create dockerfile for the backend. AC8 - Add backend to dev compose file.</p>"},{"location":"Projects/TaskManager/001%20-%20BE%20Testing/#outcome","title":"Outcome","text":""},{"location":"Projects/TaskManager/002%20-%20FE%20Navigation/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/TaskManager/002%20-%20FE%20Navigation/#description","title":"Description","text":"<p>We should implement navigation on the FE.</p>"},{"location":"Projects/TaskManager/002%20-%20FE%20Navigation/#acceptance-criteria","title":"Acceptance criteria","text":"<p>AC1 - Create navigation bar. AC2 - Create side menu. AC3 - Create routing. AC4 - Create initial pages.</p>"},{"location":"Projects/WoWScout/001%20-%20WoWScout%20Project%20Outline/","title":"WoWScout","text":"<p>The project will should allow users to do the following things in the V1 release:</p> <ul> <li>List all game characters for the user.<ul> <li>They will have the following fields shown:<ul> <li>Name</li> <li>Realm</li> <li>Class</li> <li>Race</li> <li>Faction</li> <li>Level</li> </ul> </li> <li>The user will be able to view the total characters, total alliance characters and total horde characters on the cards.</li> <li>The user will be allowed to filter characters based on region.</li> </ul> </li> <li>View the details of a character.<ul> <li>Cards with various different information.</li> <li>Equipment pane.</li> <li>Charts highlighting stat changes for character.</li> </ul> </li> </ul>"},{"location":"Projects/WoWScout/001%20-%20WoWScout%20Project%20Outline/#using-the-blizzard-apis","title":"Using the Blizzard APIs","text":"<p>To use the blizzard APIs, we need to setup OAuth client flows.</p> <p>This will entail</p> <ul> <li>Having a login page.</li> <li>Login page takes you to Blizzard OAuth.</li> <li>Blizzard OAuth redirects to local with auth code.</li> <li>We retrieve token for user, and pass it back to be stored in cookie storage.</li> </ul>"},{"location":"Projects/WoWScout/002%20-%20Login%20Flow/","title":"Background","text":"<ul> <li>Issue</li> <li>Branch</li> <li>PR</li> </ul>"},{"location":"Projects/WoWScout/002%20-%20Login%20Flow/#description","title":"Description","text":"<p>Before we can access the Blizzard APIs for WoW data, we need to create a login flow using OAuth to get a JWT to access them. This will enable the API to query on behalf of the user for the data.</p> <p>This requires 3 steps: - Hit the APIs dedicated login endpoint, that will redirect. - The redirect should lead to the blizzard oauth service. - Once logged in, we should be redirected to the service, to be able to retrieve the JWT on behalf of the user using the code.</p> <p>We will need to cache the results of the JWT.</p> <p>For caching options there are: - Redis - Memcached</p> <p>We will also need to return a cookie to the user for the session ID that belongs to them.</p> <p>It is important we do not store the JWT in the cookies as we would be allowing attackers to grab blizzard JWTs, causing harm. So instead we use a session ID that allows you to do only things with our API, which all are non-destructive actions and since this is a hobby project, will not be publicly available regardless, so the harm should be for the most part avoided.</p>"},{"location":"Projects/WoWScout/002%20-%20Login%20Flow/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>Create <code>/api/v1/auth/login</code> endpoint.<ul> <li>Which redirects users to blizzard and creates the tokens in the cache.</li> </ul> </li> <li>Create <code>/api/v1/auth/status</code> endpoint.<ul> <li>Which indicates whether the user does still have access or not. Will be used to determine if we need to recreate the token.</li> </ul> </li> <li>Update documentation with local development guide.</li> </ul>"},{"location":"Projects/WoWScout/003%20-%20Login%20Page/","title":"Background","text":"<ul> <li>Branch</li> <li>Issue</li> <li>PR</li> </ul>"},{"location":"Projects/WoWScout/003%20-%20Login%20Page/#description","title":"Description","text":"<p>Create a login page on the FE. This should simply with the click of a login button take the user to the login flow with the API.</p> <p>If the user visits the login page and they are already logged in, it should take them back to the home page.</p>"},{"location":"Projects/WoWScout/003%20-%20Login%20Page/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Basic home page that redirects if not logged in.</li> <li>AC2 - Login page that takes you to the login flow.</li> <li>AC3 - Login page that redirects you to home page if already logged in.</li> <li>BONUS<ul> <li>Update roadmap with updated issues.</li> </ul> </li> </ul>"},{"location":"Projects/WoWScout/004%20-%20Overview%20Page%20-%20Skeleton%20Filters/","title":"Background","text":"<ul> <li>Branch (Pending)</li> <li>Issue</li> <li>PR (Pending)</li> </ul>"},{"location":"Projects/WoWScout/004%20-%20Overview%20Page%20-%20Skeleton%20Filters/#description","title":"Description","text":"<p>Create the skeleton overview page with the basic components such as navigation etc.</p> <p>Create the empty skeleton filters on the page as well, that will be used and consumed in the future.</p>"},{"location":"Projects/WoWScout/004%20-%20Overview%20Page%20-%20Skeleton%20Filters/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>AC1 - Skeleton overview page.<ul> <li>AC1.1 - Navigation.</li> </ul> </li> <li>AC2 - Skeleton filters<ul> <li>AC2.1 - Region</li> </ul> </li> <li>BONUS<ul> <li>Update roadmap with updated issues.</li> <li>Update documentation on login flow/process.</li> </ul> </li> </ul>"},{"location":"Projects/WoWScout/004%20-%20Overview%20Page%20-%20Skeleton%20Filters/#outcome","title":"Outcome","text":""}]}